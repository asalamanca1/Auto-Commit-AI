# Git Commit Message Generator

Generate Git commit messages from raw code diffs using deep learning.

This project implements and compares two approaches:
- A **GPT-2 decoder-only transformer** trained from scratch.
- A **gated recurrent unit (GRU)**–based baseline model.

Each model is trained on real GitHub commits that only modify Python files. The goal is to automatically generate meaningful, readable commit messages directly from code changes.

---

<pre>## Project Structure \`\`\`

├── decoder-only-transformer/
│ ├── Data-Pipeline-for-Git-Commit-Transformer.ipynb
│ ├── Training-Git-Commit-Transformer.ipynb
│ ├── Inference-for-Git-Commit-Transformer.ipynb
│ ├── Performance_Evaluation_for_Git_Commit_Transformer.ipynb
│ ├── trained_model/
│ ├── data/
│ ├── custom_bpe_tokenizer.json ← custom trained BPE tokenizer
│ └── README.md ← model-specific details
│
├── gated-recurrent-unit/
│ ├── Data-Pipeline-for-Git-Commit-GRU.ipynb
│ ├── Training-Git-Commit-GRU.ipynb
│ ├── Inference-for-Git-Commit-GRU.ipynb
│ ├── Performance_Evaluation_for-Git-Commit-GRU.ipynb
│ ├── trained_model/
│ ├── data/
│ └── README.md  \`\`\` </pre>

Each directory contains:
- A complete notebook-based pipeline (data → tokenizer → training → inference → evaluation)
- Trained tokenizer and saved weights
- Cleaned dataset and metrics
- A `README.md` with notes specific to that model

---

## Model Details

### Decoder-Only Transformer (GPT-2)
- 20 transformer layers
- 768 hidden size
- 12 attention heads
- 48k BPE vocabulary
- Trained from scratch on GitHub commit diffs

### Gated Recurrent Unit (GRU)
- Sequence-to-sequence RNN architecture
- Serves as a simpler baseline for comparison
- Trained on the same dataset as the transformer

---

## Evaluation Metrics

Both models are evaluated on the same 100-sample test set using:

- **BLEU**
- **ROUGE-L**
- **METEOR**
- **BERTScore**
- **RAGAs Answer Correctness**  
  *(Requires OpenAI API key and active billing)*

Outputs include:
- `sample_metrics_with_ragas_and_bertscore.json`
- `all_diffs.txt` (readable format)
- Histogram plots for each metric

---

## How to Run

Pick either `decoder-only-transformer/` or `gated-recurrent-unit/` and run the notebooks in the following order:

### 1. Data Pipeline
- `Data-Pipeline-for-Git-Commit-*.ipynb`  
  Collect and clean commits + diffs from GitHub.

### 2. Training
- `Training-Git-Commit-*.ipynb`  
  Train the tokenizer and model from scratch.

### 3. Inference
- `Inference-for-Git-Commit-*.ipynb`  
  Generate commit messages from raw diffs.

### 4. Evaluation
- `Performance_Evaluation_for_Git_Commit-*.ipynb`  
  Run BLEU, ROUGE, METEOR, BERTScore, and RAGAs. Save scores and plots.
