{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUXJeGouLe4I"
      },
      "source": [
        "## Load the trained tokenizer\n",
        "\n",
        "Load the custom BPE tokenizer that was trained and saved in the previous step (`custom_bpe_tokenizer.json`) using Hugging Face’s `PreTrainedTokenizerFast`.\n",
        "\n",
        "### Special Tokens Added:\n",
        "- `<pad>` — used for padding sequences during batching\n",
        "- `<endOfCommitMessage>` — tells the model where the commit message ends\n",
        "- `<endOfDiff>` — separates the git diff from the commit message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAaimmPaLe4K"
      },
      "outputs": [],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "# Load custom BPE tokenizer\n",
        "custom_tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_file=\"custom_bpe_tokenizer.json\"\n",
        ")\n",
        "custom_tokenizer.add_special_tokens({\n",
        "    \"pad_token\": \"<pad>\",\n",
        "    \"eos_token\": \"<endOfCommitMessage>\",\n",
        "    \"bos_token\": \"<sos>\"\n",
        "})\n",
        "custom_tokenizer.add_tokens([\"<endOfDiff>\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqOy7Gt_Le4L"
      },
      "source": [
        "## GRU Commit Message Generator: Model Definition and Inference Setup\n",
        "\n",
        "This cell defines and loads a GRU-based language model for generating Git commit messages from Git diffs, and includes a decoding function that uses temperature-controlled sampling.\n",
        "\n",
        "### Components\n",
        "\n",
        "#### 1. **Device Setup**\n",
        "- Selects GPU (`cuda`) if available; otherwise falls back to CPU.\n",
        "\n",
        "#### 2. **`GRULanguageModel` Class**\n",
        "- A 4-layer GRU language model with:\n",
        "  - Shared input/output embeddings (weight tying).\n",
        "  - Dropout regularization between layers.\n",
        "  - Configurable vocabulary size, embedding dimension, hidden dimension, and padding index.\n",
        "\n",
        "#### 3. **`generate_commit_message()` Inference Function**\n",
        "- Takes in a Git diff and autoregressively generates a commit message.\n",
        "- Uses:\n",
        "  - **Temperature sampling** to control randomness.\n",
        "  - **Top-k filtering** to limit token sampling to the top `k` probable choices.\n",
        "- Stops generating once it produces the `<endOfCommitMessage>` token or hits `max_new_tokens`.\n",
        "\n",
        "#### 4. **Model Initialization and Loading**\n",
        "- Loads vocabulary size and padding ID from the tokenizer.\n",
        "- Restores model weights from `trained_model/gru_model.pt`.\n",
        "\n",
        "> Ensure that the tokenizer (`custom_tokenizer`) is already defined and includes special tokens: `<sos>`, `<endOfDiff>`, and `<endOfCommitMessage>`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UeAFRaiLe4L"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# GRU Language Model definition\n",
        "class GRULanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=512, hidden_dim=512, num_layers=4, dropout=0.2, pad_id=0):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True,\n",
        "                          dropout=dropout if num_layers > 1 else 0.0)\n",
        "        self.lm_head = nn.Linear(hidden_dim, vocab_size, bias=False)\n",
        "        self.lm_head.weight = self.embed.weight  # weight tying\n",
        "\n",
        "    def forward(self, input_ids, hidden=None):\n",
        "        x = self.drop(self.embed(input_ids))\n",
        "        out, new_h = self.gru(x, hidden)\n",
        "        logits = self.lm_head(out)\n",
        "        return logits, new_h\n",
        "\n",
        "# Inference function with temperature + top-k sampling\n",
        "def generate_commit_message(model, tokenizer, git_diff, max_new_tokens=50, device='cuda', temperature=0.8, top_k=50):\n",
        "    model.eval()\n",
        "    sep_token = \"<endOfDiff>\"\n",
        "    eos_token = \"<endOfCommitMessage>\"\n",
        "    bos_token = \"<sos>\"\n",
        "\n",
        "    input_text = bos_token + git_diff + sep_token\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "    eos_id = tokenizer.encode(eos_token)[0]\n",
        "\n",
        "    generated = input_ids\n",
        "    hidden = None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, hidden = model(generated[:, -1:], hidden)\n",
        "            logits = logits[0, -1] / temperature\n",
        "\n",
        "            if top_k > 0:\n",
        "                topk_values, topk_indices = torch.topk(logits, top_k)\n",
        "                probs = torch.zeros_like(logits).scatter_(0, topk_indices, torch.softmax(topk_values, dim=-1))\n",
        "            else:\n",
        "                probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "            next_id = torch.multinomial(probs, num_samples=1).item()\n",
        "            generated = torch.cat([generated, torch.tensor([[next_id]], device=device)], dim=1)\n",
        "\n",
        "            if next_id == eos_id:\n",
        "                break\n",
        "\n",
        "    decoded = tokenizer.decode(generated[0].tolist())\n",
        "    return decoded.split(sep_token)[1].replace(eos_token, \"\").strip()\n",
        "\n",
        "# Initialize and load the model\n",
        "vocab_size = len(custom_tokenizer)\n",
        "model = GRULanguageModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_dim=512,\n",
        "    hidden_dim=512,\n",
        "    num_layers=4,\n",
        "    dropout=0.2,\n",
        "    pad_id=custom_tokenizer.pad_token_id\n",
        ")\n",
        "model.load_state_dict(torch.load(\"trained_model/gru_model.pt\", map_location=device))\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-Q20g_kQBNf"
      },
      "source": [
        "## Inference on a Sample Git Diff with Stochastic Decoding\n",
        "\n",
        "This cell demonstrates how to generate multiple commit message predictions from a single Git diff using a trained GRU language model with temperature and top-k sampling.\n",
        "\n",
        "### Steps:\n",
        "\n",
        "- **Define a Sample Diff**  \n",
        "  A minimal Git diff is defined in `sample_diff` showing a small code modification (e.g., enabling a logging flag in `config.py`).\n",
        "\n",
        "- **Initial Commit Message Generation**\n",
        "  - `generate_commit_message(...)` is first called with:\n",
        "    - `temperature=0.8`: controls randomness in token sampling.\n",
        "    - `top_k=50`: restricts token sampling to the top 50 most probable choices.\n",
        "\n",
        "- **Multiple Variants via Sampling**\n",
        "  - The model is run **5 times** with:\n",
        "    - `temperature=0.9` and `top_k=100` to increase output diversity.\n",
        "    - Each run may yield a different commit message depending on sampling variation.\n",
        "\n",
        "### Purpose:\n",
        "This approach evaluates the model's **generative diversity** and helps identify whether it produces **semantically relevant and varied** messages for the same diff input.\n",
        "\n",
        "> Ensure that `model`, `custom_tokenizer`, and `generate_commit_message()` have been defined and loaded beforehand.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCHJtqnZLe4L"
      },
      "outputs": [],
      "source": [
        "# Sample Git diff input\n",
        "sample_diff = \"\"\"diff --git a/config.py b/config.py\n",
        "index abc123..def456 100644\n",
        "--- a/config.py\n",
        "+++ b/config.py\n",
        "@@ -1,5 +1,6 @@\n",
        " DEBUG = False\n",
        "+LOGGING_ENABLED = True\n",
        "\"\"\"\n",
        "\n",
        "# Run inference with sampling\n",
        "generated_msg = generate_commit_message(\n",
        "    model,\n",
        "    custom_tokenizer,\n",
        "    sample_diff,\n",
        "    device=device,\n",
        "    temperature=0.8,\n",
        "    top_k=50\n",
        ")\n",
        "\n",
        "for i in range(5):\n",
        "    msg = generate_commit_message(model, custom_tokenizer, sample_diff, device=device, temperature=0.9, top_k=100)\n",
        "    print(f\"[{i+1}] {msg}\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6krvZAy3QMKd"
      },
      "source": [
        "## Batch Inference on Validation Set Samples\n",
        "\n",
        "This cell evaluates the GRU commit message generation model on random samples from the validation split of the dataset.\n",
        "\n",
        "### Dataset Preparation\n",
        "- Loads the file `cleaned_python_commit_dataset.csv` from the `data/` directory.  \n",
        "  This CSV contains:\n",
        "  - A column named `diff` with raw Git diffs (code changes).\n",
        "  - A column named `commit_message` with the corresponding human-written messages.\n",
        "  \n",
        "- The dataset is read into a Pandas DataFrame. Then:\n",
        "  - Any missing values are dropped.\n",
        "  - Both `diff` and `commit_message` columns are explicitly converted to strings for consistency.\n",
        "\n",
        "### Data Splitting\n",
        "- Performs an 80/20 train-validation split, matching the setup used during training.\n",
        "- The validation set (`val_data`) is used for evaluation.\n",
        "\n",
        "### Random Sample Inference\n",
        "- Randomly selects `num_samples` (default = 5) from the validation set.\n",
        "- For each sample:\n",
        "  - The model generates a commit message using temperature sampling (`temperature=0.9`, `top_k=100`).\n",
        "  - The original Git diff, true commit message, and predicted message are printed for comparison.\n",
        "\n",
        "### Purpose\n",
        "This process provides a **qualitative evaluation** of the model’s performance and its ability to generalize to unseen Git diffs. Sampling-based decoding allows observation of **variability and creativity** in model outputs.\n",
        "\n",
        "> Requires `generate_commit_message()`, `model`, `custom_tokenizer`, and `device` to be defined beforehand.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ryHE6jiLe4M"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Load the same cleaned dataset used during training\n",
        "df = pd.read_csv(\"data/cleaned_python_commit_dataset.csv\")\n",
        "\n",
        "# Convert all diffs and messages to strings and drop rows with NaNs\n",
        "df = df.dropna(subset=[\"diff\", \"commit_message\"])\n",
        "df[\"diff\"] = df[\"diff\"].astype(str)\n",
        "df[\"commit_message\"] = df[\"commit_message\"].astype(str)\n",
        "\n",
        "data = list(zip(df[\"diff\"].tolist(), df[\"commit_message\"].tolist()))\n",
        "\n",
        "# Reproduce the same 80/20 split used in training\n",
        "random.seed(42)\n",
        "random.shuffle(data)\n",
        "split_idx = int(0.8 * len(data))\n",
        "val_data = data[split_idx:]\n",
        "\n",
        "# Set number of samples to test\n",
        "num_samples = 5\n",
        "random.seed()  # Use non-fixed seed for varied results\n",
        "\n",
        "# Run inference on multiple random samples\n",
        "for i in range(num_samples):\n",
        "    sample_diff, true_msg = random.choice(val_data)\n",
        "    predicted_msg = generate_commit_message(\n",
        "        model,\n",
        "        custom_tokenizer,\n",
        "        sample_diff,\n",
        "        device=device,\n",
        "        temperature=0.9,\n",
        "        top_k=100\n",
        "    )\n",
        "\n",
        "    print(f\"\\n======== SAMPLE {i+1} ========\")\n",
        "    print(\"Git Diff:\\n\", sample_diff.strip())\n",
        "    print(\"\\nGround Truth Commit Message:\\n\", true_msg.strip())\n",
        "    print(\"\\nGenerated Commit Message:\\n\", predicted_msg.strip())\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "-1.-1.-1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
