# ðŸ§  Gated Recurrent Unit for Git Commit Message Generation

This project trains and evaluates a baseline GRU-based language model to generate commit messages from Git diffs. The model uses a custom-trained Byte Pair Encoding (BPE) tokenizer and is evaluated using both standard NLP metrics and LLM-based factual scoring (RAGAs).

---

## Directory Structure
<pre>
gated-recurrent-unit/
â”‚
â”œâ”€â”€ custom_bpe_tokenizer.json # Custom tokenizer trained on commit message data
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ cleaned_python_commit_dataset.csv # Preprocessed dataset of Git diffs and commit messages created from custom data pipeline
â”‚
â”œâ”€â”€ trained_model/
â”‚ â””â”€â”€ gru_model.pt # Final trained GRU model weights
â”‚
â”œâ”€â”€ evaluation-metrics/
â”‚ â”œâ”€â”€ all_gru_diffs.txt # Inputs and generated outputs for 100 samples in readable format
â”‚ â”œâ”€â”€ sample_metrics_with_...json # Full JSON with BLEU, METEOR, ROUGE, BERTScore, RAGAs
â”‚ â”œâ”€â”€ gru_loss_curve.png # Training loss over epochs
â”‚ â”œâ”€â”€ gru_bleu4_distribution.png # BLEU-4 histogram
â”‚ â”œâ”€â”€ gru_meteor_distribution.png # METEOR score histogram
â”‚ â”œâ”€â”€ gru_rouge_l_distribution.png # ROUGE-L histogram
â”‚ â”œâ”€â”€ gru_bert_score_f1_distribution.png # BERTScore F1 histogram
â”‚ â”œâ”€â”€ gru_answer_correctness_distribution.png # RAGAs factual correctness histogram
â”‚
â”œâ”€â”€ Training-Git-Commit-GRU.ipynb # Notebook to train the GRU model
â”œâ”€â”€ Inference_for_Git_Commit_GRU.ipynb # Notebook to load model and generate predictions
â”œâ”€â”€ Performance_Evaluation__Git_Commit_GRU.ipynb # Notebook to compute and visualize evaluation metrics
</pre>

