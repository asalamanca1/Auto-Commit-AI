{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Getting Git Commit messages with their corresponding URLs\n",
        "\n",
        "## What the Code Does\n",
        "- **Session with Retry Logic:**  \n",
        "  Creates an HTTP session configured with a retry strategy to handle common HTTP errors and API rate limits.\n",
        "\n",
        "- **Helper Function (`safe_get`):**  \n",
        "  Implements a GET request helper that retries on errors and waits for the GitHub API rate-limit reset if needed.\n",
        "\n",
        "- **Fetching Commits (`fetch_commits`):**  \n",
        "  Retrieves commit data from the specified GitHub repository using paginated API calls.\n",
        "\n",
        "- **Filtering Commits (`is_python_only_commit`):**  \n",
        "  Checks if a commit modifies only Python files by examining if every file in the commit ends with `.py`.\n",
        "\n",
        "- **CSV Writing:**  \n",
        "  Opens (or creates) a CSV file (`python_only_commits.csv`), writes a header if the file is new, and appends commits that have:\n",
        "  - A commit message between 50 and 300 characters.\n",
        "  - Only Python file changes.\n",
        "  Flushes the CSV to ensure data is saved incrementally.\n",
        "\n",
        "## How to Use It\n",
        "1. **Configure Your Environment:**\n",
        "   - Replace the placeholder GitHub API token with your own token.\n",
        "   - Optionally, update the `repos` list to include any additional repositories in the `\"owner/repo\"` format.\n",
        "\n",
        "2. **Run the Code:**\n",
        "   - Execute the cell to begin fetching commits from the listed repositories.\n",
        "   - Monitor the progress via the `tqdm` progress bars for repositories and commits.\n",
        "\n",
        "3. **Review the Output:**\n",
        "   - The CSV file named `python_only_commits.csv` will be created (or updated), containing the commit URL and commit message for each valid Python-only commit.\n"
      ],
      "metadata": {
        "id": "PiqUAzck5jE7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLgZ7HQs48Lk"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import csv\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# GitHub API token\n",
        "GITHUB_TOKEN = ''\n",
        "\n",
        "# Headers for GitHub API requests\n",
        "headers = {\n",
        "    'Authorization': f'token {GITHUB_TOKEN}',\n",
        "    'Accept': 'application/vnd.github.v3+json'\n",
        "}\n",
        "\n",
        "repos = [\n",
        "\n",
        "    # Store list of repos\n",
        "\n",
        "    #EXAMPLE:\n",
        "    # 'discord/flask-oauthlib',\n",
        "    # 'discord/chromium-build',\n",
        "    # 'discord/node-gyp',\n",
        "    # 'discord/luigi',\n",
        "    # 'discord/hyper-h2'\n",
        "\n",
        "]\n",
        "\n",
        "# Create a persistent session with retry logic for common HTTP errors\n",
        "session = requests.Session()\n",
        "retry_strategy = Retry(\n",
        "    total=5,\n",
        "    backoff_factor=1,\n",
        "    status_forcelist=[500, 502, 503, 504],\n",
        "    allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
        ")\n",
        "adapter = HTTPAdapter(max_retries=retry_strategy)\n",
        "session.mount(\"https://\", adapter)\n",
        "session.mount(\"http://\", adapter)\n",
        "\n",
        "def safe_get(url, headers, retries=3, delay=2):\n",
        "    \"\"\"\n",
        "    A helper function to perform a GET request with GitHub API rate-limit handling.\n",
        "    It retries the request (up to 'retries' times) and, if a rate-limit (HTTP 403 with X-RateLimit-Remaining \"0\")\n",
        "    is encountered, waits until the reset time.\n",
        "    \"\"\"\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = session.get(url, headers=headers, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                return response\n",
        "            elif response.status_code == 403:\n",
        "                remaining = response.headers.get(\"X-RateLimit-Remaining\")\n",
        "                if remaining == \"0\":\n",
        "                    reset_timestamp = int(response.headers.get(\"X-RateLimit-Reset\", 0))\n",
        "                    current_time = int(time.time())\n",
        "                    wait_time = reset_timestamp - current_time + 5  # add a small buffer\n",
        "                    if wait_time > 0:\n",
        "                        print(f\"Rate limit exceeded. Waiting for {wait_time} seconds before retrying {url}...\")\n",
        "                        time.sleep(wait_time)\n",
        "                        continue  # After waiting, try again\n",
        "                else:\n",
        "                    print(f\"Attempt {attempt+1}: Received 403 for {url} (not due to rate limit).\")\n",
        "            else:\n",
        "                print(f\"Attempt {attempt+1}: Non-200 response ({response.status_code}) for {url}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt+1}: Error fetching {url}: {e}\")\n",
        "        time.sleep(delay)\n",
        "    return None\n",
        "\n",
        "def fetch_commits(repo, per_page=100, max_pages=100):\n",
        "    \"\"\"\n",
        "    Fetch commit data for the given repository using safe_get for rate-limit handling.\n",
        "    \"\"\"\n",
        "    commits = []\n",
        "    for page in range(1, max_pages + 1):\n",
        "        url = f'https://api.github.com/repos/{repo}/commits?per_page={per_page}&page={page}'\n",
        "        response = safe_get(url, headers)\n",
        "        # Stop fetching if no more commits or an error occurred.\n",
        "        if not response or not response.json():\n",
        "            print(f\"No more commits in {repo} or encountered an error.\")\n",
        "            break\n",
        "        commits.extend(response.json())\n",
        "        time.sleep(0.5)  # Small delay to reduce rate-limit issues further\n",
        "    return commits\n",
        "\n",
        "def is_python_only_commit(commit_url):\n",
        "    \"\"\"\n",
        "    Check if the commit (accessed via its URL) includes only Python (.py) files.\n",
        "    Uses safe_get for rate-limit handling.\n",
        "    \"\"\"\n",
        "    response = safe_get(commit_url, headers)\n",
        "    if not response or response.status_code != 200:\n",
        "        return False\n",
        "\n",
        "    files = response.json().get('files', [])\n",
        "    if not files:\n",
        "        return False\n",
        "\n",
        "    return all(file['filename'].endswith('.py') for file in files)\n",
        "\n",
        "csv_filename = 'python_only_commits.csv'\n",
        "print(\"Fetching commits that only include Python files...\")\n",
        "\n",
        "# Check if the CSV file exists already (to avoid rewriting headers)\n",
        "file_exists = os.path.isfile(csv_filename)\n",
        "\n",
        "# Open CSV file in append mode to save results incrementally\n",
        "with open(csv_filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "\n",
        "    # If file doesn't exist, write headers first\n",
        "    if not file_exists:\n",
        "        writer.writerow(['commit_url', 'commit_message'])\n",
        "\n",
        "    # Iterate over each repository\n",
        "    for repo in tqdm(repos, desc=\"Repositories\"):\n",
        "        commits = fetch_commits(repo)\n",
        "\n",
        "        for commit_info in tqdm(commits, desc=f\"Commits in {repo}\"):\n",
        "            commit_url = commit_info['url']\n",
        "            commit_message = commit_info['commit']['message']\n",
        "\n",
        "            # Ensure commit message is within length limits\n",
        "            if 50 <= len(commit_message) <= 300:\n",
        "                if is_python_only_commit(commit_url):\n",
        "                    writer.writerow([commit_url, commit_message])\n",
        "                    csvfile.flush()  # Ensure that each commit is saved immediately\n",
        "\n",
        "print(\"Incremental fetching complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Enriching Commits with Diff Data via Concurrent Fetching\n",
        "\n",
        "## What the Code Does\n",
        "\n",
        "- **Session & Retry Setup:**  \n",
        "  - Creates a HTTP session with a retry strategy (using `HTTPAdapter` and `Retry`) to handle network errors and GitHub API rate limits.\n",
        "\n",
        "- **Diff Fetching Function:**  \n",
        "  - **`fetch_diff`:**  \n",
        "    - Fetches the diff (the code changes) for a given commit URL.\n",
        "    - Implements retry logic and handles rate limiting by checking for HTTP 403 responses. If the rate limit is exceeded, it waits until the reset time (plus a small buffer) before retrying the request.\n",
        "\n",
        "- **Data Loading & Processing:**  \n",
        "  - Loads previously saved commit details from the CSV file (`python_only_commits.csv`) generated in Cell 1.\n",
        "  - **`process_commit`:**  \n",
        "    - Retrieves diff information for each commit by using the `fetch_diff` function.\n",
        "    - Packages the commit message along with its corresponding diff.\n",
        "\n",
        "- **Concurrent Execution & Output:**  \n",
        "  - Uses a `ThreadPoolExecutor` to fetch diff data concurrently for all commits, which significantly improves processing speed.\n",
        "  - Writes the resulting commit messages and diffs incrementally to a new CSV file (`python_commit_dataset.csv`), ensuring that each row is immediately flushed to disk.\n",
        "\n",
        "## How to Use It\n",
        "\n",
        "1. **Input Requirements:**  \n",
        "   - Make sure that the CSV file `python_only_commits.csv` (produced by Cell 1) is present.\n",
        "   - Update the `GITHUB_TOKEN` variable with a valid GitHub token if needed.\n",
        "\n",
        "2. **Execution:**  \n",
        "   - Run this cell after Cell 1 has been executed and the initial dataset has been created.\n",
        "   - The code will concurrently process each commit, fetching its diff and combining it with the commit message.\n",
        "\n",
        "3. **Output:**  \n",
        "   - A new CSV file named `python_commit_dataset.csv` is generated.\n",
        "   - This file contains two columns: `commit_message` and `diff`, providing deeper insights into the code changes for each commit.\n",
        "\n",
        "   \n"
      ],
      "metadata": {
        "id": "TAfPm9M96b9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "import concurrent.futures  # For faster parallel processing\n",
        "import time\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# GitHub API token\n",
        "GITHUB_TOKEN = ''\n",
        "\n",
        "# Create a persistent session with retry logic for common HTTP errors\n",
        "session = requests.Session()\n",
        "retry_strategy = Retry(\n",
        "    total=5,\n",
        "    backoff_factor=1,\n",
        "    status_forcelist=[500, 502, 503, 504],\n",
        "    allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
        ")\n",
        "adapter = HTTPAdapter(max_retries=retry_strategy)\n",
        "session.mount(\"https://\", adapter)\n",
        "session.mount(\"http://\", adapter)\n",
        "\n",
        "def fetch_diff(commit_url, retries=3, delay=2):\n",
        "    headers_diff = {\n",
        "        'Authorization': f'token {GITHUB_TOKEN}',\n",
        "        'Accept': 'application/vnd.github.v3.diff'\n",
        "    }\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = session.get(commit_url, headers=headers_diff, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                return response.text\n",
        "            elif response.status_code == 403:\n",
        "                # Check if rate limit has been exceeded\n",
        "                remaining = response.headers.get(\"X-RateLimit-Remaining\")\n",
        "                if remaining == \"0\":\n",
        "                    reset_timestamp = int(response.headers.get(\"X-RateLimit-Reset\", 0))\n",
        "                    current_time = int(time.time())\n",
        "                    wait_time = reset_timestamp - current_time + 5  # add a small buffer\n",
        "                    if wait_time > 0:\n",
        "                        print(f\"Rate limit exceeded. Waiting for {wait_time} seconds before retrying {commit_url}...\")\n",
        "                        time.sleep(wait_time)\n",
        "                        continue  # After waiting, retry the request\n",
        "                else:\n",
        "                    print(f\"Attempt {attempt+1}: Non-200 response ({response.status_code}) for {commit_url}\")\n",
        "            else:\n",
        "                print(f\"Attempt {attempt+1}: Non-200 response ({response.status_code}) for {commit_url}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt+1}: Error fetching {commit_url}: {e}\")\n",
        "        time.sleep(delay)\n",
        "    return None\n",
        "\n",
        "# Load previously saved commits from CSV file\n",
        "commit_data = []\n",
        "with open('python_only_commits.csv', 'r', encoding='utf-8') as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    for row in reader:\n",
        "        commit_data.append(row)\n",
        "\n",
        "def process_commit(commit):\n",
        "    diff = fetch_diff(commit['commit_url'])\n",
        "    if diff:\n",
        "        return {\n",
        "            'commit_message': commit['commit_message'],\n",
        "            'diff': diff\n",
        "        }\n",
        "    return None\n",
        "\n",
        "print(\"Fetching commit diffs...\")\n",
        "\n",
        "# Open the CSV file in write mode with proper quoting and line terminator.\n",
        "with open('python_commit_dataset.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile, quoting=csv.QUOTE_ALL, lineterminator='\\n')\n",
        "    writer.writerow(['commit_message', 'diff'])\n",
        "\n",
        "    # Use ThreadPoolExecutor for concurrent fetching.\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        futures = [executor.submit(process_commit, commit) for commit in commit_data]\n",
        "\n",
        "        # Use tqdm to display progress as futures complete.\n",
        "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
        "            result = future.result()\n",
        "            if result:\n",
        "                writer.writerow([result['commit_message'], result['diff']])\n",
        "                csvfile.flush()  # Flush after writing each row to save incrementally\n",
        "\n",
        "print(\"Final dataset created with git diffs and commits.\")\n"
      ],
      "metadata": {
        "id": "MElLCCIT6hZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Cleaning Commit Messages for Transformer Training\n",
        "\n",
        "## What the Code Does\n",
        "- **Set CSV Field Limit:**  \n",
        "  - Adjusts the field size limit so large text fields are processed without issues.\n",
        "- **Load Dataset:**  \n",
        "  - Reads `python_commit_dataset.csv` using pandas.\n",
        "- **Clean Commit Messages:**  \n",
        "  - Removes merge pull request phrases (e.g., \"Merge pull request #… from …\").\n",
        "  - Eliminates lines containing \"PiperOrigin-RevId\" and \"Signed-off-by\" to strip out non-informative metadata.\n",
        "- **Save Cleaned Data:**  \n",
        "  - Writes the cleaned dataset to `cleaned_python_commit_dataset.csv` with proper quoting.\n",
        "\n",
        "## How to Use It\n",
        "1. Make sure `python_commit_dataset.csv` is in your working directory.\n",
        "2. Run this cell to clean the commit messages.\n",
        "3. Use the resulting `cleaned_python_commit_dataset.csv` for transformer training.\n"
      ],
      "metadata": {
        "id": "2LMKh0YF63EV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import sys\n",
        "import csv\n",
        "\n",
        "# Increase CSV field size limit to handle large fields properly\n",
        "csv.field_size_limit(sys.maxsize)\n",
        "\n",
        "# Load your original dataset using pandas robustly\n",
        "df = pd.read_csv(\n",
        "    'python_commit_dataset.csv',\n",
        "    engine='python',\n",
        "    on_bad_lines='skip'\n",
        ")\n",
        "\n",
        "def clean_message(msg):\n",
        "    if pd.isna(msg):\n",
        "        return \"\"\n",
        "    # Ensure we're working with a string\n",
        "    msg = str(msg)\n",
        "\n",
        "    # Remove merge pull request phrases anywhere in the message.\n",
        "    # This pattern matches: \"Merge pull request\" then whitespace,\n",
        "    # then a '#' followed by non-whitespace, then whitespace,\n",
        "    # then \"from\" and another whitespace and non-whitespace branch name.\n",
        "    merge_pr_pattern = r\"Merge\\s+pull\\s+request\\s+#\\S+\\s+from\\s+\\S+\"\n",
        "    msg = re.sub(merge_pr_pattern, \"\", msg, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove unwanted lines containing internal metadata\n",
        "    unwanted_patterns = [\"PiperOrigin-RevId\", \"Signed-off-by\", \"Signed-Off-By:\", \"Signed-off by\", \"Signed-off-by\"]\n",
        "    msg = \"\\n\".join(\n",
        "        line for line in msg.splitlines() if not any(up in line for up in unwanted_patterns)\n",
        "    ).strip()\n",
        "\n",
        "    return msg\n",
        "\n",
        "# Apply cleaning function to the commit_message column\n",
        "df['commit_message'] = df['commit_message'].apply(clean_message)\n",
        "\n",
        "# Save cleaned dataset with proper CSV quoting\n",
        "df.to_csv('cleaned_python_commit_dataset.csv', index=False, quoting=csv.QUOTE_ALL)\n"
      ],
      "metadata": {
        "id": "UjyjHIsd679F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Verifying Dataset Integrity\n",
        "\n",
        "- **Loads Both Datasets:**  \n",
        "  - Reads the original dataset (`python_commit_dataset.csv`) and the cleaned dataset (`cleaned_python_commit_dataset.csv`) using pandas.\n",
        "- **Compares Row Counts:**  \n",
        "  - Prints the row count for each to verify no rows were lost or duplicated during cleaning.\n",
        "\n"
      ],
      "metadata": {
        "id": "aHxvZ5xG7EEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import sys\n",
        "import csv\n",
        "\n",
        "# Increase CSV field size limit (helps pandas handle large fields)\n",
        "csv.field_size_limit(sys.maxsize)\n",
        "\n",
        "# Load original dataset robustly (using pandas' reliable parsing options)\n",
        "original_df = pd.read_csv(\n",
        "    'python_commit_dataset.csv',\n",
        "    engine='python',\n",
        "    on_bad_lines='skip'\n",
        ")\n",
        "\n",
        "# Load modified (cleaned) dataset using pandas\n",
        "modified_df = pd.read_csv(\n",
        "    'cleaned_python_commit_dataset.csv',\n",
        "    engine='python',\n",
        "    on_bad_lines='skip'\n",
        ")\n",
        "\n",
        "# Display row counts clearly\n",
        "print(\"Row count comparison (using pandas):\")\n",
        "print(f\"- Original dataset: {original_df.shape[0]} rows\")\n",
        "print(f\"- Modified dataset: {modified_df.shape[0]} rows\")\n",
        "\n",
        "if original_df.shape[0] == modified_df.shape[0]:\n",
        "    print(\"Row count matches. Cleaning did not alter row count.\")\n",
        "else:\n",
        "    print(\"Row count mismatch. Cleaning altered row count!\")\n"
      ],
      "metadata": {
        "id": "eLzFdOTG7TtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Download the Cleaned Dataset"
      ],
      "metadata": {
        "id": "Gw7_Dtlf8AVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download cleaned dataset\n",
        "files.download('cleaned_python_commit_dataset.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "d2NpqYi77_nK",
        "outputId": "4c80e32d-7f6b-4faf-fcd0-784883052218"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4ea049dd-8a45-4993-aca7-672efb9b7561\", \"cleaned_python_commit_dataset.csv\", 522992)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}