{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKCLgQFYWFVW"
      },
      "source": [
        "## 1. Load the trained tokenizer\n",
        "\n",
        "Load the custom BPE tokenizer that was trained and saved in the previous step (`custom_bpe_tokenizer.json`) using Hugging Face’s `PreTrainedTokenizerFast`.\n",
        "\n",
        "### Special Tokens Added:\n",
        "- `<pad>` — used for padding sequences during batching\n",
        "- `<endOfCommitMessage>` — tells the model where the commit message ends\n",
        "- `<endOfDiff>` — separates the git diff from the commit message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kosOOxmCWH2P",
        "outputId": "55a9d95d-3120-4444-ff23-125c3f5a26e1"
      },
      "outputs": [],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "custom_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"custom_bpe_tokenizer.json\")\n",
        "custom_tokenizer.add_special_tokens({\n",
        "    \"pad_token\": \"<pad>\",\n",
        "    \"eos_token\": \"<endOfCommitMessage>\"\n",
        "})\n",
        "custom_tokenizer.add_tokens([\"<endOfDiff>\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agS7n9fwDvc_"
      },
      "source": [
        "## 2. Load Trained Model and Generate Commit Messages\n",
        "\n",
        "This cell loads your trained GPT-2 model and tokenizer, then sets up a function to generate commit messages from Git diffs.\n",
        "\n",
        "- Uses GPU if available.\n",
        "- Loads the model from the `trained_model/` folder.\n",
        "  - `<pad>`, `<endOfDiff>`, `<endOfCommitMessage>`\n",
        "- Defines `generate_commit_message()`:\n",
        "  - Takes in a Git diff.\n",
        "  - Adds `<endOfDiff>` to separate the input.\n",
        "  - Uses greedy decoding (with repetition penalty and no repeated 3-grams).\n",
        "  - Stops when it hits `<endOfCommitMessage>` or the max token limit.\n",
        "  - Returns just the commit message part of the output.\n",
        "\n",
        "Make sure the model and tokenizer files are in the current directory before running this.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1a2k5klDUAM",
        "outputId": "cb406031-3ade-4f8f-ed57-c302e6ee10a2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast\n",
        "\n",
        "# Choose device (GPU if available)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load \"trained_model\" folder from directory that contains both the model weights and configuration.\n",
        "model = GPT2LMHeadModel.from_pretrained(\"trained_model\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Load the custom tokenizer from its JSON file.\n",
        "custom_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"custom_bpe_tokenizer.json\")\n",
        "# Add special tokens\n",
        "custom_tokenizer.add_special_tokens({\n",
        "    \"pad_token\": \"<pad>\",\n",
        "    \"eos_token\": \"<endOfCommitMessage>\"\n",
        "})\n",
        "custom_tokenizer.add_tokens([\"<endOfDiff>\"])\n",
        "\n",
        "# Inference function to generate a commit message given a git diff.\n",
        "def generate_commit_message(model, tokenizer, git_diff_text, max_length=100, device='cuda'):\n",
        "    model.eval()  # Ensure the model is in evaluation mode.\n",
        "    separator = \"<endOfDiff>\"\n",
        "    eos_token = \"<endOfCommitMessage>\"\n",
        "\n",
        "    # Build the prompt by concatenating the diff with the separator.\n",
        "    prompt = git_diff_text + separator\n",
        "    input_ids = tokenizer.encode(prompt)\n",
        "    input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
        "\n",
        "    # Get the token id for the EOS token\n",
        "    eos_token_id = tokenizer.encode(eos_token)[0]\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(\n",
        "            input_tensor,\n",
        "            max_length=len(input_ids) + max_length,\n",
        "            pad_token_id=pad_token_id,\n",
        "            eos_token_id=eos_token_id,\n",
        "            do_sample=False  # Greedy decoding; change to True for sampling\n",
        "            , repetition_penalty=1.2, no_repeat_ngram_size=3\n",
        "        )[0].tolist()\n",
        "\n",
        "    generated_text = tokenizer.decode(generated_ids)\n",
        "    # Extract the commit message from the generated text.\n",
        "    if separator in generated_text:\n",
        "        commit_msg_with_eos = generated_text.split(separator, 1)[1].strip()\n",
        "        commit_msg = commit_msg_with_eos.replace(eos_token, \"\").strip()\n",
        "    else:\n",
        "        commit_msg = generated_text.strip()\n",
        "    return commit_msg\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0zdNRyNDy3s"
      },
      "source": [
        "## 3. Run an Inference\n",
        "\n",
        "This cell runs the `generate_commit_message()` function on a test Git diff to see what commit message the model generates.\n",
        "\n",
        "- The example diff shows a `DEBUG` flag being changed from `False` to `True` in `config.py`.\n",
        "- The model processes the diff and returns a predicted commit message.\n",
        "\n",
        "Make sure the CSV file used for training is located in a folder named `data/` and is named `cleaned_python_commit_dataset.csv`.\n",
        "\n",
        "Use this to quickly test if your model and tokenizer are working as expected.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAZEEsr-D4Fj",
        "outputId": "d0d0bc76-a4a4-4d3a-8d2e-2eba03a5bb20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated commit message: Remove debugging flag from config file\n"
          ]
        }
      ],
      "source": [
        "# Run an inference on sample git diff\n",
        "test_diff = \"\"\"\n",
        "diff --git a/config.py b/config.py\n",
        "index 8aef123..9cdfaa1 100644\n",
        "--- a/config.py\n",
        "+++ b/config.py\n",
        "@@\n",
        "-DEBUG = False\n",
        "+DEBUG = True\n",
        "\"\"\"\n",
        "\n",
        "commit_msg = generate_commit_message(model, custom_tokenizer, test_diff, max_length=20, device=device)\n",
        "print(\"Generated commit message:\", commit_msg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uyiYA4I3YbJ"
      },
      "source": [
        "## 4. Run Inference on a Sample from the Test Set\n",
        "\n",
        "- This cell re-loads the original dataset from `data/cleaned_python_commit_dataset.csv`, applies the same 80/20 train/val split used during training, and runs inference on a random example from the validation set.\n",
        "\n",
        "- Make sure you're using the same tokenizer (`custom_bpe_tokenizer.json`) and that the CSV file is inside a folder named `data/`.\n",
        "\n",
        "This lets you evaluate how well the model generalizes to unseen examples from the original test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56eqNLgQ3YGz",
        "outputId": "e2817737-6196-4300-cf7b-998dc19914d8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Load the same cleaned dataset used during training\n",
        "df = pd.read_csv(\"data/cleaned_python_commit_dataset.csv\")\n",
        "data = list(zip(df[\"diff\"].tolist(), df[\"commit_message\"].tolist()))\n",
        "\n",
        "# Reproduce the same 80/20 split used in training\n",
        "random.seed(42)\n",
        "random.shuffle(data)\n",
        "split_idx = int(0.8 * len(data))\n",
        "val_data = data[split_idx:]\n",
        "\n",
        "# Pick a sample from the validation set\n",
        "sample_diff, true_msg = random.choice(val_data)\n",
        "\n",
        "# Run inference\n",
        "predicted_msg = generate_commit_message(model, custom_tokenizer, sample_diff, max_length=20, device=device)\n",
        "\n",
        "# Print results\n",
        "print(\"Git Diff:\\n\", sample_diff.strip())\n",
        "print(\"\\n Ground Truth Commit Message:\\n\", true_msg.strip())\n",
        "print(\"\\n Generated Commit Message:\\n\", predicted_msg.strip())\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
