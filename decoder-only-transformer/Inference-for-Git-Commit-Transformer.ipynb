{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load the trained tokenizer\n",
        "\n",
        "Load the custom BPE tokenizer that was trained and saved in the previous step (`custom_bpe_tokenizer.json`) using Hugging Face’s `PreTrainedTokenizerFast`.\n",
        "\n",
        "### Special Tokens Added:\n",
        "- `<pad>` — used for padding sequences during batching\n",
        "- `<endOfCommitMessage>` — tells the model where the commit message ends\n",
        "- `<endOfDiff>` — separates the git diff from the commit message"
      ],
      "metadata": {
        "id": "tKCLgQFYWFVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "custom_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"custom_bpe_tokenizer.json\")\n",
        "custom_tokenizer.add_special_tokens({\n",
        "    \"pad_token\": \"<pad>\",\n",
        "    \"eos_token\": \"<endOfCommitMessage>\"\n",
        "})\n",
        "custom_tokenizer.add_tokens([\"<endOfDiff>\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kosOOxmCWH2P",
        "outputId": "55a9d95d-3120-4444-ff23-125c3f5a26e1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load Trained Model and Generate Commit Messages\n",
        "\n",
        "This cell loads your trained GPT-2 model and tokenizer, then sets up a function to generate commit messages from Git diffs.\n",
        "\n",
        "- Uses GPU if available.\n",
        "- Loads the model from the `trained_model/` folder.\n",
        "  - `<pad>`, `<endOfDiff>`, `<endOfCommitMessage>`\n",
        "- Defines `generate_commit_message()`:\n",
        "  - Takes in a Git diff.\n",
        "  - Adds `<endOfDiff>` to separate the input.\n",
        "  - Uses greedy decoding (with repetition penalty and no repeated 3-grams).\n",
        "  - Stops when it hits `<endOfCommitMessage>` or the max token limit.\n",
        "  - Returns just the commit message part of the output.\n",
        "\n",
        "Make sure the model and tokenizer files are in the current directory before running this.\n"
      ],
      "metadata": {
        "id": "agS7n9fwDvc_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1a2k5klDUAM",
        "outputId": "cb406031-3ade-4f8f-ed57-c302e6ee10a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast\n",
        "\n",
        "# Choose device (GPU if available)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load \"trained_model\" folder from directory that contains both the model weights and configuration.\n",
        "model = GPT2LMHeadModel.from_pretrained(\"trained_model\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Load the custom tokenizer from its JSON file.\n",
        "custom_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"custom_bpe_tokenizer.json\")\n",
        "# Add special tokens\n",
        "custom_tokenizer.add_special_tokens({\n",
        "    \"pad_token\": \"<pad>\",\n",
        "    \"eos_token\": \"<endOfCommitMessage>\"\n",
        "})\n",
        "custom_tokenizer.add_tokens([\"<endOfDiff>\"])\n",
        "\n",
        "# Inference function to generate a commit message given a git diff.\n",
        "def generate_commit_message(model, tokenizer, git_diff_text, max_length=100, device='cuda'):\n",
        "    model.eval()  # Ensure the model is in evaluation mode.\n",
        "    separator = \"<endOfDiff>\"\n",
        "    eos_token = \"<endOfCommitMessage>\"\n",
        "\n",
        "    # Build the prompt by concatenating the diff with the separator.\n",
        "    prompt = git_diff_text + separator\n",
        "    input_ids = tokenizer.encode(prompt)\n",
        "    input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
        "\n",
        "    # Get the token id for the EOS token\n",
        "    eos_token_id = tokenizer.encode(eos_token)[0]\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(\n",
        "            input_tensor,\n",
        "            max_length=len(input_ids) + max_length,\n",
        "            pad_token_id=pad_token_id,\n",
        "            eos_token_id=eos_token_id,\n",
        "            do_sample=False  # Greedy decoding; change to True for sampling\n",
        "            , repetition_penalty=1.2, no_repeat_ngram_size=3\n",
        "        )[0].tolist()\n",
        "\n",
        "    generated_text = tokenizer.decode(generated_ids)\n",
        "    # Extract the commit message from the generated text.\n",
        "    if separator in generated_text:\n",
        "        commit_msg_with_eos = generated_text.split(separator, 1)[1].strip()\n",
        "        commit_msg = commit_msg_with_eos.replace(eos_token, \"\").strip()\n",
        "    else:\n",
        "        commit_msg = generated_text.strip()\n",
        "    return commit_msg\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Run an Inference\n",
        "\n",
        "This cell runs the `generate_commit_message()` function on a test Git diff to see what commit message the model generates.\n",
        "\n",
        "- The example diff shows a `DEBUG` flag being changed from `False` to `True` in `config.py`.\n",
        "- The model processes the diff and returns a predicted commit message.\n",
        "\n",
        "Make sure the CSV file used for training is located in a folder named `data/` and is named `cleaned_python_commit_dataset.csv`.\n",
        "\n",
        "Use this to quickly test if your model and tokenizer are working as expected.\n"
      ],
      "metadata": {
        "id": "P0zdNRyNDy3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run an inference on sample git diff\n",
        "test_diff = \"\"\"\n",
        "diff --git a/config.py b/config.py\n",
        "index 8aef123..9cdfaa1 100644\n",
        "--- a/config.py\n",
        "+++ b/config.py\n",
        "@@\n",
        "-DEBUG = False\n",
        "+DEBUG = True\n",
        "\"\"\"\n",
        "\n",
        "commit_msg = generate_commit_message(model, custom_tokenizer, test_diff, max_length=20, device=device)\n",
        "print(\"Generated commit message:\", commit_msg)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAZEEsr-D4Fj",
        "outputId": "d0d0bc76-a4a4-4d3a-8d2e-2eba03a5bb20"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated commit message: Remove debugging flag from config file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Run Inference on a Sample from the Test Set\n",
        "\n",
        "- This cell re-loads the original dataset from `data/cleaned_python_commit_dataset.csv`, applies the same 80/20 train/val split used during training, and runs inference on a random example from the validation set.\n",
        "\n",
        "- Make sure you're using the same tokenizer (`custom_bpe_tokenizer.json`) and that the CSV file is inside a folder named `data/`.\n",
        "\n",
        "This lets you evaluate how well the model generalizes to unseen examples from the original test set.\n"
      ],
      "metadata": {
        "id": "0uyiYA4I3YbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Load the same cleaned dataset used during training\n",
        "df = pd.read_csv(\"data/cleaned_python_commit_dataset.csv\")\n",
        "data = list(zip(df[\"diff\"].tolist(), df[\"commit_message\"].tolist()))\n",
        "\n",
        "# Reproduce the same 80/20 split used in training\n",
        "random.seed(42)\n",
        "random.shuffle(data)\n",
        "split_idx = int(0.8 * len(data))\n",
        "val_data = data[split_idx:]\n",
        "\n",
        "# Pick a sample from the validation set\n",
        "sample_diff, true_msg = random.choice(val_data)\n",
        "\n",
        "# Run inference\n",
        "predicted_msg = generate_commit_message(model, custom_tokenizer, sample_diff, max_length=20, device=device)\n",
        "\n",
        "# Print results\n",
        "print(\"Git Diff:\\n\", sample_diff.strip())\n",
        "print(\"\\n Ground Truth Commit Message:\\n\", true_msg.strip())\n",
        "print(\"\\n Generated Commit Message:\\n\", predicted_msg.strip())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56eqNLgQ3YGz",
        "outputId": "e2817737-6196-4300-cf7b-998dc19914d8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git Diff:\n",
            " diff --git a/tcfl/tc_clear_bbt.py b/tcfl/tc_clear_bbt.py\n",
            "index 700f9632..075084e2 100644\n",
            "--- a/tcfl/tc_clear_bbt.py\n",
            "+++ b/tcfl/tc_clear_bbt.py\n",
            "@@ -277,9 +277,9 @@ class tc_clear_bbt_c(tcfl.tc.tc_c):\n",
            "     \"\"\"\n",
            " \n",
            "     def __init__(self, path, t_file_path):\n",
            "-        tcfl.tc.tc_c.__init__(self, commonl.name_make_safe(path),\n",
            "+        tcfl.tc.tc_c.__init__(self, path,\n",
            "                               # these two count as the ones that started this\n",
            "-                              path, t_file_path)\n",
            "+                              t_file_path, t_file_path)\n",
            "         # t_file_path goes to self.kws['thisfile'], name to self.name\n",
            "         # and self.kws['tc_name']\n",
            "         self.rel_path_in_target = None\n",
            "@@ -421,12 +421,14 @@ def start(self, ic, target):\n",
            "         # take it in. It has a bundle per line.\n",
            "         bundles = [\n",
            "             # always needs this, that installs 'bats'\n",
            "-            'dev-utils'\n",
            "+            'os-testsuite'\n",
            "         ]\n",
            "         requirements_fname = os.path.join(self.kws['srcdir'], 'requirements')\n",
            "         if os.path.exists(requirements_fname):\n",
            "             bundles += open(requirements_fname).read().split()\n",
            "-\n",
            "+        self.report_info(\"Bundle requirements: %s\" % \" \".join(bundles),\n",
            "+                         dlevel = 1)\n",
            "+        \n",
            "         # if there is no distro mirror, use proxies -- HACK\n",
            "         proxy_cmd = \"\"\n",
            "         if 'http_proxy' in ic.kws:\n",
            "\n",
            " Ground Truth Commit Message:\n",
            " tcfl/clear-bbt: fix base bundle, req reading and path\n",
            "\n",
            " Generated Commit Message:\n",
            " tcfl/clear-bbt: fix reporting of errors on symlink paths\n"
          ]
        }
      ]
    }
  ]
}