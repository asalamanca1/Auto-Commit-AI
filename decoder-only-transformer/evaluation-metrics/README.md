# Evaluation Metrics

This folder contains the outputs generated by `Performance-Evaluation-for-Git-Commit-Transformer.ipynb`, which evaluates model-generated Git commit messages against ground truth using a variety of metrics.

## Contents

### 1. `sample_metrics_with_ragas_and_bertscore.json`
- **Description**: A JSON array of 100 evaluation results (one per sample).
- **Fields per sample**:
  - `diff`: the Git diff input.
  - `generated_commit`: the model-generated commit message.
  - `label_commit`: the ground-truth commit message.
  - `bleu`: BLEU-4 score.
  - `rouge_l`: ROUGE-L F1 score.
  - `meteor`: METEOR score.
  - `bert_score_f1`: BERTScore F1 metric.
  - `answer_correctness`: RAGAs Answer Correctness score.

---

### 2. `all_diffs.txt`
- **Description**: A readable text file with all 100 evaluation samples.
- **Format**: Each block includes the sample number, generated and true commit messages, all metric scores, and the input diff.

---

### 3. Distribution Plots (PNG Files)
Each of the following plots shows the distribution of one metric across the 100 evaluation samples:

- `bleu4_distribution.png`: Histogram of BLEU-4 scores.
- `rouge_l_distribution.png`: Histogram of ROUGE-L F1 scores.
- `meteor_distribution.png`: Histogram of METEOR scores.
- `bert_score_f1_distribution.png`: Histogram of BERTScore F1 values.
- `answer_correctness_distribution.png`: Histogram of RAGAs Answer Correctness scores.

---

## Notes

- All evaluations are performed using the `safe_generate(...)` function to generate commit messages, followed by metric comparisons.
- The dataset is sampled from `cleaned_python_commit_dataset.csv`.
- The script also handles retries in case RAGAs API calls fail.

