[
  {
    "diff": "diff --git a/cron/backupSnapshot.py b/cron/backupSnapshot.py\nindex 732363a2e..8405a2b12 100755\n--- a/cron/backupSnapshot.py\n+++ b/cron/backupSnapshot.py\n@@ -36,7 +36,6 @@\n from os.path import expanduser\n \n logger = logging.getLogger(sys.argv[0])\n-logger.level=logging.DEBUG\n formatter = logging.Formatter('%(asctime)s %(name)s %(levelname)s %(message)s')\n \n \n",
    "generated_commit": "Remove unused variable",
    "label_commit": "Modify logger level in backup snapshot cron",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": 0.043898385018110275,
    "answer_correctness": 0.1741429590154255
  },
  {
    "diff": "diff --git a/airflow/operators/mysql_to_gcs.py b/airflow/operators/mysql_to_gcs.py\nindex b2282ab14d7cb..4ce135ee87789 100644\n--- a/airflow/operators/mysql_to_gcs.py\n+++ b/airflow/operators/mysql_to_gcs.py\n@@ -113,6 +113,8 @@ def convert_type(self, value, schema_type):\n         :param schema_type: BigQuery data type\n         :type schema_type: str\n         \"\"\"\n+        if value is None:\n+            return value\n         if isinstance(value, (datetime, date)):\n             return calendar.timegm(value.timetuple())\n         if isinstance(value, timedelta):\ndiff --git a/tests/operators/test_mysql_to_gcs.py b/tests/operators/test_mysql_to_gcs.py\nindex dd4d394d4e40c..a9a259045b5b4 100644\n--- a/tests/operators/test_mysql_to_gcs.py\n+++ b/tests/operators/test_mysql_to_gcs.py\n@@ -88,6 +88,7 @@ def test_init(self):\n         (datetime.datetime(1970, 1, 1, 1, 0), None, 3600),\n         (decimal.Decimal(5), None, 5),\n         (b\"bytes\", \"BYTES\", \"Ynl0ZXM=\"),\n+        (None, \"BYTES\", None)\n     ])\n     def test_convert_type(self, value, schema_type, expected):\n         op = MySqlToGoogleCloudStorageOperator(\n",
    "generated_commit": "[AIRFLOW-XXX] Fix MySQL to work with empty value (#5033)",
    "label_commit": "[AIRFLOW-6246] Add support None in mysql_to_gcs (#6808)",
    "bleu": 0,
    "rouge_l": 0.3,
    "meteor": 0.0,
    "bert_score_f1": 0.37801292538642883,
    "answer_correctness": 0.42102323067533387
  },
  {
    "diff": "diff --git a/mkt/webapps/serializers.py b/mkt/webapps/serializers.py\nindex 643be563f7e..f3abbe19e0a 100644\n--- a/mkt/webapps/serializers.py\n+++ b/mkt/webapps/serializers.py\n@@ -547,8 +547,8 @@ class ESAppFeedSerializer(BaseESAppFeedSerializer):\n     class Meta(ESAppSerializer.Meta):\n         fields = [\n             'author', 'device_types', 'icons', 'id', 'is_packaged',\n-            'manifest_url', 'name', 'payment_required', 'price',\n-            'price_locale', 'ratings', 'slug'\n+            'manifest_url', 'name', 'payment_required', 'premium_type',\n+            'price', 'price_locale', 'ratings', 'slug'\n         ]\n \n \n",
    "generated_commit": "Add back data of the geo to the Data paragraph",
    "label_commit": "forgot premium_type in feed app serializer (bug 1055742)",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": -0.10498198866844177,
    "answer_correctness": 0.1711745007114591
  },
  {
    "diff": "diff --git a/mkt/api/fields.py b/mkt/api/fields.py\nindex fb11f307114..83b3d90fc7e 100644\n--- a/mkt/api/fields.py\n+++ b/mkt/api/fields.py\n@@ -318,6 +318,17 @@ def __init__(self, *args, **kwargs):\n         kwargs['choices'] = slugs_choices + ids_choices\n         return super(SlugChoiceField, self).__init__(*args, **kwargs)\n \n+    def metadata(self):\n+        \"\"\"Return metadata about the choices. It's customized to return the\n+        name of each choice, because in that class, choices values are objects,\n+        not strings directly. This makes it possible to serialize the metadata\n+        without errors, which is necessary to answer OPTIONS (bug 984899)\"\"\"\n+        data = super(SlugChoiceField, self).metadata()\n+        data['choices'] = [{'value': v,\n+                            'display_name': unicode(getattr(n, 'name', n))}\n+                            for v, n in self.choices]\n+        return data\n+\n     def to_native(self, value):\n         if value:\n             choice = self.ids_choices_dict.get(value, None)\ndiff --git a/mkt/api/tests/test_oauth.py b/mkt/api/tests/test_oauth.py\nindex 1aba1729ea1..2c540d25bed 100644\n--- a/mkt/api/tests/test_oauth.py\n+++ b/mkt/api/tests/test_oauth.py\n@@ -132,6 +132,10 @@ def patch(self, url, data='', **kw):\n         response = self.request(**kw)\n         return response\n \n+    def options(self, url):\n+        url, headers, body = self.sign('OPTIONS', self.get_absolute_url(url))\n+        return wrap(super(OAuthClient, self).options(url, **self.kw(headers)))\n+\n \n class BaseOAuth(BaseAPI):\n     fixtures = fixture('user_2519', 'group_admin', 'group_editor',\ndiff --git a/mkt/collections/tests/test_serializers.py b/mkt/collections/tests/test_serializers.py\nindex fcd6cff47ed..7088c34af21 100644\n--- a/mkt/collections/tests/test_serializers.py\n+++ b/mkt/collections/tests/test_serializers.py\n@@ -1,4 +1,5 @@\n # -*- coding: utf-8 -*-\n+import json\n from mock import patch\n from nose.tools import eq_, ok_\n from rest_framework import serializers\n@@ -227,6 +228,9 @@ def setUp(self):\n         self.serializer = CollectionSerializer(self.collection,\n                                                context=minimal_context)\n \n+    def test_metadata_is_serialized_to_json(self):\n+        ok_(json.dumps(self.serializer.metadata()))\n+\n     def test_to_native(self, apps=None):\n         if apps:\n             for app in apps:\ndiff --git a/mkt/collections/tests/test_views.py b/mkt/collections/tests/test_views.py\nindex 36ff102dd75..a8f6318cc04 100644\n--- a/mkt/collections/tests/test_views.py\n+++ b/mkt/collections/tests/test_views.py\n@@ -153,7 +153,7 @@ def create_additional_data(self):\n \n class TestCollectionViewSetListing(BaseCollectionViewSetTest):\n     \"\"\"\n-    Tests the handling of GET requests to the list endpoint of\n+    Tests the handling of GET/OPTIONS requests to the list endpoint of\n     CollectionViewSet.\n     \"\"\"\n     def listing(self, client):\n@@ -183,6 +183,11 @@ def test_listing_has_perms(self):\n         self.make_publisher()\n         self.listing(self.client)\n \n+    def test_options_has_perms(self):\n+        self.make_publisher()\n+        res = self.client.options(self.list_url)\n+        eq_(res.status_code, 200)\n+\n     def test_listing_curator(self):\n         self.make_curator()\n         self.listing(self.client)\n",
    "generated_commit": "Add tests for Data Profiling",
    "label_commit": "Make region/carrier choices in rocketfuel API serializable, to answer OPTIONS (bug 984899)",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": -0.028275558724999428,
    "answer_correctness": 0.18071875925819442
  },
  {
    "diff": "diff --git a/apps/comm/utils.py b/apps/comm/utils.py\nindex e8ba5b0a508..9fd98636bee 100644\n--- a/apps/comm/utils.py\n+++ b/apps/comm/utils.py\n@@ -3,16 +3,27 @@\n \n import commonware.log\n from email_reply_parser import EmailReplyParser\n+import waffle\n \n from access import acl\n from access.models import Group\n from comm.models import (CommunicationNote, CommunicationNoteRead,\n-                         CommunicationThreadCC, CommunicationThreadToken)\n+                         CommunicationThreadCC, CommunicationThread,\n+                         CommunicationThreadToken)\n from mkt.constants import comm\n from users.models import UserProfile\n \n \n log = commonware.log.getLogger('comm')\n+action_note_types = {\n+    'approve': comm.APPROVAL,\n+    'disable': comm.DISABLED,\n+    'escalate': comm.ESCALATION,\n+    'info': comm.MORE_INFO_REQUIRED,\n+    'comment': comm.REVIEWER_COMMENT,\n+    'reject': comm.REJECTION,\n+    'resubmit': comm.RESUBMISSION\n+}\n \n \n class ThreadObjectPermission(object):\n@@ -215,3 +226,40 @@ def get_recipients(note, fresh_thread=False):\n         new_recipients_list.append((user_email, tok.uuid))\n \n     return new_recipients_list\n+\n+\n+def create_comm_thread(**kwargs):\n+    if not waffle.switch_is_active('comm-dashboard'):\n+        return\n+\n+    addon = kwargs['addon']\n+    version = kwargs['version']\n+    thread = CommunicationThread.objects.filter(addon=addon, version=version)\n+\n+    perms = {}\n+    for key in kwargs['perms']:\n+        perms['read_permission_%s' % key] = True\n+\n+    if thread.exists():\n+        thread = thread[0]\n+    else:\n+        thread = CommunicationThread.objects.create(addon=addon,\n+            version=version, **perms)\n+\n+    note = CommunicationNote.objects.create(\n+        note_type=action_note_types[kwargs['action']],\n+        body=kwargs['comments'], author=kwargs['profile'],\n+        thread=thread, **perms)\n+\n+    moz_emails = addon.get_mozilla_contacts()\n+\n+    # CC mozilla contact.\n+    for email in moz_emails:\n+        try:\n+            moz_contact = UserProfile.objects.get(email=email)\n+        except UserProfile.DoesNotExist:\n+            pass\n+        else:\n+            CommunicationThreadCC.objects.get_or_create(\n+                thread=thread, user=moz_contact)\n+    return thread, note\ndiff --git a/mkt/developers/tests/test_views_versions.py b/mkt/developers/tests/test_views_versions.py\nindex aeca75b40c8..84c4da96626 100644\n--- a/mkt/developers/tests/test_views_versions.py\n+++ b/mkt/developers/tests/test_views_versions.py\n@@ -116,6 +116,19 @@ def test_rejected(self):\n             addon=webapp, activity_log__action=action.id).exists(), (\n                 \"Didn't find `%s` action in logs.\" % action.short)\n \n+    def test_comm_thread_after_resubmission(self):\n+        self.create_switch('comm-dashboard')\n+        self.webapp.update(status=amo.STATUS_REJECTED)\n+        amo.set_user(UserProfile.objects.get(username='admin'))\n+        (self.webapp.versions.latest()\n+                             .all_files[0].update(status=amo.STATUS_DISABLED))\n+        my_reply = 'no give up'\n+        self.client.post(self.url, {'notes': my_reply,\n+                                    'resubmit-app': ''})\n+        notes = CommunicationNote.objects.all()\n+        eq_(notes.count(), 1)\n+        eq_(notes[0].body, my_reply)\n+\n     def test_rejected_packaged(self):\n         self.webapp.update(is_packaged=True)\n         comments = \"oh no you di'nt!!\"\ndiff --git a/mkt/developers/views.py b/mkt/developers/views.py\nindex b8882b69108..fa1531cd310 100644\n--- a/mkt/developers/views.py\n+++ b/mkt/developers/views.py\n@@ -32,6 +32,7 @@\n                             post_required)\n from amo.urlresolvers import reverse\n from amo.utils import escape_all\n+from comm.utils import create_comm_thread\n from devhub.models import AppLog\n from files.models import File, FileUpload\n from files.utils import parse_addon\n@@ -54,7 +55,6 @@\n                                   trap_duplicate)\n from mkt.developers.utils import check_upload\n from mkt.developers.tasks import run_validator\n-from mkt.reviewers.utils import create_comm_thread\n from mkt.submit.forms import AppFeaturesForm, NewWebappVersionForm\n from mkt.webapps.tasks import _update_manifest, update_manifests\n from mkt.webapps.models import Webapp\n@@ -209,6 +209,11 @@ def status(request, addon_id, addon, webapp=False):\n     if request.method == 'POST':\n         if 'resubmit-app' in request.POST and form.is_valid():\n             form.save()\n+            perms = ('reviewer', 'senior_reviewer', 'staff')\n+            create_comm_thread(action='resubmit', addon=addon,\n+                comments=form.data['notes'], profile=request.amo_user,\n+                version=addon.current_version, perms=perms)\n+\n             messages.success(request, _('App successfully resubmitted.'))\n             return redirect(addon.get_dev_url('versions'))\n \ndiff --git a/mkt/reviewers/utils.py b/mkt/reviewers/utils.py\nindex 5d478e74f68..b2225d190c6 100644\n--- a/mkt/reviewers/utils.py\n+++ b/mkt/reviewers/utils.py\n@@ -16,9 +16,7 @@\n from amo.helpers import absolutify\n from amo.urlresolvers import reverse\n from amo.utils import JSONEncoder, send_mail_jinja, to_language\n-from comm.models import (CommunicationNote, CommunicationThread,\n-                         CommunicationThreadCC)\n-from comm.utils import get_recipients\n+from comm.utils import create_comm_thread, get_recipients\n from editors.models import EscalationQueue, RereviewQueue, ReviewerScore\n from files.models import File\n \n@@ -26,18 +24,9 @@\n from mkt.constants.features import FeatureProfile\n from mkt.site.helpers import product_as_dict\n from mkt.webapps.models import Webapp\n-from users.models import UserProfile\n \n \n log = commonware.log.getLogger('z.mailer')\n-action_note_types = {\n-    'approve': comm.APPROVAL,\n-    'disable': comm.DISABLED,\n-    'escalate': comm.ESCALATION,\n-    'info': comm.MORE_INFO_REQUIRED,\n-    'comment': comm.REVIEWER_COMMENT,\n-    'reject': comm.REJECTION\n-}\n \n \n def send_mail(subject, template, context, emails, perm_setting=None, cc=None,\n@@ -74,43 +63,6 @@ def send_note_emails(note):\n                   [email], perm_setting='app_reviewed', reply_to=reply_to)\n \n \n-def create_comm_thread(**kwargs):\n-    if not waffle.switch_is_active('comm-dashboard'):\n-        return\n-\n-    addon = kwargs['addon']\n-    version = kwargs['version']\n-    thread = CommunicationThread.objects.filter(addon=addon, version=version)\n-\n-    perms = {}\n-    for key in kwargs['perms']:\n-        perms['read_permission_%s' % key] = True\n-\n-    if thread.exists():\n-        thread = thread[0]\n-    else:\n-        thread = CommunicationThread.objects.create(addon=addon,\n-            version=version, **perms)\n-\n-    note = CommunicationNote.objects.create(\n-        note_type=action_note_types[kwargs['action']],\n-        body=kwargs['comments'], author=kwargs['profile'],\n-        thread=thread, **perms)\n-\n-    moz_emails = addon.get_mozilla_contacts()\n-\n-    # CC mozilla contact.\n-    for email in moz_emails:\n-        try:\n-            moz_contact = UserProfile.objects.get(email=email)\n-        except UserProfile.DoesNotExist:\n-            pass\n-        else:\n-            CommunicationThreadCC.objects.get_or_create(\n-                thread=thread, user=moz_contact)\n-    return thread, note\n-\n-\n class ReviewBase(object):\n \n     def __init__(self, request, addon, version, review_type,\n",
    "generated_commit": "Remove the usage of built-in API calls",
    "label_commit": "create thread/notes on resubmission from devhub (bug 908652)",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": 0.030680593103170395,
    "answer_correctness": 0.18002533668429452
  },
  {
    "diff": "diff --git a/tests/plugins/test_plugins_manager.py b/tests/plugins/test_plugins_manager.py\nindex 9ed00cae05a01..9ae6f55b6be2e 100644\n--- a/tests/plugins/test_plugins_manager.py\n+++ b/tests/plugins/test_plugins_manager.py\n@@ -65,6 +65,7 @@ def on_load(self, *args, **kwargs):\n \n @pytest.fixture(autouse=True, scope=\"module\")\n def clean_plugins():\n+    get_listener_manager().clear()\n     yield\n     get_listener_manager().clear()\n \ndiff --git a/tests/task/task_runner/test_standard_task_runner.py b/tests/task/task_runner/test_standard_task_runner.py\nindex c54a27ae8997b..797462136a47c 100644\n--- a/tests/task/task_runner/test_standard_task_runner.py\n+++ b/tests/task/task_runner/test_standard_task_runner.py\n@@ -72,6 +72,7 @@ def logging_and_db(self):\n         (as the test environment does not have enough context for the normal\n         way to run) and ensures they reset back to normal on the way out.\n         \"\"\"\n+        get_listener_manager().clear()\n         clear_db_runs()\n         dictConfig(LOGGING_CONFIG)\n         yield\n@@ -79,6 +80,7 @@ def logging_and_db(self):\n         airflow_logger.handlers = []\n         clear_db_runs()\n         dictConfig(DEFAULT_LOGGING_CONFIG)\n+        get_listener_manager().clear()\n \n     def test_start_and_terminate(self):\n         local_task_job = mock.Mock()\n",
    "generated_commit": "[AIRFLOW-XXX] Fix flaky tests in unittest (#4259)",
    "label_commit": "tests: always cleanup registered test listeners (#27896)",
    "bleu": 0,
    "rouge_l": 0.13333333333333333,
    "meteor": 0.07142857142857142,
    "bert_score_f1": 0.12523123621940613,
    "answer_correctness": 0.1943560625346185
  },
  {
    "diff": "diff --git a/stream_alert/alert_processor/output_base.py b/stream_alert/alert_processor/output_base.py\nindex 23b9ea739..c8b8790fd 100644\n--- a/stream_alert/alert_processor/output_base.py\n+++ b/stream_alert/alert_processor/output_base.py\n@@ -139,6 +139,9 @@ def _get_creds_from_s3(self, cred_location, descriptor):\n             [boolean] True if download of creds from s3 was a success\n         \"\"\"\n         try:\n+            if not os.path.exists(os.path.dirname(cred_location)):\n+                os.makedirs(os.path.dirname(cred_location))\n+\n             client = boto3.client('s3', region_name=self.region)\n             with open(cred_location, 'wb') as cred_output:\n                 client.download_fileobj(self.secrets_bucket,\n@@ -253,7 +256,7 @@ def output_cred_name(self, descriptor):\n \n         # should descriptor be enforced in all rules?\n         if descriptor:\n-            cred_name = '{}_{}'.format(cred_name, descriptor)\n+            cred_name = '{}/{}'.format(cred_name, descriptor)\n \n         return cred_name\n \ndiff --git a/stream_alert_cli/outputs.py b/stream_alert_cli/outputs.py\nindex 104127da7..407a350e5 100644\n--- a/stream_alert_cli/outputs.py\n+++ b/stream_alert_cli/outputs.py\n@@ -125,7 +125,8 @@ def send_creds_to_s3(region, bucket, key, blob_data):\n         client.put_object(\n             Body=blob_data,\n             Bucket=bucket,\n-            Key=key\n+            Key=key,\n+            ServerSideEncryption='AES256'\n         )\n     except ClientError as err:\n         LOGGER_CLI.exception('an error occurred while sending credentials for key [%s] to S3: %s',\n",
    "generated_commit": "fixing issues with missing urls (#218)",
    "label_commit": "[lambda][output][cli] credential storage on s3 will be prefixed with the service",
    "bleu": 0.01774239756616722,
    "rouge_l": 0.10526315789473684,
    "meteor": 0.04761904761904762,
    "bert_score_f1": 0.06969757378101349,
    "answer_correctness": 0.1734276762933457
  },
  {
    "diff": "diff --git a/celery/concurrency/processes/__init__.py b/celery/concurrency/processes/__init__.py\nindex 4d8c74d5e11..abc1694a315 100644\n--- a/celery/concurrency/processes/__init__.py\n+++ b/celery/concurrency/processes/__init__.py\n@@ -3,6 +3,8 @@\n Process Pools.\n \n \"\"\"\n+import traceback\n+\n from time import sleep, time\n \n from celery import log\n@@ -134,9 +136,20 @@ def on_ready(self, callbacks, errbacks, ret_value):\n             if isinstance(ret_value.exception, (\n                     SystemExit, KeyboardInterrupt)):\n                 raise ret_value.exception\n-            [errback(ret_value) for errback in errbacks]\n+            [self.safe_apply_callback(errback, ret_value)\n+                    for errback in errbacks]\n         else:\n-            [callback(ret_value) for callback in callbacks]\n+            [self.safe_apply_callback(callback, ret_value)\n+                    for callback in callbacks]\n+\n+    def safe_apply_callback(self, fun, *args):\n+        try:\n+            fun(*args)\n+        except:\n+            self.logger.error(\"Pool callback raised exception: %s\" % (\n+                traceback.format_exc(), ))\n+\n+\n \n     @property\n     def info(self):\ndiff --git a/celery/worker/controllers.py b/celery/worker/controllers.py\nindex 20c54af1418..9ec1156d0a2 100644\n--- a/celery/worker/controllers.py\n+++ b/celery/worker/controllers.py\n@@ -4,6 +4,7 @@\n \n \"\"\"\n import threading\n+import traceback\n from Queue import Empty as QueueEmpty\n \n from celery import log\n@@ -31,6 +32,7 @@ def __init__(self, ready_queue, callback, logger=None):\n         self._shutdown = threading.Event()\n         self._stopped = threading.Event()\n         self.setDaemon(True)\n+        self.setName(self.__class__.__name__)\n \n     def move(self):\n         try:\n@@ -45,7 +47,14 @@ def move(self):\n         self.logger.debug(\n             \"Mediator: Running callback for task: %s[%s]\" % (\n                 task.task_name, task.task_id))\n-        self.callback(task)                 # execute\n+\n+        try:\n+            self.callback(task)\n+        except Exception, exc:\n+            self.logger.error(\"Mediator callback raised exception %r\\n%s\" % (\n+                exc, traceback.format_exc()))\n+\n+\n \n     def run(self):\n         while not self._shutdown.isSet():\n",
    "generated_commit": "Task decorator should be called with the original stack trace",
    "label_commit": "Catch exceptions occuring in Mediator+Pool callbacks. Thanks to Christopher Hesse.",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": 0.20867705345153809,
    "answer_correctness": 0.18737042405348808
  },
  {
    "diff": "diff --git a/alerts/lib/alerttask.py b/alerts/lib/alerttask.py\nindex 533dc746e..e2539eb72 100644\n--- a/alerts/lib/alerttask.py\n+++ b/alerts/lib/alerttask.py\n@@ -267,13 +267,17 @@ def searchEventsAggreg(self, aggregField, samplesLimit=5):\n                 idict = {\n                     'value': i[0],\n                     'count': i[1],\n-                    'events': []\n+                    'events': [],\n+                    'allevents': []\n                 }\n                 for r in results:\n                     if r['_source']['details'][aggregField].encode('ascii', 'ignore') == i[0]:\n                         # copy events detail into this aggregation up to our samples limit\n                         if len(idict['events']) < samplesLimit:\n                             idict['events'].append(r)\n+                        # also copy all events to a non-sampled list\n+                        # so we mark all events as alerted and don't re-alert\n+                        idict['allevents'].append(r)\n                 aggregList.append(idict)\n \n             self.aggregations = aggregList\n@@ -306,7 +310,7 @@ def walkAggregations(self, threshold):\n                     self.log.debug(alert)\n                     if alert:\n                         alertResultES = self.alertToES(alert)\n-                        self.tagEventsAlert(aggreg['events'], alertResultES)\n+                        self.tagEventsAlert(aggreg['allevents'], alertResultES)\n                         self.alertToMessageQueue(alert)\n \n     def createAlertDict(self, summary, category, tags, events, severity='NOTICE'):\n",
    "generated_commit": "fixing issue with missing keys (#2186)",
    "label_commit": "add full list of events to aggregated alert, closes #229",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": 0.2387731522321701,
    "answer_correctness": 0.19268202151070518
  },
  {
    "diff": "diff --git a/mq/plugins/guardDuty.py b/mq/plugins/guardDuty.py\nindex d15175f1a..4a00c9198 100644\n--- a/mq/plugins/guardDuty.py\n+++ b/mq/plugins/guardDuty.py\n@@ -21,8 +21,8 @@ def __init__(self):\n         # AWS guard duty sends dates as iso_8601 which ES doesn't appreciate\n         # here's a list of date fields we'll convert to isoformat\n         self.date_keys = [\n-            'details.service.eventLastSeen',\n-            'details.service.eventFirstSeen',\n+            'details.finding.eventLastSeen',\n+            'details.finding.eventFirstSeen',\n             'details.resource.instanceDetails.launchTime',\n             'details.createdAt',\n             'details.updatedAt'\n@@ -32,8 +32,8 @@ def __init__(self):\n         # Lets pick out some likely targets and format them\n         # so other mozdef plugins can rely on their location\n         self.ipaddress_keys =[\n-            'details.service.action.networkConnectionAction.remoteIpDetails.ipAddressV4',\n-            'details.service.action.awsApiCallAction.remoteIpDetails.ipAdrressV4'\n+            'details.finding.action.networkConnectionAction.remoteIpDetails.ipAddressV4',\n+            'details.finding.action.awsApiCallAction.remoteIpDetails.ipAdrressV4'\n         ]\n \n     def convert_key_date_format(self, needle, haystack):\n",
    "generated_commit": "Fixing typo in OPA API docs (#36)",
    "label_commit": "rename details.service to details.finding\n\nAs per: https://github.com/mozilla/guardDuty2MozDef/pull/1/files",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": -0.14205269515514374,
    "answer_correctness": 0.19192791996839892
  },
  {
    "diff": "diff --git a/airflow/dag_processing/processor.py b/airflow/dag_processing/processor.py\nindex 1a7fa6f10565a..b7dac828e2894 100644\n--- a/airflow/dag_processing/processor.py\n+++ b/airflow/dag_processing/processor.py\n@@ -417,26 +417,23 @@ def manage_slas(self, dag: DAG, session: Session = None) -> None:\n \n             sla_misses = []\n             next_info = dag.next_dagrun_info(dag.get_run_data_interval(ti.dag_run), restricted=False)\n-            if next_info is None:\n-                self.log.info(\"Skipping SLA check for %s because task does not have scheduled date\", ti)\n-            else:\n-                while next_info.logical_date < ts:\n-                    next_info = dag.next_dagrun_info(next_info.data_interval, restricted=False)\n-\n-                    if next_info is None:\n-                        break\n-                    if (ti.dag_id, ti.task_id, next_info.logical_date) in recorded_slas_query:\n-                        break\n-                    if next_info.logical_date + task.sla < ts:\n-\n-                        sla_miss = SlaMiss(\n-                            task_id=ti.task_id,\n-                            dag_id=ti.dag_id,\n-                            execution_date=next_info.logical_date,\n-                            timestamp=ts,\n-                        )\n-                        sla_misses.append(sla_miss)\n-                        Stats.incr('sla_missed')\n+            while next_info and next_info.logical_date < ts:\n+                next_info = dag.next_dagrun_info(next_info.data_interval, restricted=False)\n+\n+                if next_info is None:\n+                    break\n+                if (ti.dag_id, ti.task_id, next_info.logical_date) in recorded_slas_query:\n+                    break\n+                if next_info.logical_date + task.sla < ts:\n+\n+                    sla_miss = SlaMiss(\n+                        task_id=ti.task_id,\n+                        dag_id=ti.dag_id,\n+                        execution_date=next_info.logical_date,\n+                        timestamp=ts,\n+                    )\n+                    sla_misses.append(sla_miss)\n+                    Stats.incr('sla_missed')\n             if sla_misses:\n                 session.add_all(sla_misses)\n         session.commit()\ndiff --git a/airflow/jobs/scheduler_job.py b/airflow/jobs/scheduler_job.py\nindex 62907e7d7dd36..40411f270e172 100644\n--- a/airflow/jobs/scheduler_job.py\n+++ b/airflow/jobs/scheduler_job.py\n@@ -1318,6 +1318,10 @@ def _send_sla_callbacks_to_processor(self, dag: DAG) -> None:\n             self.log.debug(\"Skipping SLA check for %s because no tasks in DAG have SLAs\", dag)\n             return\n \n+        if not dag.timetable.periodic:\n+            self.log.debug(\"Skipping SLA check for %s because DAG is not scheduled\", dag)\n+            return\n+\n         request = SlaCallbackRequest(full_filepath=dag.fileloc, dag_id=dag.dag_id)\n         self.executor.send_callback(request)\n \ndiff --git a/tests/jobs/test_scheduler_job.py b/tests/jobs/test_scheduler_job.py\nindex 584059d414ad2..0466a66455704 100644\n--- a/tests/jobs/test_scheduler_job.py\n+++ b/tests/jobs/test_scheduler_job.py\n@@ -2985,10 +2985,18 @@ def test_send_sla_callbacks_to_processor_sla_no_task_slas(self, dag_maker):\n             self.scheduler_job._send_sla_callbacks_to_processor(dag)\n             self.scheduler_job.executor.callback_sink.send.assert_not_called()\n \n-    def test_send_sla_callbacks_to_processor_sla_with_task_slas(self, dag_maker):\n+    @pytest.mark.parametrize(\n+        \"schedule\",\n+        [\n+            \"@daily\",\n+            \"0 10 * * *\",\n+            timedelta(hours=2),\n+        ],\n+    )\n+    def test_send_sla_callbacks_to_processor_sla_with_task_slas(self, schedule, dag_maker):\n         \"\"\"Test SLA Callbacks are sent to the DAG Processor when SLAs are defined on tasks\"\"\"\n         dag_id = 'test_send_sla_callbacks_to_processor_sla_with_task_slas'\n-        with dag_maker(dag_id=dag_id, schedule='@daily') as dag:\n+        with dag_maker(dag_id=dag_id, schedule=schedule) as dag:\n             EmptyOperator(task_id='task1', sla=timedelta(seconds=60))\n \n         with patch.object(settings, \"CHECK_SLAS\", True):\n@@ -3000,6 +3008,26 @@ def test_send_sla_callbacks_to_processor_sla_with_task_slas(self, dag_maker):\n             expected_callback = SlaCallbackRequest(full_filepath=dag.fileloc, dag_id=dag.dag_id)\n             self.scheduler_job.executor.callback_sink.send.assert_called_once_with(expected_callback)\n \n+    @pytest.mark.parametrize(\n+        \"schedule\",\n+        [\n+            None,\n+            [Dataset(\"foo\")],\n+        ],\n+    )\n+    def test_send_sla_callbacks_to_processor_sla_dag_not_scheduled(self, schedule, dag_maker):\n+        \"\"\"Test SLA Callbacks are not sent when DAG isn't scheduled\"\"\"\n+        dag_id = 'test_send_sla_callbacks_to_processor_sla_no_task_slas'\n+        with dag_maker(dag_id=dag_id, schedule=schedule) as dag:\n+            EmptyOperator(task_id='task1', sla=timedelta(seconds=5))\n+\n+        with patch.object(settings, \"CHECK_SLAS\", True):\n+            self.scheduler_job = SchedulerJob(subdir=os.devnull)\n+            self.scheduler_job.executor = MockExecutor()\n+\n+            self.scheduler_job._send_sla_callbacks_to_processor(dag)\n+            self.scheduler_job.executor.callback_sink.send.assert_not_called()\n+\n     def test_create_dag_runs(self, dag_maker):\n         \"\"\"\n         Test various invariants of _create_dag_runs.\n",
    "generated_commit": "[AIRFLOW-3254] Fix scheduling failure in scheduler job creation (#4201)",
    "label_commit": "Only send an SlaCallbackRequest if the DAG is scheduled (#26089)",
    "bleu": 0,
    "rouge_l": 0.10000000000000002,
    "meteor": 0.05050505050505051,
    "bert_score_f1": 0.10497915744781494,
    "answer_correctness": 0.19834223052652644
  },
  {
    "diff": "diff --git a/celery/app/utils.py b/celery/app/utils.py\nindex ba5e1bb8b14..a409d8fac12 100644\n--- a/celery/app/utils.py\n+++ b/celery/app/utils.py\n@@ -175,11 +175,12 @@ def filter_hidden_settings(conf):\n     def maybe_censor(key, value, mask='*' * 8):\n         if isinstance(value, Mapping):\n             return filter_hidden_settings(value)\n-        if isinstance(value, string_t) and HIDDEN_SETTINGS.search(key):\n-            return mask\n-        if isinstance(key, string_t) and 'BROKER_URL' in key.upper():\n-            from kombu import Connection\n-            return Connection(value).as_uri(mask=mask)\n+        if isinstance(key, string_t):\n+            if HIDDEN_SETTINGS.search(key):\n+                return mask\n+            if 'BROKER_URL' in key.upper():\n+                from kombu import Connection\n+                return Connection(value).as_uri(mask=mask)\n         return value\n \n     return {k: maybe_censor(k, v) for k, v in items(conf)}\ndiff --git a/celery/tests/app/test_utils.py b/celery/tests/app/test_utils.py\nindex dc7e3811042..b0ff108e834 100644\n--- a/celery/tests/app/test_utils.py\n+++ b/celery/tests/app/test_utils.py\n@@ -2,7 +2,7 @@\n \n from collections import Mapping, MutableMapping\n \n-from celery.app.utils import Settings, bugreport\n+from celery.app.utils import Settings, filter_hidden_settings, bugreport\n \n from celery.tests.case import AppCase, Mock\n \n@@ -20,6 +20,22 @@ def test_is_mutable_mapping(self):\n         self.assertTrue(issubclass(Settings, MutableMapping))\n \n \n+class test_filter_hidden_settings(AppCase):\n+\n+    def test_handles_non_string_keys(self):\n+        \"\"\"filter_hidden_settings shouldn't raise an exception when handling\n+        mappings with non-string keys\"\"\"\n+        conf = {\n+            'STRING_KEY': 'VALUE1',\n+            ('NON', 'STRING', 'KEY'): 'VALUE2',\n+            'STRING_KEY2': {\n+                'STRING_KEY3': 1,\n+                ('NON', 'STRING', 'KEY', '2'): 2\n+            },\n+        }\n+        filter_hidden_settings(conf)\n+\n+\n class test_bugreport(AppCase):\n \n     def test_no_conn_driver_info(self):\n",
    "generated_commit": "[sentinel] now raises KeyError when setting base_url is not set (#5535)",
    "label_commit": "Fix handling of non-string keys in filter_hidden_settings",
    "bleu": 0,
    "rouge_l": 0.0909090909090909,
    "meteor": 0.06756756756756757,
    "bert_score_f1": 0.16667726635932922,
    "answer_correctness": 0.1797947848035552
  },
  {
    "diff": "diff --git a/src/collectors/postgres/postgres.py b/src/collectors/postgres/postgres.py\nindex 7af28e5a3..0b329adbc 100644\n--- a/src/collectors/postgres/postgres.py\n+++ b/src/collectors/postgres/postgres.py\n@@ -373,7 +373,7 @@ class TransactionCount(QueryStats):\n \n \n class IdleInTransactions(QueryStats):\n-    path = \"%(datname)s.longest_running.%(metric)s\"\n+    path = \"%(datname)s.idle_in_tranactions.%(metric)s\"\n     multi_db = True\n     query = \"\"\"\n         SELECT 'idle_in_transaction',\n",
    "generated_commit": "fix doc string",
    "label_commit": "rename postgre collector IdleInTransactions metric",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": 0.1413061022758484,
    "answer_correctness": 0.17733419477250684
  },
  {
    "diff": "diff --git a/airflow/bin/cli.py b/airflow/bin/cli.py\nindex 6aa71196973ff..41f979fa51edb 100755\n--- a/airflow/bin/cli.py\n+++ b/airflow/bin/cli.py\n@@ -19,6 +19,7 @@\n import reprlib\n \n import os\n+import socket\n import subprocess\n import textwrap\n import warnings\n@@ -384,6 +385,9 @@ def run(args, dag=None):\n             level=settings.LOGGING_LEVEL,\n             format=settings.LOG_FORMAT)\n \n+    hostname = socket.getfqdn()\n+    logging.info(\"Running on host {}\".format(hostname))\n+\n     if not args.pickle and not dag:\n         dag = get_dag(args)\n     elif not dag:\n",
    "generated_commit": "[AIRFLOW-XXX] Add missing parenthesis for running the task (#3974)",
    "label_commit": "[AIRFLOW-908] Print hostname at the start of cli run\n\nCloses #2329 from AllisonWang/master",
    "bleu": 0.015375695992413887,
    "rouge_l": 0.16,
    "meteor": 0.07936507936507937,
    "bert_score_f1": 0.25782907009124756,
    "answer_correctness": 0.1916192022492026
  },
  {
    "diff": "diff --git a/mq/esworker_cloudtrail.py b/mq/esworker_cloudtrail.py\nindex e7c34b31f..48f47efac 100755\n--- a/mq/esworker_cloudtrail.py\n+++ b/mq/esworker_cloudtrail.py\n@@ -505,4 +505,10 @@ def initConfig():\n     es = esConnect()\n \n     pluginList = registerPlugins()\n-    main()\n+\n+    try:\n+        main()\n+    except Exception as e:\n+        if options.esbulksize != 0:\n+            es.finish_bulk()\n+        raise\ndiff --git a/mq/esworker_eventtask.py b/mq/esworker_eventtask.py\nindex 3a7a27923..bc1e1408b 100755\n--- a/mq/esworker_eventtask.py\n+++ b/mq/esworker_eventtask.py\n@@ -362,4 +362,9 @@ def initConfig():\n \n     pluginList = registerPlugins()\n \n-    main()\n+    try:\n+        main()\n+    except Exception as e:\n+        if options.esbulksize != 0:\n+            es.finish_bulk()\n+        raise\ndiff --git a/mq/esworker_papertrail.py b/mq/esworker_papertrail.py\nindex e96858dc5..ef402f28b 100755\n--- a/mq/esworker_papertrail.py\n+++ b/mq/esworker_papertrail.py\n@@ -267,11 +267,8 @@ def run(self):\n \n                 time.sleep(options.ptinterval)\n \n-            except KeyboardInterrupt:\n-                sys.exit(1)\n             except ValueError as e:\n                 logger.exception('Exception while handling message: %r' % e)\n-                sys.exit(1)\n \n     def on_message(self, body, message):\n         # print(\"RECEIVED MESSAGE: %r\" % (body, ))\n@@ -417,4 +414,9 @@ def initConfig():\n \n     pluginList = registerPlugins()\n \n-    main()\n+    try:\n+        main()\n+    except Exception as e:\n+        if options.esbulksize != 0:\n+            es.finish_bulk()\n+        raise\ndiff --git a/mq/esworker_sns_sqs.py b/mq/esworker_sns_sqs.py\nindex 63e702bcf..10e816de5 100755\n--- a/mq/esworker_sns_sqs.py\n+++ b/mq/esworker_sns_sqs.py\n@@ -61,24 +61,20 @@ def run(self):\n         self.taskQueue.set_message_class(RawMessage)\n \n         while True:\n-            try:\n-                records = self.taskQueue.get_messages(self.options.prefetch)\n-                for msg in records:\n-                    msg_body = msg.get_body()\n-                    try:\n-                        # get_body() should be json\n-                        message_json = json.loads(msg_body)\n-                        self.on_message(message_json)\n-                        # delete message from queue\n-                        self.taskQueue.delete_message(msg)\n-                    except ValueError:\n-                        logger.error('Invalid message, not JSON <dropping message and continuing>: %r' % msg_body)\n-                        self.taskQueue.delete_message(msg)\n-                        continue\n-                time.sleep(.1)\n-            except Exception as e:\n-                logger.exception(e)\n-                sys.exit(1)\n+            records = self.taskQueue.get_messages(self.options.prefetch)\n+            for msg in records:\n+                msg_body = msg.get_body()\n+                try:\n+                    # get_body() should be json\n+                    message_json = json.loads(msg_body)\n+                    self.on_message(message_json)\n+                    # delete message from queue\n+                    self.taskQueue.delete_message(msg)\n+                except ValueError:\n+                    logger.error('Invalid message, not JSON <dropping message and continuing>: %r' % msg_body)\n+                    self.taskQueue.delete_message(msg)\n+                    continue\n+            time.sleep(.1)\n \n     def on_message(self, message):\n         try:\n@@ -241,4 +237,10 @@ def initConfig():\n \n     # open ES connection globally so we don't waste time opening it per message\n     es = esConnect()\n-    main()\n+\n+    try:\n+        main()\n+    except Exception as e:\n+        if options.esbulksize != 0:\n+            es.finish_bulk()\n+        raise\ndiff --git a/mq/esworker_sqs.py b/mq/esworker_sqs.py\nindex 602f422e5..a12814eb2 100755\n--- a/mq/esworker_sqs.py\n+++ b/mq/esworker_sqs.py\n@@ -218,11 +218,9 @@ def run(self):\n                     self.taskQueue.delete_message(msg)\n                 time.sleep(.1)\n \n-            except KeyboardInterrupt:\n-                sys.exit(1)\n             except ValueError as e:\n                 logger.exception('Exception while handling message: %r' % e)\n-                sys.exit(1)\n+                self.taskQueue.delete_message(msg)\n \n     def on_message(self, body, message):\n         # print(\"RECEIVED MESSAGE: %r\" % (body, ))\n@@ -399,4 +397,9 @@ def initConfig():\n \n     pluginList = registerPlugins()\n \n-    main()\n+    try:\n+        main()\n+    except Exception as e:\n+        if options.esbulksize != 0:\n+            es.finish_bulk()\n+        raise\n",
    "generated_commit": "Ignore errors when processing task queries",
    "label_commit": "Modify workers to stop bulk queue on errors",
    "bleu": 0.02925226826055808,
    "rouge_l": 0.14285714285714288,
    "meteor": 0.06410256410256411,
    "bert_score_f1": 0.25818222761154175,
    "answer_correctness": 0.19301583638702585
  },
  {
    "diff": "diff --git a/apps/addons/models.py b/apps/addons/models.py\nindex c9b41438fdd..d4c6331f93b 100644\n--- a/apps/addons/models.py\n+++ b/apps/addons/models.py\n@@ -906,8 +906,8 @@ def get_icon_url(self, size, use_default=True):\n             icon_type_split = self.icon_type.split('/')\n \n         # Get the closest allowed size without going over\n-        if (size not in amo.ADDON_ICON_SIZES and\n-            size >= amo.ADDON_ICON_SIZES[0]):\n+        if (size not in amo.ADDON_ICON_SIZES\n+                and size >= amo.ADDON_ICON_SIZES[0]):\n             size = [s for s in amo.ADDON_ICON_SIZES if s < size][-1]\n         elif size < amo.ADDON_ICON_SIZES[0]:\n             size = amo.ADDON_ICON_SIZES[0]\ndiff --git a/apps/search/tests/test_views.py b/apps/search/tests/test_views.py\nindex 0d23cd2828b..aa04e20b2d1 100644\n--- a/apps/search/tests/test_views.py\n+++ b/apps/search/tests/test_views.py\n@@ -1061,7 +1061,8 @@ def search_addons(self, url, params, addons=[], types=amo.ADDON_TYPES,\n             if src:\n                 expected_url += '?src=ss'\n             eq_(got['url'], expected_url)\n-            eq_(got['icon'], expected.icon_url)\n+            eq_(got['icons'], {'32': expected.get_icon_url(32),\n+                               '64': expected.get_icon_url(64)})\n \n             assert expected.status in amo.REVIEWED_STATUSES, (\n                 'Unreviewed add-ons should not appear in search results.')\ndiff --git a/apps/search/views.py b/apps/search/views.py\nindex ac0db4e8b97..f40c6c97b36 100644\n--- a/apps/search/views.py\n+++ b/apps/search/views.py\n@@ -120,7 +120,10 @@ class BaseAjaxSearch(object):\n             \"id\": 1865,\n             \"name\": \"Adblock Plus\",\n             \"url\": \"http://path/to/details/page\",\n-            \"icon\": \"http://path/to/icon\",\n+            \"icons\": {\n+                \"32\": \"http://path/to/icon-32\",\n+                \"64\": \"http://path/to/icon-64\"\n+            }\n         },\n         ...\n     ]\n@@ -141,7 +144,10 @@ def __init__(self, request, excluded_ids=(), ratings=False):\n             'id': 'id',\n             'name': 'name',\n             'url': 'get_url_path',\n-            'icon': 'icon_url'\n+            'icons': {\n+                '32': ('get_icon_url', 32),\n+                '64': ('get_icon_url', 64)\n+            }\n         }\n         self.fields = getattr(self, 'fields', default_fields)\n         if self.ratings:\n@@ -171,18 +177,29 @@ def queryset(self):\n                                     status__in=amo.REVIEWED_STATUSES)\n         return results\n \n+    def _build_fields(self, item, fields):\n+        data = {}\n+        for key, prop in fields.iteritems():\n+            if isinstance(prop, dict):\n+                data[key] = self._build_fields(item, prop)\n+            else:\n+                # prop is a tuple like: ('method', 'arg1, 'argN').\n+                if isinstance(prop, tuple):\n+                    val = getattr(item, prop[0])(*prop[1:])\n+                else:\n+                    val = getattr(item, prop, '')\n+                    if callable(val):\n+                        val = val()\n+                data[key] = unicode(val)\n+        return data\n+\n     def build_list(self):\n         \"\"\"Populate a list of dictionaries based on label => property.\"\"\"\n         results = []\n         for item in self.queryset()[:self.limit]:\n             if item.id in self.excluded_ids:\n                 continue\n-            d = {}\n-            for key, prop in self.fields.iteritems():\n-                val = getattr(item, prop, '')\n-                if callable(val):\n-                    val = val()\n-                d[key] = unicode(val)\n+            d = self._build_fields(item, self.fields)\n             if self.src and 'url' in d:\n                 d['url'] = urlparams(d['url'], src=self.src)\n             results.append(d)\n",
    "generated_commit": "Fix bug with missing args (#866)",
    "label_commit": "updated autosuggest API to replace 'icon' with 'icons' dict (bug 948367)",
    "bleu": 0.01774239756616722,
    "rouge_l": 0.11764705882352942,
    "meteor": 0.04761904761904762,
    "bert_score_f1": 0.18763847649097443,
    "answer_correctness": 0.1880908837187156
  },
  {
    "diff": "diff --git a/ichnaea/service/search/views.py b/ichnaea/service/search/views.py\nindex fe69e567c..c7fbe5fad 100644\n--- a/ichnaea/service/search/views.py\n+++ b/ichnaea/service/search/views.py\n@@ -33,7 +33,6 @@ def configure_search(config):\n \n \n def search_cell(session, data):\n-    sql_null = None  # avoid pep8 warning\n     radio = RADIO_TYPE.get(data['radio'], -1)\n     cells = []\n     for cell in data['cell']:\n@@ -51,8 +50,8 @@ def search_cell(session, data):\n             Cell.mnc == cell['mnc']).filter(\n             Cell.lac == cell['lac']).filter(\n             Cell.cid == cell['cid']).filter(\n-            Cell.lat != sql_null).filter(\n-            Cell.lon != sql_null\n+            Cell.lat.isnot(None)).filter(\n+            Cell.lon.isnot(None)\n         )\n         result = query.first()\n         if result is not None:\n@@ -72,7 +71,6 @@ def search_cell(session, data):\n \n \n def search_cell_lac(session, data):\n-    sql_null = None  # avoid pep8 warning\n     radio = RADIO_TYPE.get(data['radio'], -1)\n     lacs = []\n     for cell in data['cell']:\n@@ -90,8 +88,8 @@ def search_cell_lac(session, data):\n             Cell.mnc == cell['mnc']).filter(\n             Cell.lac == cell['lac']).filter(\n             Cell.cid == CELLID_LAC).filter(\n-            Cell.lat != sql_null).filter(\n-            Cell.lon != sql_null\n+            Cell.lat.isnot(None)).filter(\n+            Cell.lon.isnot(None)\n         )\n         result = query.first()\n         if result is not None:\n@@ -132,11 +130,10 @@ def signal_strength(w):\n     if len(wifi_keys) < 3:\n         # we didn't even get three keys, bail out\n         return None\n-    sql_null = None  # avoid pep8 warning\n     query = session.query(Wifi.key, Wifi.lat, Wifi.lon).filter(\n         Wifi.key.in_(wifi_keys)).filter(\n-        Wifi.lat != sql_null).filter(\n-        Wifi.lon != sql_null)\n+        Wifi.lat.isnot(None)).filter(\n+        Wifi.lon.isnot(None))\n     wifis = query.all()\n     if len(wifis) < 3:\n         # we got fewer than three actual matches\ndiff --git a/ichnaea/service/submit/tasks.py b/ichnaea/service/submit/tasks.py\nindex bcb3c4a7b..bda48ae72 100644\n--- a/ichnaea/service/submit/tasks.py\n+++ b/ichnaea/service/submit/tasks.py\n@@ -36,9 +36,6 @@\n from ichnaea.worker import celery\n \n \n-sql_null = None  # avoid pep8 warning\n-\n-\n def process_mapstat_keyed(factor, stat_key, positions, session):\n     tiles = defaultdict(int)\n     # aggregate to tiles, according to factor\n",
    "generated_commit": "Remove unused code related to Python 3 compatibility (#214)",
    "label_commit": "Use `isnot(None)` instead of `!= None` in SQL queries",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": 0.00843386072665453,
    "answer_correctness": 0.1826025342118784
  },
  {
    "diff": "diff --git a/mkt/api/tests/test_base.py b/mkt/api/tests/test_base.py\nindex 2f27fa2597b..dcd592ef37b 100644\n--- a/mkt/api/tests/test_base.py\n+++ b/mkt/api/tests/test_base.py\n@@ -67,7 +67,7 @@ def test_cors(self):\n         def foo(request):\n             return Response()\n         request = RequestFactory().get('/')\n-        r = foo(request)\n+        foo(request)\n         eq_(request.CORS, ['GET', 'PATCH'])\n \n \n@@ -84,9 +84,9 @@ def test_format_is_included(self):\n             {'name': 'bar-list', 'pattern': '^(?P<pk>[^/]+)/foo/$' },\n             {'name': 'bar-detail', 'pattern': '^(?P<pk>[^/]+)/foo/$' },\n             {'name': 'bar-list',\n-             'pattern': '^(?P<pk>[^/.]+)/foo\\\\.(?P<format>[a-z]+)$' },\n+             'pattern': '^(?P<pk>[^/.]+)/foo\\\\.(?P<format>[a-z0-9]+)$' },\n             {'name': 'bar-detail',\n-             'pattern': '^(?P<pk>[^/.]+)/foo\\\\.(?P<format>[a-z]+)$'},\n+             'pattern': '^(?P<pk>[^/.]+)/foo\\\\.(?P<format>[a-z0-9]+)$'},\n         ]\n         actual = [{\n             'name': url.name, 'pattern': url.regex.pattern\n@@ -101,9 +101,9 @@ def test_format_is_included_no_trailing_slashes(self):\n             {'name': 'bar-list', 'pattern': '^(?P<pk>[^/.]+)/foo$' },\n             {'name': 'bar-detail', 'pattern': '^(?P<pk>[^/.]+)/foo$' },\n             {'name': 'bar-list',\n-             'pattern': '^(?P<pk>[^/.]+)/foo\\\\.(?P<format>[a-z]+)$' },\n+             'pattern': '^(?P<pk>[^/.]+)/foo\\\\.(?P<format>[a-z0-9]+)$' },\n             {'name': 'bar-detail',\n-             'pattern': '^(?P<pk>[^/.]+)/foo\\\\.(?P<format>[a-z]+)$'},\n+             'pattern': '^(?P<pk>[^/.]+)/foo\\\\.(?P<format>[a-z0-9]+)$'},\n         ]\n         actual = [{\n             'name': url.name, 'pattern': url.regex.pattern\n",
    "generated_commit": "Fix bug in parse tests",
    "label_commit": "Fix mkt.api.tests.test_base.TestSubRouterWithFormat tests\n\nDRF apparently made the format slightly more permissive, update\nour expected pattern.",
    "bleu": 0.008647154598524984,
    "rouge_l": 0.16000000000000003,
    "meteor": 0.07142857142857142,
    "bert_score_f1": 0.043426916003227234,
    "answer_correctness": 0.1983828536908741
  },
  {
    "diff": "diff --git a/kitsune/customercare/forms.py b/kitsune/customercare/forms.py\nindex 2a7a0e1cacd..02225b4a432 100644\n--- a/kitsune/customercare/forms.py\n+++ b/kitsune/customercare/forms.py\n@@ -3,6 +3,8 @@\n \n from kitsune.customercare.zendesk import CATEGORY_CHOICES, OS_CHOICES, ZendeskClient\n \n+PRODUCTS_WITH_OS = [\"firefox-private-network-vpn\"]\n+\n \n class ZendeskForm(forms.Form):\n     \"\"\"Form for submitting a ticket to Zendesk.\"\"\"\n@@ -23,6 +25,8 @@ class ZendeskForm(forms.Form):\n     def __init__(self, *args, product, **kwargs):\n         kwargs.update({\"initial\": {\"product\": product.slug}})\n         super().__init__(*args, **kwargs)\n+        if product.slug not in PRODUCTS_WITH_OS:\n+            del self.fields[\"os\"]\n \n     def send(self, user):\n         client = ZendeskClient()\ndiff --git a/kitsune/questions/config.py b/kitsune/questions/config.py\nindex 3a392358a32..068011a4736 100644\n--- a/kitsune/questions/config.py\n+++ b/kitsune/questions/config.py\n@@ -773,6 +773,17 @@\n                 ),\n             },\n         ),\n+        (\n+            \"relay\",\n+            {\n+                \"name\": _lazy(\"Firefox Relay\"),\n+                \"subtitle\": _lazy(\"Service that lets you create aliases to hide your real email\"),\n+                \"extra_fields\": [],\n+                \"tags\": [\"relay\"],\n+                \"product\": \"relay\",\n+                \"categories\": OrderedDict([]),\n+            },\n+        ),\n     ]\n )\n \n",
    "generated_commit": "Adding the ability to control the service on the geographic location",
    "label_commit": "Enable support for Relay",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": 0.22972796857357025,
    "answer_correctness": 0.19695447099621105
  },
  {
    "diff": "diff --git a/ichnaea/api/locate/searcher.py b/ichnaea/api/locate/searcher.py\nindex 108c7a524..646ea61a8 100644\n--- a/ichnaea/api/locate/searcher.py\n+++ b/ichnaea/api/locate/searcher.py\n@@ -128,10 +128,10 @@ class PositionSearcher(Searcher):\n \n     result_type = Position\n     source_classes = (\n-        ('geoip', GeoIPPositionSource),\n         ('internal', InternalPositionSource),\n         ('ocid', OCIDPositionSource),\n         ('fallback', FallbackPositionSource),\n+        ('geoip', GeoIPPositionSource),\n     )\n \n     def format_result(self, result):\n",
    "generated_commit": "Fix the order of consecutive space in the docs",
    "label_commit": "Reorder position sources, put geoip as a last resort.",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.1111111111111111,
    "bert_score_f1": 0.09746069461107254,
    "answer_correctness": 0.1879554928855493
  },
  {
    "diff": "diff --git a/airflow/providers/microsoft/azure/operators/container_instances.py b/airflow/providers/microsoft/azure/operators/container_instances.py\nindex da297c0a65462..2b317d81d7d71 100644\n--- a/airflow/providers/microsoft/azure/operators/container_instances.py\n+++ b/airflow/providers/microsoft/azure/operators/container_instances.py\n@@ -199,7 +199,7 @@ def execute(self, context: dict) -> int:\n         # Check name again in case it was templated.\n         self._check_name(self.name)\n \n-        self._ci_hook = AzureContainerInstanceHook(self.ci_conn_id)\n+        self._ci_hook = AzureContainerInstanceHook(conn_id=self.ci_conn_id)\n \n         if self.fail_if_exists:\n             self.log.info(\"Testing if container group already exists\")\n",
    "generated_commit": "[AIRFLOW-XXX] Fix ubuntu error when querying the master branch (#4275)",
    "label_commit": "Fix argument error in AzureContainerInstancesOperator (#19668)",
    "bleu": 0.025098621243978974,
    "rouge_l": 0.23529411764705885,
    "meteor": 0.15625,
    "bert_score_f1": 0.09908629953861237,
    "answer_correctness": 0.19184613062841555
  },
  {
    "diff": "diff --git a/tests/autoclassify/test_classify_failures.py b/tests/autoclassify/test_classify_failures.py\nindex 22ddedd67c7..b13f32f599e 100644\n--- a/tests/autoclassify/test_classify_failures.py\n+++ b/tests/autoclassify/test_classify_failures.py\n@@ -168,8 +168,8 @@ def test_autoclassified_no_update_after_manual_classification_1(test_job_2,\n         error_line.refresh_from_db()\n         failure_line.refresh_from_db()\n \n-    assert len(test_error_lines[0].matches.all()) == 0\n-    assert len(test_failure_lines[0].matches.all()) == 0\n+    assert not test_error_lines[0].matches.all().exists()\n+    assert not test_failure_lines[0].matches.all().exists()\n \n \n def test_autoclassified_no_update_after_manual_classification_2(test_user, test_job_2,\n@@ -189,7 +189,7 @@ def test_autoclassified_no_update_after_manual_classification_2(test_user, test_\n     for item in test_failure_lines:\n         item.refresh_from_db()\n \n-    assert len(test_failure_lines[0].matches.all()) == 0\n+    assert not test_failure_lines[0].matches.all().exists()\n \n \n def test_classify_skip_ignore(test_job_2,\ndiff --git a/tests/e2e/test_client_job_ingestion.py b/tests/e2e/test_client_job_ingestion.py\nindex f9cdb0c8e30..e1d1dd460c7 100644\n--- a/tests/e2e/test_client_job_ingestion.py\n+++ b/tests/e2e/test_client_job_ingestion.py\n@@ -117,7 +117,7 @@ def test_post_job_pending_to_completed_with_unparsed_log(test_repository,\n     post_collection(test_repository.name, tjc)\n     # should have no text log errors or bug suggestions\n     assert TextLogError.objects.count() == 0\n-    assert len(get_error_summary(Job.objects.get(guid=job_guid))) == 0\n+    assert get_error_summary(Job.objects.get(guid=job_guid)) == []\n \n     # the second time, post a log that will get parsed\n     log_url = add_log_response(\"mozilla-central-macosx64-debug-bm65-build1-build15.txt.gz\")\ndiff --git a/tests/etl/test_job_ingestion.py b/tests/etl/test_job_ingestion.py\nindex 8e5a944c796..fa6c062652c 100644\n--- a/tests/etl/test_job_ingestion.py\n+++ b/tests/etl/test_job_ingestion.py\n@@ -228,7 +228,7 @@ def test_remove_existing_jobs_single_existing(test_repository, failure_classific\n     assert Job.objects.count() == 1\n \n     data = _remove_existing_jobs(job_data)\n-    assert len(data) == 0\n+    assert data == []\n \n \n def test_remove_existing_jobs_one_existing_one_new(test_repository, failure_classifications,\ndiff --git a/tests/model/test_bugscache.py b/tests/model/test_bugscache.py\nindex e88bcb56a8b..d2e5fdd3307 100644\n--- a/tests/model/test_bugscache.py\n+++ b/tests/model/test_bugscache.py\n@@ -88,7 +88,7 @@ def test_get_open_recent_bugs(transactional_db, sample_bugs, search_term, exp_bu\n     suggestions = Bugscache.search(search_term)\n     open_recent_bugs = [b['id'] for b in suggestions['open_recent']]\n     assert open_recent_bugs == exp_bugs\n-    assert len(suggestions['all_others']) == 0\n+    assert suggestions['all_others'] == []\n \n \n @pytest.mark.parametrize((\"search_term\", \"exp_bugs\"), BUG_SEARCHES)\n@@ -103,7 +103,7 @@ def test_get_all_other_bugs(transactional_db, sample_bugs, search_term, exp_bugs\n     _update_bugscache(bug_list)\n \n     suggestions = Bugscache.search(search_term)\n-    assert len(suggestions['open_recent']) == 0\n+    assert suggestions['open_recent'] == []\n     all_others_bugs = [b['id'] for b in suggestions['all_others']]\n     assert all_others_bugs == exp_bugs\n \n@@ -122,8 +122,7 @@ def test_get_recent_resolved_bugs(transactional_db, sample_bugs):\n     _update_bugscache(bug_list)\n \n     suggestions = Bugscache.search(search_term)\n-    print(suggestions)\n-    assert len(suggestions['open_recent']) == 0\n+    assert suggestions['open_recent'] == []\n     all_others_bugs = [b['id'] for b in suggestions['all_others']]\n     assert all_others_bugs == exp_bugs\n \ndiff --git a/tests/model/test_classified_failure.py b/tests/model/test_classified_failure.py\nindex 8b8ac3cc0d3..38c0565459a 100644\n--- a/tests/model/test_classified_failure.py\n+++ b/tests/model/test_classified_failure.py\n@@ -43,7 +43,7 @@ def test_set_bug_duplicate(failure_lines, classified_failures, test_matcher):\n     # Check we picked the better of the two scores for the new match.\n     assert matches[0].score == Decimal(\"0.8\")\n     # Ensure we deleted the ClassifiedFailure on which we tried to set the bug\n-    assert len(ClassifiedFailure.objects.filter(id=classified_failures[1].id)) == 0\n+    assert not ClassifiedFailure.objects.filter(id=classified_failures[1].id).exists()\n \n \n def test_update_autoclassification_bug(test_job, test_job_2,\ndiff --git a/tests/webapp/api/test_job_details_api.py b/tests/webapp/api/test_job_details_api.py\nindex c1a2fb93f51..253c30a4b14 100644\n--- a/tests/webapp/api/test_job_details_api.py\n+++ b/tests/webapp/api/test_job_details_api.py\n@@ -103,7 +103,7 @@ def test_job_details(test_repository, failure_classifications,\n                       '?repository={}&job_id__in=3'.format(\n                           test_repository.name))\n     assert resp.status_int == 200\n-    assert len(resp.json['results']) == 0\n+    assert resp.json['results'] == []\n \n     # add an extra one, but filter to just get those with a specific title.\n     # we should only have one\ndiff --git a/tests/webapp/api/test_performance_alertsummary_api.py b/tests/webapp/api/test_performance_alertsummary_api.py\nindex f130a555686..414fec45e42 100644\n--- a/tests/webapp/api/test_performance_alertsummary_api.py\n+++ b/tests/webapp/api/test_performance_alertsummary_api.py\n@@ -97,7 +97,7 @@ def test_alert_summaries_get(webapp, test_perf_alert_summary,\n         'related_summary_id',\n         'classifier'\n     ])\n-    assert len(resp.json['results'][0]['related_alerts']) == 0\n+    assert resp.json['results'][0]['related_alerts'] == []\n \n \n def test_alert_summaries_get_onhold(webapp, test_perf_alert_summary,\n@@ -141,7 +141,7 @@ def test_alert_summaries_get_onhold(webapp, test_perf_alert_summary,\n         'related_summary_id',\n         'classifier'\n     ])\n-    assert len(resp.json['results'][0]['related_alerts']) == 0\n+    assert resp.json['results'][0]['related_alerts'] == []\n \n \n def test_alert_summaries_put(webapp, test_repository, test_perf_signature,\ndiff --git a/treeherder/etl/jobs.py b/treeherder/etl/jobs.py\nindex d49dfb946f7..55e6049931e 100644\n--- a/treeherder/etl/jobs.py\n+++ b/treeherder/etl/jobs.py\n@@ -123,7 +123,7 @@ def _load_job(repository, job_datum, push_id, lower_tier_signatures):\n         symbol=job_datum.get('group_symbol') or 'unknown')\n \n     product_name = job_datum.get('product_name', 'unknown')\n-    if len(product_name.strip()) == 0:\n+    if not product_name.strip():\n         product_name = 'unknown'\n     product, _ = Product.objects.get_or_create(name=product_name)\n \ndiff --git a/treeherder/perf/management/commands/test_analyze_perf.py b/treeherder/perf/management/commands/test_analyze_perf.py\nindex caaf7d040f4..1523855d23a 100644\n--- a/treeherder/perf/management/commands/test_analyze_perf.py\n+++ b/treeherder/perf/management/commands/test_analyze_perf.py\n@@ -105,10 +105,7 @@ def handle(self, *args, **options):\n                 for r in detect_changes(data):\n                     if r.state == 'regression':\n                         pushes = pc.get_pushes(project, id=r.testrun_id)\n-                        if len(pushes):\n-                            revision = pushes[0]['revision']\n-                        else:\n-                            revision = ''\n+                        revision = pushes[0]['revision'] if pushes else ''\n                         initial_value = r.historical_stats['avg']\n                         new_value = r.forward_stats['avg']\n                         if initial_value != 0:\ndiff --git a/treeherder/perf/models.py b/treeherder/perf/models.py\nindex e9fd7757c68..d402709dd58 100644\n--- a/treeherder/perf/models.py\n+++ b/treeherder/perf/models.py\n@@ -249,16 +249,15 @@ def autodetermine_status(self):\n                   PerformanceAlert.objects.filter(related_summary=self))\n \n         # if no alerts yet, we'll say untriaged\n-        if len(alerts) == 0:\n+        if not alerts:\n             return PerformanceAlertSummary.UNTRIAGED\n \n         # if any untriaged, then set to untriaged\n-        if len([a for a in alerts if a.status == PerformanceAlert.UNTRIAGED]):\n+        if any(alert.status == PerformanceAlert.UNTRIAGED for alert in alerts):\n             return PerformanceAlertSummary.UNTRIAGED\n \n         # if all invalid, then set to invalid\n-        if all([alert.status == PerformanceAlert.INVALID for alert in\n-                alerts]):\n+        if all(alert.status == PerformanceAlert.INVALID for alert in alerts):\n             return PerformanceAlertSummary.INVALID\n \n         # otherwise filter out invalid alerts\n@@ -267,9 +266,9 @@ def autodetermine_status(self):\n         # if there are any \"acknowledged\" alerts, then set to investigating\n         # if not one of the resolved statuses and there are regressions,\n         # otherwise we'll say it's an improvement\n-        if len([a for a in alerts if a.status == PerformanceAlert.ACKNOWLEDGED]):\n-            if all([not alert.is_regression for alert in\n-                    alerts if alert.status == PerformanceAlert.ACKNOWLEDGED]):\n+        if any(alert.status == PerformanceAlert.ACKNOWLEDGED for alert in alerts):\n+            if all(not alert.is_regression for alert in\n+                   alerts if alert.status == PerformanceAlert.ACKNOWLEDGED):\n                 return PerformanceAlertSummary.IMPROVEMENT\n             elif self.status not in (PerformanceAlertSummary.IMPROVEMENT,\n                                      PerformanceAlertSummary.INVESTIGATING,\n@@ -284,7 +283,7 @@ def autodetermine_status(self):\n         # alerts of its own: all alerts should be either reassigned,\n         # downstream, or invalid (but not all invalid, that case is covered\n         # above)\n-        if len([a for a in alerts if a.status == PerformanceAlert.REASSIGNED]):\n+        if any(alert.status == PerformanceAlert.REASSIGNED for alert in alerts):\n             return PerformanceAlertSummary.REASSIGNED\n \n         return PerformanceAlertSummary.DOWNSTREAM\ndiff --git a/treeherder/perfalert/perfalert/__init__.py b/treeherder/perfalert/perfalert/__init__.py\nindex 16ae974ac56..b74a52606e9 100644\n--- a/treeherder/perfalert/perfalert/__init__.py\n+++ b/treeherder/perfalert/perfalert/__init__.py\n@@ -60,7 +60,7 @@ def calc_t(w1, w2, weight_fn=None):\n \n     See the analyze() function for a description of the `weight_fn` argument.\n     \"\"\"\n-    if len(w1) == 0 or len(w2) == 0:\n+    if not w1 or not w2:\n         return 0\n \n     s1 = analyze(w1, weight_fn)\ndiff --git a/treeherder/seta/high_value_jobs.py b/treeherder/seta/high_value_jobs.py\nindex e40633141b2..09a84d66a81 100644\n--- a/treeherder/seta/high_value_jobs.py\n+++ b/treeherder/seta/high_value_jobs.py\n@@ -27,7 +27,7 @@ def check_removal(failures, removals):\n             if not found:\n                 results[failure].append(failure_job)\n \n-        if len(results[failure]) == 0:\n+        if not results[failure]:\n             del results[failure]\n \n     return results\ndiff --git a/treeherder/seta/preseed.py b/treeherder/seta/preseed.py\nindex f5e22bccb63..89472261de3 100644\n--- a/treeherder/seta/preseed.py\n+++ b/treeherder/seta/preseed.py\n@@ -32,7 +32,7 @@ def load_preseed():\n                 queryset = queryset.filter(**{field: job[field]})\n \n         # Deal with the case where we have a new entry in preseed\n-        if len(queryset) == 0:\n+        if not queryset:\n             create_new_entry(job)\n         else:\n             # We can have wildcards, so loop on all returned values in data\n",
    "generated_commit": "A few changes around issues causing multiple bugs",
    "label_commit": "Bug 1452212 - Don't use len() to check if sequences are empty\n\nFixes pylint `len-as-condition`.",
    "bleu": 0,
    "rouge_l": 0.07999999999999999,
    "meteor": 0.03496503496503496,
    "bert_score_f1": -0.03959793224930763,
    "answer_correctness": 0.17452968683470255
  },
  {
    "diff": "diff --git a/bugbug/repository.py b/bugbug/repository.py\nindex e21a60a362..dda194771d 100644\n--- a/bugbug/repository.py\n+++ b/bugbug/repository.py\n@@ -229,8 +229,8 @@ def _transform(commit):\n     obj[\"average_file_size\"] = (\n         obj[\"total_file_size\"] / len(sizes) if len(sizes) > 0 else 0\n     )\n-    obj[\"maximum_file_size\"] = max(sizes)\n-    obj[\"minimum_file_size\"] = min(sizes)\n+    obj[\"maximum_file_size\"] = max(sizes) if len(sizes) > 0 else 0\n+    obj[\"minimum_file_size\"] = min(sizes) if len(sizes) > 0 else 0\n \n     obj[\"files_modified_num\"] = len(patch_data)\n \n",
    "generated_commit": "Fix bug in filtering file limit (#781)",
    "label_commit": "Don't fail on max or min when the file sizes list is empty",
    "bleu": 0.014017732413740807,
    "rouge_l": 0.09523809523809523,
    "meteor": 0.040322580645161296,
    "bert_score_f1": 0.15994316339492798,
    "answer_correctness": 0.1890904439577914
  },
  {
    "diff": "diff --git a/celery/bin/celeryctl.py b/celery/bin/celeryctl.py\nindex 3ced1c14324..dbd39e29048 100644\n--- a/celery/bin/celeryctl.py\n+++ b/celery/bin/celeryctl.py\n@@ -381,8 +381,13 @@ def run(self, force_ipython=False, force_bpython=False,\n             import_module(\"celery.concurrency.eventlet\")\n         if gevent:\n             import_module(\"celery.concurrency.gevent\")\n+        from .. import task\n         self.app.loader.import_default_modules()\n-        self.locals = {\"celery\": self.app}\n+        self.locals = {\"celery\": self.app,\n+                       \"BaseTask\": task.BaseTask,\n+                       \"TaskSet\": task.TaskSet,\n+                       \"chord\": task.chord,\n+                       \"group\": task.group}\n \n         if not without_tasks:\n             self.locals.update(dict((task.__name__, task)\n",
    "generated_commit": "celeryctl shell now works with celery.Task.TaskSet",
    "label_commit": "celeryctl shell: chord, TaskSet/group and BaseTask is now in locals",
    "bleu": 0.024925978674400287,
    "rouge_l": 0.3157894736842105,
    "meteor": 0.10416666666666666,
    "bert_score_f1": 0.40162602066993713,
    "answer_correctness": 0.4638773533589393
  },
  {
    "diff": "diff --git a/airflow/api/auth/backend/basic_auth.py b/airflow/api/auth/backend/basic_auth.py\nindex dbfeaa5f806d1..6d4b507a0345f 100644\n--- a/airflow/api/auth/backend/basic_auth.py\n+++ b/airflow/api/auth/backend/basic_auth.py\n@@ -18,15 +18,17 @@\n from __future__ import annotations\n \n from functools import wraps\n-from typing import Any, Callable, TypeVar, cast\n+from typing import TYPE_CHECKING, Any, Callable, TypeVar, cast\n \n from flask import Response, request\n from flask_appbuilder.const import AUTH_LDAP\n from flask_login import login_user\n \n-from airflow.auth.managers.fab.models import User\n from airflow.utils.airflow_flask_app import get_airflow_app\n \n+if TYPE_CHECKING:\n+    from airflow.auth.managers.fab.models import User\n+\n CLIENT_AUTH: tuple[str, str] | Any | None = None\n \n \ndiff --git a/airflow/api/client/__init__.py b/airflow/api/client/__init__.py\nindex 35608032abcb6..26b0fa31f288e 100644\n--- a/airflow/api/client/__init__.py\n+++ b/airflow/api/client/__init__.py\n@@ -19,11 +19,14 @@\n from __future__ import annotations\n \n from importlib import import_module\n+from typing import TYPE_CHECKING\n \n from airflow import api\n-from airflow.api.client.api_client import Client\n from airflow.configuration import conf\n \n+if TYPE_CHECKING:\n+    from airflow.api.client.api_client import Client\n+\n \n def get_current_api_client() -> Client:\n     \"\"\"Return current API Client based on current Airflow configuration.\"\"\"\ndiff --git a/airflow/api/common/delete_dag.py b/airflow/api/common/delete_dag.py\nindex 1d879a667addc..c94b3c39dfeca 100644\n--- a/airflow/api/common/delete_dag.py\n+++ b/airflow/api/common/delete_dag.py\n@@ -19,9 +19,9 @@\n from __future__ import annotations\n \n import logging\n+from typing import TYPE_CHECKING\n \n from sqlalchemy import and_, delete, or_, select\n-from sqlalchemy.orm import Session\n \n from airflow import models\n from airflow.exceptions import AirflowException, DagNotFound\n@@ -31,6 +31,9 @@\n from airflow.utils.session import NEW_SESSION, provide_session\n from airflow.utils.state import TaskInstanceState\n \n+if TYPE_CHECKING:\n+    from sqlalchemy.orm import Session\n+\n log = logging.getLogger(__name__)\n \n \ndiff --git a/airflow/api/common/experimental/__init__.py b/airflow/api/common/experimental/__init__.py\nindex 35a4da3f1735a..e8f5548c4e629 100644\n--- a/airflow/api/common/experimental/__init__.py\n+++ b/airflow/api/common/experimental/__init__.py\n@@ -18,10 +18,15 @@\n \"\"\"Experimental APIs.\"\"\"\n from __future__ import annotations\n \n-from datetime import datetime\n+from typing import TYPE_CHECKING\n \n from airflow.exceptions import DagNotFound, DagRunNotFound, TaskNotFound\n-from airflow.models import DagBag, DagModel, DagRun\n+from airflow.models import DagBag, DagModel\n+\n+if TYPE_CHECKING:\n+    from datetime import datetime\n+\n+    from airflow.models import DagRun\n \n \n def check_and_get_dag(dag_id: str, task_id: str | None = None) -> DagModel:\ndiff --git a/airflow/api/common/experimental/get_dag_run_state.py b/airflow/api/common/experimental/get_dag_run_state.py\nindex cdf044a1569b1..fe246e8f77c21 100644\n--- a/airflow/api/common/experimental/get_dag_run_state.py\n+++ b/airflow/api/common/experimental/get_dag_run_state.py\n@@ -18,12 +18,15 @@\n \"\"\"DAG run APIs.\"\"\"\n from __future__ import annotations\n \n-from datetime import datetime\n+from typing import TYPE_CHECKING\n \n from deprecated import deprecated\n \n from airflow.api.common.experimental import check_and_get_dag, check_and_get_dagrun\n \n+if TYPE_CHECKING:\n+    from datetime import datetime\n+\n \n @deprecated(reason=\"Use DagRun().get_state() instead\", version=\"2.2.4\")\n def get_dag_run_state(dag_id: str, execution_date: datetime) -> dict[str, str]:\ndiff --git a/airflow/api/common/experimental/get_lineage.py b/airflow/api/common/experimental/get_lineage.py\nindex 73bc9dd862ef3..45bf2249c005b 100644\n--- a/airflow/api/common/experimental/get_lineage.py\n+++ b/airflow/api/common/experimental/get_lineage.py\n@@ -19,16 +19,18 @@\n from __future__ import annotations\n \n import collections\n-import datetime\n-from typing import Any\n-\n-from sqlalchemy.orm import Session\n+from typing import TYPE_CHECKING, Any\n \n from airflow.api.common.experimental import check_and_get_dag, check_and_get_dagrun\n from airflow.lineage import PIPELINE_INLETS, PIPELINE_OUTLETS\n from airflow.models.xcom import XCom\n from airflow.utils.session import NEW_SESSION, provide_session\n \n+if TYPE_CHECKING:\n+    import datetime\n+\n+    from sqlalchemy.orm import Session\n+\n \n @provide_session\n def get_lineage(\ndiff --git a/airflow/api/common/experimental/get_task.py b/airflow/api/common/experimental/get_task.py\nindex 34e0fac37983f..8e1c4f641db9e 100644\n--- a/airflow/api/common/experimental/get_task.py\n+++ b/airflow/api/common/experimental/get_task.py\n@@ -18,10 +18,14 @@\n \"\"\"Task APIs.\"\"\"\n from __future__ import annotations\n \n+from typing import TYPE_CHECKING\n+\n from deprecated import deprecated\n \n from airflow.api.common.experimental import check_and_get_dag\n-from airflow.models import TaskInstance\n+\n+if TYPE_CHECKING:\n+    from airflow.models import TaskInstance\n \n \n @deprecated(reason=\"Use DAG().get_task\", version=\"2.2.4\")\ndiff --git a/airflow/api/common/experimental/get_task_instance.py b/airflow/api/common/experimental/get_task_instance.py\nindex cc8c734338284..abc178516c3ef 100644\n--- a/airflow/api/common/experimental/get_task_instance.py\n+++ b/airflow/api/common/experimental/get_task_instance.py\n@@ -18,13 +18,17 @@\n \"\"\"Task instance APIs.\"\"\"\n from __future__ import annotations\n \n-from datetime import datetime\n+from typing import TYPE_CHECKING\n \n from deprecated import deprecated\n \n from airflow.api.common.experimental import check_and_get_dag, check_and_get_dagrun\n from airflow.exceptions import TaskInstanceNotFound\n-from airflow.models import TaskInstance\n+\n+if TYPE_CHECKING:\n+    from datetime import datetime\n+\n+    from airflow.models import TaskInstance\n \n \n @deprecated(version=\"2.2.4\", reason=\"Use DagRun.get_task_instance instead\")\ndiff --git a/airflow/api/common/experimental/pool.py b/airflow/api/common/experimental/pool.py\nindex 1134bda989014..87adacdd76fb7 100644\n--- a/airflow/api/common/experimental/pool.py\n+++ b/airflow/api/common/experimental/pool.py\n@@ -18,14 +18,18 @@\n \"\"\"Pool APIs.\"\"\"\n from __future__ import annotations\n \n+from typing import TYPE_CHECKING\n+\n from deprecated import deprecated\n from sqlalchemy import select\n-from sqlalchemy.orm import Session\n \n from airflow.exceptions import AirflowBadRequest, PoolNotFound\n from airflow.models import Pool\n from airflow.utils.session import NEW_SESSION, provide_session\n \n+if TYPE_CHECKING:\n+    from sqlalchemy.orm import Session\n+\n \n @deprecated(reason=\"Use Pool.get_pool() instead\", version=\"2.2.4\")\n @provide_session\ndiff --git a/airflow/api/common/mark_tasks.py b/airflow/api/common/mark_tasks.py\nindex 4d2df78e82db9..d71957f86b81d 100644\n--- a/airflow/api/common/mark_tasks.py\n+++ b/airflow/api/common/mark_tasks.py\n@@ -18,15 +18,12 @@\n \"\"\"Marks tasks APIs.\"\"\"\n from __future__ import annotations\n \n-from datetime import datetime\n from typing import TYPE_CHECKING, Collection, Iterable, Iterator, NamedTuple\n \n from sqlalchemy import or_, select\n-from sqlalchemy.orm import Session as SASession, lazyload\n+from sqlalchemy.orm import lazyload\n \n-from airflow.models.dag import DAG\n from airflow.models.dagrun import DagRun\n-from airflow.models.operator import Operator\n from airflow.models.taskinstance import TaskInstance\n from airflow.operators.subdag import SubDagOperator\n from airflow.utils import timezone\n@@ -35,6 +32,14 @@\n from airflow.utils.state import DagRunState, State, TaskInstanceState\n from airflow.utils.types import DagRunType\n \n+if TYPE_CHECKING:\n+    from datetime import datetime\n+\n+    from sqlalchemy.orm import Session as SASession\n+\n+    from airflow.models.dag import DAG\n+    from airflow.models.operator import Operator\n+\n \n class _DagRunInfo(NamedTuple):\n     logical_date: datetime\ndiff --git a/airflow/api/common/trigger_dag.py b/airflow/api/common/trigger_dag.py\nindex 01da7745c7bf3..a522b938df449 100644\n--- a/airflow/api/common/trigger_dag.py\n+++ b/airflow/api/common/trigger_dag.py\n@@ -19,7 +19,7 @@\n from __future__ import annotations\n \n import json\n-from datetime import datetime\n+from typing import TYPE_CHECKING\n \n from airflow.exceptions import DagNotFound, DagRunAlreadyExists\n from airflow.models import DagBag, DagModel, DagRun\n@@ -27,6 +27,9 @@\n from airflow.utils.state import DagRunState\n from airflow.utils.types import DagRunType\n \n+if TYPE_CHECKING:\n+    from datetime import datetime\n+\n \n def _trigger_dag(\n     dag_id: str,\n",
    "generated_commit": "[AIRFLOW-3624] Fix get_task params in trigger_dag (#3708)",
    "label_commit": "Improve importing the module in Airflow API package (#33797)",
    "bleu": 0.024822529802838878,
    "rouge_l": 0.10526315789473685,
    "meteor": 0.05681818181818182,
    "bert_score_f1": 0.15693987905979156,
    "answer_correctness": 0.19754869890534893
  },
  {
    "diff": "diff --git a/airflow/api_connexion/endpoints/dag_warning_endpoint.py b/airflow/api_connexion/endpoints/dag_warning_endpoint.py\nindex 367b0ae104571..c9d8207b0f65f 100644\n--- a/airflow/api_connexion/endpoints/dag_warning_endpoint.py\n+++ b/airflow/api_connexion/endpoints/dag_warning_endpoint.py\n@@ -22,7 +22,6 @@\n from sqlalchemy import select\n \n from airflow.api_connexion import security\n-from airflow.api_connexion.exceptions import PermissionDenied\n from airflow.api_connexion.parameters import apply_sorting, check_limit, format_parameters\n from airflow.api_connexion.schemas.dag_warning_schema import (\n     DagWarningCollection,\n@@ -40,7 +39,12 @@\n     from airflow.api_connexion.types import APIResponse\n \n \n-@security.requires_access([(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING)])\n+@security.requires_access(\n+    [\n+        (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),\n+        (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING),\n+    ]\n+)\n @format_parameters({\"limit\": check_limit})\n @provide_session\n def get_dag_warnings(\n@@ -60,8 +64,6 @@ def get_dag_warnings(\n     allowed_filter_attrs = [\"dag_id\", \"warning_type\", \"message\", \"timestamp\"]\n     query = select(DagWarningModel)\n     if dag_id:\n-        if not get_airflow_app().appbuilder.sm.can_read_dag(dag_id, g.user):\n-            raise PermissionDenied(detail=f\"User not allowed to access this DAG: {dag_id}\")\n         query = query.where(DagWarningModel.dag_id == dag_id)\n     else:\n         readable_dags = get_airflow_app().appbuilder.sm.get_accessible_dag_ids(g.user)\n",
    "generated_commit": "[AIRFLOW-XXX] Fix invalid SQL error in the UI (#3736)",
    "label_commit": "Use requires_access to check read permission on dag instead of checking it explicitly (#34940)",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": 0.12013157457113266,
    "answer_correctness": 0.18759021799266318
  },
  {
    "diff": "diff --git a/treeherder/services/pulse/consumers.py b/treeherder/services/pulse/consumers.py\nindex 91aecb8a44a..55862e8f786 100644\n--- a/treeherder/services/pulse/consumers.py\n+++ b/treeherder/services/pulse/consumers.py\n@@ -42,9 +42,12 @@\n     \"exchange/hgpushes/v1.#\",\n ]\n \n-MOZCI_CLASSIFICATION_BINDINGS = [\n+MOZCI_CLASSIFICATION_PRODUCTION_BINDINGS = [\n     \"exchange/taskcluster-queue/v1/task-completed.route.index.project.mozci.classification.#\",\n ]\n+MOZCI_CLASSIFICATION_TESTING_BINDINGS = [\n+    \"exchange/taskcluster-queue/v1/task-completed.route.index.project.mozci.testing.classification.#\",\n+]\n \n \n class PulseConsumer(ConsumerMixin):\n@@ -171,7 +174,15 @@ class MozciClassificationConsumer(PulseConsumer):\n     queue_suffix = env(\"PULSE_MOZCI_CLASSIFICATION_QUEUE_NAME\", default=\"tasksclassification\")\n \n     def bindings(self):\n-        return MOZCI_CLASSIFICATION_BINDINGS\n+        mozci_env = env('PULSE_MOZCI_ENVIRONMENT', default='production')\n+        if mozci_env == 'testing':\n+            return MOZCI_CLASSIFICATION_TESTING_BINDINGS\n+\n+        if mozci_env != 'production':\n+            logger.warning(\n+                f'PULSE_MOZCI_ENVIRONMENT should be testing or production not {mozci_env}, defaulting to production'\n+            )\n+        return MOZCI_CLASSIFICATION_PRODUCTION_BINDINGS\n \n     @newrelic.agent.background_task(\n         name='pulse-listener-tasks-classification.on_message', group='Pulse Listener'\n@@ -228,7 +239,16 @@ def bindings(self):\n         if self.source.get('tasks'):\n             rv += TASKCLUSTER_TASK_BINDINGS\n         if self.source.get('mozci-classification'):\n-            rv += MOZCI_CLASSIFICATION_BINDINGS\n+            mozci_env = env('PULSE_MOZCI_ENVIRONMENT', default='production')\n+            if mozci_env == 'testing':\n+                rv += MOZCI_CLASSIFICATION_TESTING_BINDINGS\n+            else:\n+                if mozci_env != 'production':\n+                    logger.warning(\n+                        f'PULSE_MOZCI_ENVIRONMENT should be testing or production not {mozci_env}, defaulting to production'\n+                    )\n+                rv += MOZCI_CLASSIFICATION_PRODUCTION_BINDINGS\n+\n         return rv\n \n     @newrelic.agent.background_task(name='pulse-joint-listener.on_message', group='Pulse Listener')\n",
    "generated_commit": "rewrite the OUUS bottleneck into account for efficiency (#241)",
    "label_commit": "Switch between Mozci prod/testing bindings with an env variable (#7415)\n\nCo-authored-by: Eva Bardou <ebardou@teklia.com>",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": -0.08003851026296616,
    "answer_correctness": 0.1781231032999646
  },
  {
    "diff": "diff --git a/pontoon/base/views.py b/pontoon/base/views.py\nindex b9e6faedd1..6f5cba5760 100755\n--- a/pontoon/base/views.py\n+++ b/pontoon/base/views.py\n@@ -173,7 +173,8 @@ def _get_entities_list(locale, project, form):\n     \"\"\"\n     entities = (\n         Entity.objects.filter(pk__in=form.cleaned_data['entity_ids'])\n-        .prefetch_resources_translations(locale)\n+        .prefetch_related('resource')\n+        .prefetch_translations(locale)\n         .distinct()\n         .order_by('order')\n     )\n@@ -302,7 +303,8 @@ def batch_edit_translations(request):\n \n     entities = (\n         Entity.objects.filter(pk__in=entity_pks)\n-        .prefetch_resources_translations(locale)\n+        .prefetch_related('resource')\n+        .prefetch_translations(locale)\n     )\n \n     if not entities.exists():\n",
    "generated_commit": "Fixing issue with filtering API (#707)",
    "label_commit": "Fix bug 1408637: EntityQuerySet.prefetch_resources_translations() is deprecated (#734)",
    "bleu": 0,
    "rouge_l": 0.125,
    "meteor": 0.07246376811594203,
    "bert_score_f1": 0.14631111919879913,
    "answer_correctness": 0.19219225179000815
  },
  {
    "diff": "diff --git a/knowledge_repo/utils/encoding.py b/knowledge_repo/utils/encoding.py\nindex b0188ae9d..9d56f2547 100644\n--- a/knowledge_repo/utils/encoding.py\n+++ b/knowledge_repo/utils/encoding.py\n@@ -1,6 +1,6 @@\n+import logging\n import os\n import sys\n-import logging\n \n \n __all__ = ['encode', 'decode']\ndiff --git a/knowledge_repo/utils/exec_code.py b/knowledge_repo/utils/exec_code.py\nindex b0e12f0dd..bba3bc3d6 100644\n--- a/knowledge_repo/utils/exec_code.py\n+++ b/knowledge_repo/utils/exec_code.py\n@@ -1,5 +1,5 @@\n-import sys\n import importlib\n+import sys\n import time\n \n \ndiff --git a/knowledge_repo/utils/git.py b/knowledge_repo/utils/git.py\nindex f55ef2aa7..5087a3486 100644\n--- a/knowledge_repo/utils/git.py\n+++ b/knowledge_repo/utils/git.py\n@@ -1,4 +1,3 @@\n-import errno\n import os\n import shutil\n import tempfile\ndiff --git a/knowledge_repo/utils/import_submodules.py b/knowledge_repo/utils/import_submodules.py\nindex c70acf652..738d4b518 100644\n--- a/knowledge_repo/utils/import_submodules.py\n+++ b/knowledge_repo/utils/import_submodules.py\n@@ -1,6 +1,6 @@\n-import sys\n import importlib\n import pkgutil\n+import sys\n \n \n def import_submodules(package_name):\ndiff --git a/knowledge_repo/utils/types.py b/knowledge_repo/utils/types.py\nindex 98098caa8..3232bdcb8 100644\n--- a/knowledge_repo/utils/types.py\n+++ b/knowledge_repo/utils/types.py\n@@ -1,5 +1,3 @@\n-import sys\n-\n from sqlalchemy import Text\n from sqlalchemy.dialects.mysql import MEDIUMTEXT\n \n",
    "generated_commit": "Remove unused imports",
    "label_commit": "Adjust the sequence of import statements based on alphabetical order",
    "bleu": 0,
    "rouge_l": 0.15384615384615383,
    "meteor": 0.053763440860215055,
    "bert_score_f1": 0.06475330889225006,
    "answer_correctness": 0.20357378636809897
  },
  {
    "diff": "diff --git a/celery/bin/celerybeat.py b/celery/bin/celerybeat.py\nindex 71b10156582..c277388b004 100644\n--- a/celery/bin/celerybeat.py\n+++ b/celery/bin/celerybeat.py\n@@ -72,7 +72,7 @@\n             help=\"Path to the schedule database. The extension \\\n                     '.db' will be appended to the filename. Default: %s\" % (\n                     conf.CELERYBEAT_SCHEDULE_FILENAME)),\n-    optparse.make_option('-f', '--logfile', default=conf.CELERYBEAT_LOG_FILE,\n+    optparse.make_option('-f', '--logfile', default=EAT_LOG_FILE,\n             action=\"store\", dest=\"logfile\",\n             help=\"Path to log file.\"),\n     optparse.make_option('-l', '--loglevel',\n@@ -115,8 +115,6 @@ def run_clockservice(detach=False, loglevel=conf.CELERYBEAT_LOG_LEVEL,\n     # Setup logging\n     if not isinstance(loglevel, int):\n         loglevel = conf.LOG_LEVELS[loglevel.upper()]\n-    if not detach:\n-        logfile = None # log to stderr when not running in the background.\n \n     # Run the worker init handler.\n     # (Usually imports task modules and such.)\ndiff --git a/celery/bin/celeryd.py b/celery/bin/celeryd.py\nindex 047244c7112..85132e53940 100644\n--- a/celery/bin/celeryd.py\n+++ b/celery/bin/celeryd.py\n@@ -165,8 +165,6 @@ def run_worker(concurrency=conf.CELERYD_CONCURRENCY, detach=False,\n     # Setup logging\n     if not isinstance(loglevel, int):\n         loglevel = conf.LOG_LEVELS[loglevel.upper()]\n-    if not detach:\n-        logfile = None # log to stderr when not running in the background.\n \n     if discard:\n         discarded_count = discard_all()\ndiff --git a/celery/conf.py b/celery/conf.py\nindex 6ade4a8cf7a..babb49007d6 100644\n--- a/celery/conf.py\n+++ b/celery/conf.py\n@@ -30,15 +30,15 @@\n     \"CELERYD_CONCURRENCY\": 0, # defaults to cpu count\n     \"CELERYD_LOG_FORMAT\": DEFAULT_P_LOG_FMT,\n     \"CELERYD_LOG_LEVEL\": \"WARN\",\n-    \"CELERYD_LOG_FILE\": \"celeryd.log\",\n+    \"CELERYD_LOG_FILE\": None, # stderr\n     \"CELERYD_PID_FILE\": \"celeryd.pid\",\n     \"CELERYBEAT_SCHEDULE_FILENAME\": \"celerybeat-schedule\",\n     \"CELERYBEAT_MAX_LOOP_INTERVAL\": 5 * 60, # five minutes.\n     \"CELERYBEAT_LOG_LEVEL\": \"INFO\",\n-    \"CELERYBEAT_LOG_FILE\": \"celerybeat.log\",\n+    \"CELERYBEAT_LOG_FILE\": None, # stderr\n     \"CELERYBEAT_PID_FILE\": \"celerybeat.pid\",\n     \"CELERYMON_LOG_LEVEL\": \"INFO\",\n-    \"CELERYMON_LOG_FILE\": \"celerymon.log\",\n+    \"CELERYMON_LOG_FILE\": None, # stderr\n     \"CELERYMON_PID_FILE\": \"celerymon.pid\",\n     \"CELERYMON_LOG_FORMAT\": DEFAULT_LOG_FMT,\n     \"CELERY_BROADCAST_QUEUE\": \"celeryctl\",\n",
    "generated_commit": "Detach celery settings fixes",
    "label_commit": "Allow for --logfile to work even if not detached, also set all default log files to None (stderr)",
    "bleu": 0,
    "rouge_l": 0.1818181818181818,
    "meteor": 0.03012048192771084,
    "bert_score_f1": -0.08381593227386475,
    "answer_correctness": 0.17545568077977283
  },
  {
    "diff": "diff --git a/scripts/in_review.py b/scripts/in_review.py\nnew file mode 100755\nindex 00000000000..9154d0b8565\n--- /dev/null\n+++ b/scripts/in_review.py\n@@ -0,0 +1,351 @@\n+#!/usr/bin/env python\n+import datetime\n+import subprocess\n+import sys\n+import textwrap\n+import urllib\n+\n+import requests\n+\n+\n+BUGZILLA_API_URL = 'https://bugzilla.mozilla.org/rest/'\n+BUGZILLA_PRODUCT = 'support.mozilla.org'\n+\n+QUARTERS = {\n+    1: [(1, 1), (3, 31)],\n+    2: [(4, 1), (6, 30)],\n+    3: [(7, 1), (9, 30)],\n+    4: [(10, 1), (12, 31)]\n+}\n+\n+USAGE = 'Usage: in_review.py <YEAR> [<QUARTER>]'\n+HEADER = 'in_review.py: find out what happened year or quarter!'\n+\n+all_people = set()\n+\n+\n+def wrap(text, indent='    ', subsequent='    '):\n+    text = text.split('\\n\\n')\n+    text = [textwrap.fill(part, expand_tabs=True, initial_indent=indent,\n+                          subsequent_indent=subsequent)\n+            for part in text]\n+    return '\\n\\n'.join(text)\n+\n+\n+def fetch(url):\n+    resp = requests.get(url)\n+    if resp.status_code != 200:\n+        raise Exception(resp.text)\n+\n+    json_data = resp.json()\n+\n+    if json_data.get('error'):\n+        raise Exception('ERROR ' + repr(json_data))\n+\n+    return json_data\n+\n+\n+def fetch_bugs(params):\n+    url = BUGZILLA_API_URL + '/bug' + '?' + urllib.urlencode(params)\n+    return fetch(url)\n+\n+\n+def fetch_bug_history(bugid):\n+    url = BUGZILLA_API_URL + ('/bug/%d/history' % bugid)\n+    return fetch(url)\n+\n+\n+def fetch_bug_comments(bugid):\n+    url = BUGZILLA_API_URL + ('/bug/%d/comment' % bugid)\n+    return fetch(url)\n+\n+\n+def print_bugzilla_stats(from_date, to_date):\n+    # ------------------------------------------------\n+    # Bug creation stats\n+    # ------------------------------------------------\n+    params = {\n+        'product': BUGZILLA_PRODUCT,\n+        'f1': 'creation_ts',\n+        'o1': 'greaterthaneq',\n+        'v1': from_date.strftime('%Y-%m-%d'),\n+        'f2': 'creation_ts',\n+        'o2': 'lessthan',\n+        'v2': to_date.strftime('%Y-%m-%d')\n+    }\n+\n+    json_data = fetch_bugs(params)\n+\n+    creation_count = len(json_data['bugs'])\n+\n+    creators = {}\n+    for bug in json_data['bugs']:\n+        creator = bug['creator_detail']['real_name']\n+        if not creator:\n+            creator = bug['creator'].split('@')[0]\n+        creators[creator] = creators.get(creator, 0) + 1\n+        all_people.add(creator)\n+\n+    print 'Bugs created: %s' % creation_count\n+    print 'Creators: %s' % len(creators)\n+    print ''\n+    creators = sorted(creators.items(), reverse=True, key=lambda item: item[1])\n+    for person, count in creators:\n+        print ' %34s : %s' % (person[:30], count)\n+    print ''\n+\n+    # ------------------------------------------------\n+    # Bug resolution stats\n+    # ------------------------------------------------\n+    params = {\n+        'product': BUGZILLA_PRODUCT,\n+        'f1': 'cf_last_resolved',\n+        'o1': 'greaterthaneq',\n+        'v1': from_date.strftime('%Y-%m-%d'),\n+        'f2': 'cf_last_resolved',\n+        'o2': 'lessthan',\n+        'v2': to_date.strftime('%Y-%m-%d')\n+    }\n+\n+    json_data = fetch_bugs(params)\n+\n+    resolved_count = len(json_data['bugs'])\n+    resolved_map = {}\n+    resolvers = {}\n+    traceback_bugs = []\n+    research_bugs = []\n+    tracker_bugs = []\n+    commenters = {}\n+\n+    for bug in json_data['bugs']:\n+        summary = bug['summary'].lower()\n+        if summary.startswith('[traceback]'):\n+            traceback_bugs.append(bug)\n+        elif summary.startswith('[research]'):\n+            research_bugs.append(bug)\n+        elif summary.startswith('[tracker]'):\n+            tracker_bugs.append(bug)\n+\n+        history = fetch_bug_history(bug['id'])\n+\n+        resolution = bug['resolution']\n+        resolved_map[resolution] = resolved_map.get(resolution, 0) + 1\n+        assigned = bug['assigned_to_detail']['real_name']\n+        if not assigned:\n+            assigned = bug['assigned_to'].split('@')[0]\n+\n+        if 'nobody' in assigned.lower():\n+            # If no one was assigned, we give \"credit\" to whoever\n+            # triaged the bug. We go through the history in reverse\n+            # order because the \"resolver\" is the last person to\n+            # resolve the bug.\n+            for item in reversed(history['bugs'][0]['history']):\n+                # See if this item in the history is the resolving event.\n+                # If it is, then we know who resolved the bug and we\n+                # can stop looking at history.\n+                changes = [change for change in item['changes']\n+                           if change['field_name'] == 'status' and\n+                           change['added'] == 'RESOLVED']\n+\n+                if not changes:\n+                    continue\n+\n+                assigned = item['who']\n+                break\n+\n+        if assigned:\n+            if '@' in assigned:\n+                assigned = assigned.split('@')[0]\n+\n+            resolvers[assigned] = resolvers.get(assigned, 0) + 1\n+            all_people.add(assigned)\n+\n+        # Now get all the commenters\n+        comments = fetch_bug_comments(bug['id'])\n+        # The Bugzilla REST api has some interesting things about it.\n+        for comment in comments['bugs'][str(bug['id'])]['comments']:\n+            commenter = comment['author']\n+            if '@' in commenter:\n+                commenter = commenter.split('@')[0]\n+\n+            commenters[commenter] = commenters.get(commenter, 0) + 1\n+            all_people.add(commenter)\n+\n+    print 'Bugs resolved: %s' % resolved_count\n+    print ''\n+    for resolution, count in resolved_map.items():\n+        print ' %34s : %s' % (resolution, count)\n+\n+    print ''\n+    for title, count in [('Tracebacks', len(traceback_bugs)),\n+                         ('Research', len(research_bugs)),\n+                         ('Tracker', len(tracker_bugs))]:\n+        print ' %34s : %s' % (title, count)\n+\n+    print ''\n+    print 'Research bugs: %s' % len(research_bugs)\n+    print ''\n+    for bug in research_bugs:\n+        print wrap('%s: %s' % (bug['id'], bug['summary']),\n+                   subsequent='        ')\n+\n+    print ''\n+    print 'Tracker bugs: %s' % len(tracker_bugs)\n+    print ''\n+    for bug in tracker_bugs:\n+        print wrap('%s: %s' % (bug['id'], bug['summary']),\n+                   subsequent='        ')\n+\n+    print ''\n+    print 'Resolvers: %s' % len(resolvers)\n+    print ''\n+    resolvers = sorted(resolvers.items(), reverse=True,\n+                       key=lambda item: item[1])\n+    for person, count in resolvers:\n+        print ' %34s : %s' % (person[:30], count)\n+\n+    print ''\n+    print 'Commenters: %s' % len(commenters)\n+    print ''\n+    commenters = sorted(commenters.items(), reverse=True,\n+                        key=lambda item: item[1])\n+    for person, count in commenters:\n+        print ' %34s : %s' % (person[:30], count)\n+\n+\n+def git(*args):\n+    return subprocess.check_output(args)\n+\n+\n+def print_git_stats(from_date, to_date):\n+    all_commits = subprocess.check_output([\n+        'git', 'log',\n+        '--after=' + from_date.strftime('%Y-%m-%d'),\n+        '--before=' + to_date.strftime('%Y-%m-%d'),\n+        '--format=%H'\n+    ])\n+\n+    all_commits = all_commits.splitlines()\n+\n+    # Person -> # commits\n+    committers = {}\n+\n+    # Person -> (# files changed, # inserted, # deleted)\n+    changes = {}\n+\n+    for commit in all_commits:\n+        author = git('git', 'log', '--format=%an',\n+                     '{0}~..{1}'.format(commit, commit))\n+\n+        author = author.strip()\n+        # FIXME - this is lame. what's going on is that there are\n+        # merge commits which have multiple authors, so we just grab\n+        # the second one.\n+        if '\\n' in author:\n+            author = author.splitlines()[1]\n+\n+        committers[author] = committers.get(author, 0) + 1\n+        all_people.add(author)\n+\n+        diff_data = git('git', 'diff', '--numstat', '--find-copies-harder',\n+                        '{0}~..{1}'.format(commit, commit))\n+        total_added = 0\n+        total_deleted = 0\n+        total_files = 0\n+\n+        for line in diff_data.splitlines():\n+            added, deleted, fn = line.split('\\t')\n+            if fn.startswith('vendor/'):\n+                continue\n+            if added != '-':\n+                total_added += int(added)\n+            if deleted != '-':\n+                total_deleted += int(deleted)\n+            total_files += 1\n+\n+        old_changes = changes.get(author, (0, 0, 0))\n+        changes[author] = (\n+            old_changes[0] + total_added,\n+            old_changes[1] + total_deleted,\n+            old_changes[2] + total_files\n+        )\n+\n+    print 'Total commits:', len(all_commits)\n+    print ''\n+\n+    committers = sorted(\n+        committers.items(), key=lambda item: item[1], reverse=True)\n+    for person, count in committers:\n+        print '  %20s : %5s  (+%s, -%s, files %s)' % (\n+            person, count,\n+            changes[person][0], changes[person][1], changes[person][2])\n+        all_people.add(person)\n+\n+    # This is goofy summing, but whatevs.\n+    print ''\n+    print 'Total lines added:', sum([item[0] for item in changes.values()])\n+    print 'Total lines deleted:', sum([item[1] for item in changes.values()])\n+    print 'Total files changed:', sum([item[2] for item in changes.values()])\n+\n+\n+def print_all_people():\n+    # We do this sorting thing to make it a little easier to suss out\n+    # duplicates since we're pulling names from three different forms\n+    # between Bugzilla and git. You're still going to have to go\n+    # through it by hand to remove duplicates.\n+    people = sorted(all_people, key=lambda a: a.decode('utf-8').lower())\n+\n+    for person in people:\n+        print '    ' + person\n+\n+\n+def print_header(text):\n+    print ''\n+    print text\n+    print '=' * len(text)\n+    print ''\n+\n+\n+def main(argv):\n+    # XXX: This helps debug bugzilla xmlrpc bits.\n+    # logging.basicConfig(level=logging.DEBUG)\n+\n+    if len(argv) < 1:\n+        print USAGE\n+        print 'Error: Must specify year or year and quarter. Examples:'\n+        print 'in_review.py 2014'\n+        print 'in_review.py 2014 1'\n+        return 1\n+\n+    print HEADER\n+\n+    year = int(argv[0])\n+    if len(argv) == 1:\n+        from_date = datetime.date(year, 1, 1)\n+        to_date = datetime.date(year, 12, 31)\n+        print_header('Year %s (%s -> %s)' % (year, from_date, to_date))\n+\n+    else:\n+        quarter = int(argv[1])\n+        quarter_dates = QUARTERS[quarter]\n+\n+        from_date = datetime.date(\n+            year, quarter_dates[0][0], quarter_dates[0][1])\n+        to_date = datetime.date(\n+            year, quarter_dates[1][0], quarter_dates[1][1])\n+\n+        print_header('Quarter %sq%s (%s -> %s)' % (\n+            year, quarter, from_date, to_date))\n+\n+    print_header('Bugzilla')\n+    print_bugzilla_stats(from_date, to_date)\n+\n+    print_header('git')\n+    print_git_stats(from_date, to_date)\n+\n+    print_header('Everyone')\n+    print_all_people()\n+\n+\n+if __name__ == '__main__':\n+    sys.exit(main(sys.argv[1:]))\n",
    "generated_commit": "Add code to test the number of events that were accidentally consumed",
    "label_commit": "Cosmetic: Add in_review.py script",
    "bleu": 0.017033186037639283,
    "rouge_l": 0.1111111111111111,
    "meteor": 0.10416666666666667,
    "bert_score_f1": 0.24716711044311523,
    "answer_correctness": 0.17627646213612772
  },
  {
    "diff": "diff --git a/sky/cloud_stores.py b/sky/cloud_stores.py\nindex 89f8e02cb5b..33aab44501d 100644\n--- a/sky/cloud_stores.py\n+++ b/sky/cloud_stores.py\n@@ -133,7 +133,7 @@ def is_directory(self, url: str) -> bool:\n     def make_sync_dir_command(self, source: str, destination: str) -> str:\n         \"\"\"Downloads a directory using gsutil.\"\"\"\n         download_via_gsutil = (\n-            f'{self._GSUTIL} -m rsync -r {source} {destination}')\n+            f'{self._GSUTIL} -m rsync -e -r {source} {destination}')\n         all_commands = [self._GET_GSUTIL]\n         all_commands.append(download_via_gsutil)\n         return ' && '.join(all_commands)\ndiff --git a/sky/data/storage.py b/sky/data/storage.py\nindex bb2c9ab2ebb..7ee08fbb674 100644\n--- a/sky/data/storage.py\n+++ b/sky/data/storage.py\n@@ -1507,14 +1507,15 @@ def batch_gsutil_rsync(self,\n \n         def get_file_sync_command(base_dir_path, file_names):\n             sync_format = '|'.join(file_names)\n-            sync_command = (f'gsutil -m rsync -x \\'^(?!{sync_format}$).*\\' '\n+            sync_command = (f'gsutil -m rsync -e -x \\'^(?!{sync_format}$).*\\' '\n                             f'{base_dir_path} gs://{self.name}')\n             return sync_command\n \n         def get_dir_sync_command(src_dir_path, dest_dir_name):\n             # we exclude .git directory from the sync\n-            sync_command = (f'gsutil -m rsync -r -x \\'.git/*\\' {src_dir_path} '\n-                            f'gs://{self.name}/{dest_dir_name}')\n+            sync_command = (\n+                f'gsutil -m rsync -e -r -x \\'.git/*\\' {src_dir_path} '\n+                f'gs://{self.name}/{dest_dir_name}')\n             return sync_command\n \n         # Generate message for upload\n",
    "generated_commit": "Fix bug in cloud store (#3621)\n\nThis PR fixes that issue were caused by missing external changes to\nthe old behaviour of this commit.",
    "label_commit": "[Storage] gsutil rsync failing to sync source with dangling symlink (#2165)\n\n* update_gsutil_rsync_cmd\n\n* fixes\n\n* format\n\n* nit\n\n* nit",
    "bleu": 0.009410489957951634,
    "rouge_l": 0.04651162790697675,
    "meteor": 0.04694835680751174,
    "bert_score_f1": 0.022087808698415756,
    "answer_correctness": 0.34036123558491393
  },
  {
    "diff": "diff --git a/privaterelay/tests/views_tests.py b/privaterelay/tests/views_tests.py\nindex 4cffe86db7..66f78b7cfb 100644\n--- a/privaterelay/tests/views_tests.py\n+++ b/privaterelay/tests/views_tests.py\n@@ -127,6 +127,27 @@ def test_update_all_data_conflict(self):\n         assert sa2.extra_data == extra_data\n         assert ea2.email == \"user2@example.com\"\n \n+    def test_update_all_data_no_email_address(self):\n+        user = baker.make(User)\n+        sa = baker.make(\n+            SocialAccount,\n+            user=user,\n+            provider=\"fxa\",\n+            extra_data=json.loads('{\"test\": \"test\"}'),\n+        )\n+        new_extra_data = json.loads('{\"test\": \"updated\"}')\n+        new_email = \"newemail@example.com\"\n+\n+        response = _update_all_data(sa, new_extra_data, new_email)\n+\n+        assert response.status_code == 202\n+\n+        sa.refresh_from_db()\n+        assert sa.extra_data == new_extra_data\n+\n+        ea = sa.user.emailaddress_set.get()\n+        assert ea.email == new_email\n+\n \n @pytest.mark.django_db\n def test_logout_page(client, settings):\ndiff --git a/privaterelay/views.py b/privaterelay/views.py\nindex 41da4fee99..06790d221e 100644\n--- a/privaterelay/views.py\n+++ b/privaterelay/views.py\n@@ -351,8 +351,11 @@ def _update_all_data(\n             social_account.user.email = new_email\n             social_account.user.save()\n             email_address_record = social_account.user.emailaddress_set.first()\n-            email_address_record.email = new_email\n-            email_address_record.save()\n+            if email_address_record:\n+                email_address_record.email = new_email\n+                email_address_record.save()\n+            else:\n+                social_account.user.emailaddress_set.create(email=new_email)\n             return HttpResponse(\"202 Accepted\", status=202)\n     except IntegrityError as e:\n         sentry_sdk.capture_exception(e)\n",
    "generated_commit": "Add an integration test to check for hostname address (#2254)",
    "label_commit": "Handle FxA change event without emailaddress",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": 0.15557420253753662,
    "answer_correctness": 0.17358559186927078
  },
  {
    "diff": "diff --git a/mkt/comm/utils_mail.py b/mkt/comm/utils_mail.py\nindex d99e20b79d3..1c165a47691 100644\n--- a/mkt/comm/utils_mail.py\n+++ b/mkt/comm/utils_mail.py\n@@ -104,7 +104,7 @@ def email_recipients(recipients, note, template=None):\n                         get_mail_context(note), recipient_list=[email],\n                         from_email=settings.MKT_REVIEWERS_EMAIL,\n                         perm_setting='app_reviewed',\n-                        headers={'reply_to': reply_to})\n+                        headers={'Reply-To': reply_to})\n \n \n def get_mail_context(note):\n",
    "generated_commit": "Add another possible to send the message to the email address",
    "label_commit": "fix reply to header name for commbadge (bug 1075233)",
    "bleu": 0.018850319022747353,
    "rouge_l": 0.09999999999999999,
    "meteor": 0.05434782608695651,
    "bert_score_f1": -0.10901439189910889,
    "answer_correctness": 0.18023426527683084
  },
  {
    "diff": "diff --git a/knowledge_repo/app/deploy/gunicorn.py b/knowledge_repo/app/deploy/gunicorn.py\nindex 47a9d5846..456c1b3f5 100644\n--- a/knowledge_repo/app/deploy/gunicorn.py\n+++ b/knowledge_repo/app/deploy/gunicorn.py\n@@ -38,7 +38,7 @@ def load_config(self):\n         # Update the configuration with the options specified\n         # via KnowledgeDeployer\n         options = {\n-            'bind': '{}:{}'.format(self.host, self.port),\n+            'bind': f'{self.host}:{self.port}',\n             'workers': self.workers,\n             'timeout': self.timeout,\n         }\n@@ -63,13 +63,13 @@ def load_config_from_filename(self, filename):\n         \"\"\"\n \n         if not os.path.exists(filename):\n-            raise RuntimeError(\"%r doesn't exist\" % filename)\n+            raise RuntimeError(f\"{filename} doesn't exist\")\n \n         ext = os.path.splitext(filename)[1]\n \n         try:\n             module_name = '__config__'\n-            if ext in [\".py\", \".pyc\"]:\n+            if ext in ['.py', '.pyc']:\n                 spec = importlib.util.spec_from_file_location(\n                     module_name, filename)\n             else:\n@@ -83,7 +83,7 @@ def load_config_from_filename(self, filename):\n             sys.modules[module_name] = mod\n             spec.loader.exec_module(mod)\n         except Exception:\n-            print(\"Failed to read config file: %s\" % filename, file=sys.stderr)\n+            print(f'Failed to read config file: {filename}', file=sys.stderr)\n             traceback.print_exc()\n             sys.stderr.flush()\n             sys.exit(1)\n@@ -97,7 +97,7 @@ def load_config_from_filename(self, filename):\n             try:\n                 self.cfg.set(k.lower(), v)\n             except Exception:\n-                print(\"Invalid value for %s: %s\\n\" % (k, v), file=sys.stderr)\n+                print(f'Invalid value for {k}: {v}\\n', file=sys.stderr)\n                 sys.stderr.flush()\n                 raise\n \n",
    "generated_commit": "Fix missing error message when loading from env",
    "label_commit": "Using the latest string format for embedded variables",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": 0.2704899311065674,
    "answer_correctness": 0.18559586311760212
  },
  {
    "diff": "diff --git a/emails/migrations/0029_profile_add_deleted_metric_and_changeserver_storage_default.py b/emails/migrations/0029_profile_add_deleted_metric_and_changeserver_storage_default.py\nnew file mode 100644\nindex 0000000000..d3697f3814\n--- /dev/null\n+++ b/emails/migrations/0029_profile_add_deleted_metric_and_changeserver_storage_default.py\n@@ -0,0 +1,29 @@\n+# Generated by Django 2.2.24 on 2021-10-19 15:38\n+\n+from django.db import migrations, models\n+import emails.models\n+\n+\n+class Migration(migrations.Migration):\n+\n+    dependencies = [\n+        ('emails', '0028_copy_subdomain_to_registeredsubdomain'),\n+    ]\n+\n+    operations = [\n+        migrations.AddField(\n+            model_name='profile',\n+            name='num_email_blocked_in_deleted_address',\n+            field=models.PositiveIntegerField(default=0),\n+        ),\n+        migrations.AddField(\n+            model_name='profile',\n+            name='num_email_forwarded_in_deleted_address',\n+            field=models.PositiveIntegerField(default=0),\n+        ),\n+        migrations.AlterField(\n+            model_name='profile',\n+            name='server_storage',\n+            field=models.BooleanField(default=emails.models.default_server_storage),\n+        ),\n+    ]\ndiff --git a/emails/models.py b/emails/models.py\nindex d4886ed510..3839ed9232 100644\n--- a/emails/models.py\n+++ b/emails/models.py\n@@ -79,6 +79,8 @@ class Profile(models.Model):\n     last_hard_bounce = models.DateTimeField(\n         blank=True, null=True, db_index=True\n     )\n+    num_email_forwarded_in_deleted_address = models.PositiveIntegerField(default=0)\n+    num_email_blocked_in_deleted_address = models.PositiveIntegerField(default=0)\n     subdomain = models.CharField(\n         blank=True, null=True, unique=True, max_length=63, db_index=True,\n         validators=[valid_available_subdomain]\n",
    "generated_commit": "Docs",
    "label_commit": "Add deleted address metric on profile\nand create migration for field changes on profile",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": -0.10773029178380966,
    "answer_correctness": 0.1690959967722643
  },
  {
    "diff": "diff --git a/framework/pym/play/cmdloader.py b/framework/pym/play/cmdloader.py\nindex b0f10fef85..80138bd0d5 100644\n--- a/framework/pym/play/cmdloader.py\n+++ b/framework/pym/play/cmdloader.py\n@@ -36,5 +36,10 @@ def _load_cmd_from(self, mod):\n \n def load_python_module(name, location):\n     mod_desc = imp.find_module(name, [location])\n-    return imp.load_module(name, mod_desc[0], mod_desc[1], mod_desc[2])\n+    mod_file = mod_desc[0]\n+    try:\n+        return imp.load_module(name, mod_desc[0], mod_desc[1], mod_desc[2])\n+    finally:\n+        if mod_file is not None and not mod_file.closed:\n+            mod_file.close()\n \n",
    "generated_commit": "Fix bug in the loading of python module (#214)",
    "label_commit": "Ensure files opened by Python are closed when loading modules",
    "bleu": 0.02145851237135921,
    "rouge_l": 0.2105263157894737,
    "meteor": 0.15151515151515152,
    "bert_score_f1": 0.29951030015945435,
    "answer_correctness": 0.7035475554507208
  },
  {
    "diff": "diff --git a/privaterelay/tests/mgmt_update_phone_remaining_stats_tests.py b/privaterelay/tests/mgmt_update_phone_remaining_stats_tests.py\nindex 5837a0164f..7a1ac53cc7 100644\n--- a/privaterelay/tests/mgmt_update_phone_remaining_stats_tests.py\n+++ b/privaterelay/tests/mgmt_update_phone_remaining_stats_tests.py\n@@ -304,3 +304,15 @@ def test_phone_subscriber_with_subscription_end_date_after_reset_phone_limits_up\n     assert num_profiles_updated == 1\n     relay_number.refresh_from_db()\n     assert relay_number.remaining_texts == settings.MAX_TEXTS_PER_BILLING_CYCLE\n+\n+\n+@pytest.mark.django_db\n+def test_update_phones_command(capsys):\n+    call_command(\"update_phone_remaining_stats\")\n+    out, err = capsys.readouterr()\n+\n+    out = out.split(\" \")\n+    numProfiles, numReset = int(out[2]), int(out[4])\n+\n+    assert numProfiles == 0\n+    assert numReset == 0\n",
    "generated_commit": "Add tests for update cherry-pick conflict (#2272)",
    "label_commit": "added a test for the Command() in update_phone_remaining_stats.py for the remaining coverage",
    "bleu": 0.016170365420297903,
    "rouge_l": 0.25,
    "meteor": 0.08695652173913043,
    "bert_score_f1": 0.1488715410232544,
    "answer_correctness": 0.1962389031791532
  },
  {
    "diff": "diff --git a/airflow/providers/databricks/hooks/databricks_base.py b/airflow/providers/databricks/hooks/databricks_base.py\nindex a68fe5aaa4526..1f8680e41e42b 100644\n--- a/airflow/providers/databricks/hooks/databricks_base.py\n+++ b/airflow/providers/databricks/hooks/databricks_base.py\n@@ -144,7 +144,7 @@ def user_agent_value(self) -> str:\n         python_version = platform.python_version()\n         system = platform.system().lower()\n         ua_string = (\n-            f\"databricks-aiflow/{version} _/0.0.0 python/{python_version} os/{system} \"\n+            f\"databricks-airflow/{version} _/0.0.0 python/{python_version} os/{system} \"\n             f\"airflow/{__version__} operator/{self.caller}\"\n         )\n         return ua_string\n",
    "generated_commit": "[AIRFLOW-XXX] Fix AttributeError in the correct version of `spark()` (#4337)",
    "label_commit": "Databricks: fix provider name in the User-Agent string (#25873)\n\n`databricks-aiflow` -> `databricks-airflow` (:facepalm:)",
    "bleu": 0.03306446654126846,
    "rouge_l": 0.23076923076923075,
    "meteor": 0.20122484689413825,
    "bert_score_f1": 0.0669238418340683,
    "answer_correctness": 0.7446093721942291
  },
  {
    "diff": "diff --git a/sites/paymentsalt/settings_base.py b/sites/paymentsalt/settings_base.py\nindex 33ff19ccf33..6440c5b1de7 100644\n--- a/sites/paymentsalt/settings_base.py\n+++ b/sites/paymentsalt/settings_base.py\n@@ -106,14 +106,14 @@\n \n RESPONSYS_ID = private.RESPONSYS_ID\n \n-CRONJOB_LOCK_PREFIX = 'marketplace-stage'\n+CRONJOB_LOCK_PREFIX = 'marketplace-paymentsalt'\n \n GOOGLE_ANALYTICS_CREDENTIALS = private.GOOGLE_ANALYTICS_CREDENTIALS\n GOOGLE_API_CREDENTIALS = private.GOOGLE_API_CREDENTIALS\n \n ES_HOSTS = splitstrip(private.ES_HOSTS)\n ES_URLS = ['http://%s' % h for h in ES_HOSTS]\n-ES_INDEXES = dict((k, '%s_stage' % v) for k, v in ES_INDEXES.items())\n+ES_INDEXES = dict((k, '%s_paymentsalt' % v) for k, v in ES_INDEXES.items())\n \n STATSD_HOST = private.STATSD_HOST\n STATSD_PORT = private.STATSD_PORT\n",
    "generated_commit": "Update settings_base to use new CA & default (#172)",
    "label_commit": "update CRONJOB_LOCK_PREFIX and ES_INDEXES to use paymentsalt.",
    "bleu": 0.050712153369465586,
    "rouge_l": 0.3157894736842105,
    "meteor": 0.35493827160493824,
    "bert_score_f1": 0.1248498186469078,
    "answer_correctness": 0.18129768609928182
  },
  {
    "diff": "diff --git a/airflow/providers/amazon/aws/hooks/aws_dynamodb.py b/airflow/providers/amazon/aws/hooks/aws_dynamodb.py\nindex f197aa7aece7f..2062e528bc974 100644\n--- a/airflow/providers/amazon/aws/hooks/aws_dynamodb.py\n+++ b/airflow/providers/amazon/aws/hooks/aws_dynamodb.py\n@@ -20,6 +20,8 @@\n \"\"\"\n This module contains the AWS DynamoDB hook\n \"\"\"\n+from typing import Iterable, List, Optional\n+\n from airflow.exceptions import AirflowException\n from airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\n \n@@ -40,12 +42,15 @@ class AwsDynamoDBHook(AwsBaseHook):\n     :type table_name: str\n     \"\"\"\n \n-    def __init__(self, *args, table_keys=None, table_name=None, **kwargs):\n+    def __init__(\n+        self, *args, table_keys: Optional[List] = None, table_name: Optional[str] = None, **kwargs\n+    ) -> None:\n         self.table_keys = table_keys\n         self.table_name = table_name\n-        super().__init__(resource_type='dynamodb', *args, **kwargs)\n+        kwargs[\"resource_type\"] = \"dynamodb\"\n+        super().__init__(*args, **kwargs)\n \n-    def write_batch_data(self, items):\n+    def write_batch_data(self, items: Iterable):\n         \"\"\"\n         Write batch items to DynamoDB table with provisioned throughout capacity.\n         \"\"\"\n@@ -58,5 +63,5 @@ def write_batch_data(self, items):\n             return True\n         except Exception as general_error:\n             raise AirflowException(\n-                'Failed to insert items in dynamodb, error: {error}'.format(error=str(general_error))\n+                \"Failed to insert items in dynamodb, error: {error}\".format(error=str(general_error))\n             )\ndiff --git a/airflow/providers/amazon/aws/hooks/base_aws.py b/airflow/providers/amazon/aws/hooks/base_aws.py\nindex 674beff50b4b6..8b1a5a7639aad 100644\n--- a/airflow/providers/amazon/aws/hooks/base_aws.py\n+++ b/airflow/providers/amazon/aws/hooks/base_aws.py\n@@ -39,7 +39,7 @@\n \n \n class _SessionFactory(LoggingMixin):\n-    def __init__(self, conn: Connection, region_name: str, config: Config):\n+    def __init__(self, conn: Connection, region_name: Optional[str], config: Config) -> None:\n         super().__init__()\n         self.conn = conn\n         self.region_name = region_name\n@@ -191,7 +191,7 @@ def _assume_role_with_saml(\n             RoleArn=role_arn, PrincipalArn=principal_arn, SAMLAssertion=saml_assertion, **assume_role_kwargs\n         )\n \n-    def _fetch_saml_assertion_using_http_spegno_auth(self, saml_config: Dict[str, Any]):\n+    def _fetch_saml_assertion_using_http_spegno_auth(self, saml_config: Dict[str, Any]) -> str:\n         import requests\n \n         # requests_gssapi will need paramiko > 2.6 since you'll need\n@@ -285,9 +285,9 @@ def __init__(\n         self.config = config\n \n         if not (self.client_type or self.resource_type):\n-            raise AirflowException('Either client_type or resource_type' ' must be provided.')\n+            raise AirflowException('Either client_type or resource_type must be provided.')\n \n-    def _get_credentials(self, region_name):\n+    def _get_credentials(self, region_name: Optional[str]) -> Tuple[boto3.session.Session, Optional[str]]:\n \n         if not self.aws_conn_id:\n             session = boto3.session.Session(region_name=region_name)\n@@ -327,7 +327,9 @@ def _get_credentials(self, region_name):\n         session = boto3.session.Session(region_name=region_name)\n         return session, None\n \n-    def get_client_type(self, client_type, region_name=None, config=None):\n+    def get_client_type(\n+        self, client_type: str, region_name: Optional[str] = None, config: Optional[Config] = None,\n+    ) -> boto3.client:\n         \"\"\"Get the underlying boto3 client using boto3 session\"\"\"\n         session, endpoint_url = self._get_credentials(region_name)\n \n@@ -338,7 +340,9 @@ def get_client_type(self, client_type, region_name=None, config=None):\n \n         return session.client(client_type, endpoint_url=endpoint_url, config=config, verify=self.verify)\n \n-    def get_resource_type(self, resource_type, region_name=None, config=None):\n+    def get_resource_type(\n+        self, resource_type: str, region_name: Optional[str] = None, config: Optional[Config] = None,\n+    ) -> boto3.resource:\n         \"\"\"Get the underlying boto3 resource using boto3 session\"\"\"\n         session, endpoint_url = self._get_credentials(region_name)\n \n@@ -350,7 +354,7 @@ def get_resource_type(self, resource_type, region_name=None, config=None):\n         return session.resource(resource_type, endpoint_url=endpoint_url, config=config, verify=self.verify)\n \n     @cached_property\n-    def conn(self):\n+    def conn(self) -> Union[boto3.client, boto3.resource]:\n         \"\"\"\n         Get the underlying boto3 client/resource (cached)\n \n@@ -365,7 +369,7 @@ def conn(self):\n             # Rare possibility - subclasses have not specified a client_type or resource_type\n             raise NotImplementedError('Could not get boto3 connection!')\n \n-    def get_conn(self):\n+    def get_conn(self) -> Union[boto3.client, boto3.resource]:\n         \"\"\"\n         Get the underlying boto3 client/resource (cached)\n \n@@ -378,12 +382,12 @@ def get_conn(self):\n         # Compat shim\n         return self.conn\n \n-    def get_session(self, region_name=None):\n+    def get_session(self, region_name: Optional[str] = None) -> boto3.session.Session:\n         \"\"\"Get the underlying boto3.session.\"\"\"\n         session, _ = self._get_credentials(region_name)\n         return session\n \n-    def get_credentials(self, region_name=None):\n+    def get_credentials(self, region_name: Optional[str] = None) -> Tuple[Optional[str], Optional[str]]:\n         \"\"\"\n         Get the underlying `botocore.Credentials` object.\n \n@@ -395,7 +399,7 @@ def get_credentials(self, region_name=None):\n         # See https://stackoverflow.com/a/36291428/8283373\n         return session.get_credentials().get_frozen_credentials()\n \n-    def expand_role(self, role):\n+    def expand_role(self, role: str) -> str:\n         \"\"\"\n         If the IAM role is a role name, get the Amazon Resource Name (ARN) for the role.\n         If IAM role is already an IAM role ARN, no change is made.\n@@ -409,7 +413,9 @@ def expand_role(self, role):\n             return self.get_client_type(\"iam\").get_role(RoleName=role)[\"Role\"][\"Arn\"]\n \n \n-def _parse_s3_config(config_file_name, config_format=\"boto\", profile=None):\n+def _parse_s3_config(\n+    config_file_name: str, config_format: Optional[str] = \"boto\", profile: Optional[str] = None\n+) -> Tuple[Optional[str], Optional[str]]:\n     \"\"\"\n     Parses a config file for s3 credentials. Can currently\n     parse boto, s3cmd.conf and AWS SDK config formats\ndiff --git a/airflow/providers/amazon/aws/hooks/ec2.py b/airflow/providers/amazon/aws/hooks/ec2.py\nindex 34517d752e699..27563be6c3284 100644\n--- a/airflow/providers/amazon/aws/hooks/ec2.py\n+++ b/airflow/providers/amazon/aws/hooks/ec2.py\n@@ -33,8 +33,9 @@ class EC2Hook(AwsBaseHook):\n         :class:`~airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`\n     \"\"\"\n \n-    def __init__(self, *args, **kwargs):\n-        super().__init__(resource_type=\"ec2\", *args, **kwargs)\n+    def __init__(self, *args, **kwargs) -> None:\n+        kwargs[\"resource_type\"] = \"ec2\"\n+        super().__init__(*args, **kwargs)\n \n     def get_instance(self, instance_id: str):\n         \"\"\"\ndiff --git a/airflow/providers/amazon/aws/hooks/emr.py b/airflow/providers/amazon/aws/hooks/emr.py\nindex 6bad910f472c9..8c2ed3f8830a7 100644\n--- a/airflow/providers/amazon/aws/hooks/emr.py\n+++ b/airflow/providers/amazon/aws/hooks/emr.py\n@@ -15,6 +15,7 @@\n # KIND, either express or implied.  See the License for the\n # specific language governing permissions and limitations\n # under the License.\n+from typing import Dict, List, Optional\n \n from airflow.exceptions import AirflowException\n from airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\n@@ -32,13 +33,15 @@ class EmrHook(AwsBaseHook):\n         :class:`~airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`\n     \"\"\"\n \n-    def __init__(self, emr_conn_id=None, *args, **kwargs):\n+    def __init__(self, emr_conn_id: Optional[str] = None, *args, **kwargs) -> None:\n         self.emr_conn_id = emr_conn_id\n-        super().__init__(client_type='emr', *args, **kwargs)\n+        kwargs[\"client_type\"] = \"emr\"\n+        super().__init__(*args, **kwargs)\n \n-    def get_cluster_id_by_name(self, emr_cluster_name, cluster_states):\n+    def get_cluster_id_by_name(self, emr_cluster_name: str, cluster_states: List[str]) -> Optional[str]:\n         \"\"\"\n-        Fetch id of EMR cluster with given name and (optional) states. Will return only if single id is found.\n+        Fetch id of EMR cluster with given name and (optional) states.\n+        Will return only if single id is found.\n \n         :param emr_cluster_name: Name of a cluster to find\n         :type emr_cluster_name: str\n@@ -63,7 +66,7 @@ def get_cluster_id_by_name(self, emr_cluster_name, cluster_states):\n             self.log.info('No cluster found for name %s', emr_cluster_name)\n             return None\n \n-    def create_job_flow(self, job_flow_overrides):\n+    def create_job_flow(self, job_flow_overrides: Dict):\n         \"\"\"\n         Creates a job flow using the config from the EMR connection.\n         Keys of the json extra hash may have the arguments of the boto3\ndiff --git a/airflow/providers/amazon/aws/hooks/kinesis.py b/airflow/providers/amazon/aws/hooks/kinesis.py\nindex 1c8480a8e2f79..304f8cd3245a5 100644\n--- a/airflow/providers/amazon/aws/hooks/kinesis.py\n+++ b/airflow/providers/amazon/aws/hooks/kinesis.py\n@@ -19,6 +19,8 @@\n \"\"\"\n This module contains AWS Firehose hook\n \"\"\"\n+from typing import Iterable\n+\n from airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\n \n \n@@ -36,11 +38,12 @@ class AwsFirehoseHook(AwsBaseHook):\n     :type delivery_stream: str\n     \"\"\"\n \n-    def __init__(self, delivery_stream, *args, **kwargs):\n+    def __init__(self, delivery_stream, *args, **kwargs) -> None:\n         self.delivery_stream = delivery_stream\n-        super().__init__(client_type='firehose', *args, **kwargs)\n+        kwargs[\"client_type\"] = \"firehose\"\n+        super().__init__(*args, **kwargs)\n \n-    def put_records(self, records):\n+    def put_records(self, records: Iterable):\n         \"\"\"\n         Write batch records to Kinesis Firehose\n         \"\"\"\ndiff --git a/airflow/providers/amazon/aws/hooks/lambda_function.py b/airflow/providers/amazon/aws/hooks/lambda_function.py\nindex a1d9b6142ffdf..008aafb78f596 100644\n--- a/airflow/providers/amazon/aws/hooks/lambda_function.py\n+++ b/airflow/providers/amazon/aws/hooks/lambda_function.py\n@@ -44,20 +44,21 @@ class AwsLambdaHook(AwsBaseHook):\n \n     def __init__(\n         self,\n-        function_name,\n-        log_type='None',\n-        qualifier='$LATEST',\n-        invocation_type='RequestResponse',\n+        function_name: str,\n+        log_type: str = 'None',\n+        qualifier: str = '$LATEST',\n+        invocation_type: str = 'RequestResponse',\n         *args,\n         **kwargs,\n-    ):\n+    ) -> None:\n         self.function_name = function_name\n         self.log_type = log_type\n         self.invocation_type = invocation_type\n         self.qualifier = qualifier\n-        super().__init__(client_type='lambda', *args, **kwargs)\n+        kwargs[\"client_type\"] = \"lambda\"\n+        super().__init__(*args, **kwargs)\n \n-    def invoke_lambda(self, payload):\n+    def invoke_lambda(self, payload: str) -> str:\n         \"\"\"\n         Invoke Lambda Function\n         \"\"\"\ndiff --git a/airflow/providers/amazon/aws/hooks/logs.py b/airflow/providers/amazon/aws/hooks/logs.py\nindex 1abb83d1a82dc..16b9f941a82a4 100644\n--- a/airflow/providers/amazon/aws/hooks/logs.py\n+++ b/airflow/providers/amazon/aws/hooks/logs.py\n@@ -20,6 +20,7 @@\n This module contains a hook (AwsLogsHook) with some very basic\n functionality for interacting with AWS CloudWatch.\n \"\"\"\n+from typing import Dict, Generator, Optional\n \n from airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\n \n@@ -35,10 +36,18 @@ class AwsLogsHook(AwsBaseHook):\n         :class:`~airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`\n     \"\"\"\n \n-    def __init__(self, *args, **kwargs):\n-        super().__init__(client_type='logs', *args, **kwargs)\n-\n-    def get_log_events(self, log_group, log_stream_name, start_time=0, skip=0, start_from_head=True):\n+    def __init__(self, *args, **kwargs) -> None:\n+        kwargs[\"client_type\"] = \"logs\"\n+        super().__init__(*args, **kwargs)\n+\n+    def get_log_events(\n+        self,\n+        log_group: str,\n+        log_stream_name: str,\n+        start_time: int = 0,\n+        skip: int = 0,\n+        start_from_head: bool = True,\n+    ) -> Generator:\n         \"\"\"\n         A generator for log items in a single stream. This will yield all the\n         items that are available at the current moment.\n@@ -67,7 +76,7 @@ def get_log_events(self, log_group, log_stream_name, start_time=0, skip=0, start\n         event_count = 1\n         while event_count > 0:\n             if next_token is not None:\n-                token_arg = {'nextToken': next_token}\n+                token_arg: Optional[Dict[str, str]] = {'nextToken': next_token}\n             else:\n                 token_arg = {}\n \ndiff --git a/airflow/providers/amazon/aws/hooks/redshift.py b/airflow/providers/amazon/aws/hooks/redshift.py\nindex e2d3144c472ca..c97708d5347ad 100644\n--- a/airflow/providers/amazon/aws/hooks/redshift.py\n+++ b/airflow/providers/amazon/aws/hooks/redshift.py\n@@ -35,8 +35,9 @@ class RedshiftHook(AwsBaseHook):\n         :class:`~airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`\n     \"\"\"\n \n-    def __init__(self, *args, **kwargs):\n-        super().__init__(client_type='redshift', *args, **kwargs)\n+    def __init__(self, *args, **kwargs) -> None:\n+        kwargs[\"client_type\"] = \"redshift\"\n+        super().__init__(*args, **kwargs)\n \n     # TODO: Wrap create_cluster_snapshot\n     def cluster_status(self, cluster_identifier: str) -> str:\ndiff --git a/airflow/providers/amazon/aws/hooks/sqs.py b/airflow/providers/amazon/aws/hooks/sqs.py\nindex 6c43f7f70b8ba..c40a432b0fbd0 100644\n--- a/airflow/providers/amazon/aws/hooks/sqs.py\n+++ b/airflow/providers/amazon/aws/hooks/sqs.py\n@@ -19,6 +19,8 @@\n \"\"\"\n This module contains AWS SQS hook\n \"\"\"\n+from typing import Dict, Optional\n+\n from airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\n \n \n@@ -33,10 +35,11 @@ class SQSHook(AwsBaseHook):\n         :class:`~airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`\n     \"\"\"\n \n-    def __init__(self, *args, **kwargs):\n-        super().__init__(client_type='sqs', *args, **kwargs)\n+    def __init__(self, *args, **kwargs) -> None:\n+        kwargs[\"client_type\"] = \"sqs\"\n+        super().__init__(*args, **kwargs)\n \n-    def create_queue(self, queue_name, attributes=None):\n+    def create_queue(self, queue_name: str, attributes: Optional[Dict] = None) -> Dict:\n         \"\"\"\n         Create queue using connection object\n \n@@ -52,7 +55,13 @@ def create_queue(self, queue_name, attributes=None):\n         \"\"\"\n         return self.get_conn().create_queue(QueueName=queue_name, Attributes=attributes or {})\n \n-    def send_message(self, queue_url, message_body, delay_seconds=0, message_attributes=None):\n+    def send_message(\n+        self,\n+        queue_url: str,\n+        message_body: str,\n+        delay_seconds: int = 0,\n+        message_attributes: Optional[Dict] = None,\n+    ) -> Dict:\n         \"\"\"\n         Send message to the queue\n \ndiff --git a/airflow/providers/amazon/aws/hooks/step_function.py b/airflow/providers/amazon/aws/hooks/step_function.py\nindex d83d1afa6257a..d120cbfae81cc 100644\n--- a/airflow/providers/amazon/aws/hooks/step_function.py\n+++ b/airflow/providers/amazon/aws/hooks/step_function.py\n@@ -32,8 +32,9 @@ class StepFunctionHook(AwsBaseHook):\n         :class:`~airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`\n     \"\"\"\n \n-    def __init__(self, region_name=None, *args, **kwargs):\n-        super().__init__(client_type='stepfunctions', *args, **kwargs)\n+    def __init__(self, region_name: Optional[str] = None, *args, **kwargs) -> None:\n+        kwargs[\"client_type\"] = \"stepfunctions\"\n+        super().__init__(*args, **kwargs)\n \n     def start_execution(\n         self,\n",
    "generated_commit": "Allows bootstrap for creating queries",
    "label_commit": "Add more type annotations to AWS hooks (#10671)\n\nCo-authored-by: Kamil Bregua <kamil.bregula@polidea.com>",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": -0.12556390464305878,
    "answer_correctness": 0.18751395220984712
  },
  {
    "diff": "diff --git a/airflow/contrib/hooks/qubole_hook.py b/airflow/contrib/hooks/qubole_hook.py\nindex 717027ccd8057..7e2fdb32dcbcc 100755\n--- a/airflow/contrib/hooks/qubole_hook.py\n+++ b/airflow/contrib/hooks/qubole_hook.py\n@@ -1,5 +1,6 @@\n import os\n import time\n+import datetime\n import logging\n \n from airflow.utils import AirflowException\ndiff --git a/airflow/contrib/operators/qubole_operator.py b/airflow/contrib/operators/qubole_operator.py\nindex d8c47b98ef1ce..063eb8c169bcb 100755\n--- a/airflow/contrib/operators/qubole_operator.py\n+++ b/airflow/contrib/operators/qubole_operator.py\n@@ -102,10 +102,10 @@ def __init__(self, qubole_conn_id=\"qubole_default\", *args, **kwargs):\n         self.args = args\n         self.kwargs = kwargs\n         self.kwargs['qubole_conn_id'] = qubole_conn_id\n+        self.hook = QuboleHook(*self.args, **self.kwargs)\n         super(QuboleOperator, self).__init__(*args, **kwargs)\n \n     def execute(self, context):\n-        self.hook = QuboleHook(*self.args, **self.kwargs)\n         return self.hook.execute(context)\n \n     def on_kill(self, ti):\n",
    "generated_commit": "[AIRFLOW-XXX] Use BaseHook for HIVEHook (#5033)",
    "label_commit": "Fixing small issue with qbol operator and hook",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": -0.036697059869766235,
    "answer_correctness": 0.19296389080760318
  },
  {
    "diff": "diff --git a/tests/test_pipelines.py b/tests/test_pipelines.py\nnew file mode 100644\nindex 0000000000..7747c7f7aa\n--- /dev/null\n+++ b/tests/test_pipelines.py\n@@ -0,0 +1,25 @@\n+# -*- coding: utf-8 -*-\n+# This Source Code Form is subject to the terms of the Mozilla Public\n+# License, v. 2.0. If a copy of the MPL was not distributed with this file,\n+# You can obtain one at http://mozilla.org/MPL/2.0/.\n+\n+import os\n+\n+import jsone\n+import pytest\n+import yaml\n+\n+\n+@pytest.mark.parametrize(\n+    \"pipeline_file\",\n+    (\n+        os.path.realpath(os.path.join(\"infra\", f))\n+        for f in os.listdir(\"infra\")\n+        if f.endswith(\".yml\")\n+    ),\n+)\n+def test_jsone_validates(pipeline_file):\n+    with open(pipeline_file, \"r\") as f:\n+        yaml_content = yaml.safe_load(f.read())\n+\n+    jsone.render(yaml_content, context={\"version\": \"42.0\"})\n",
    "generated_commit": "add tests",
    "label_commit": "Ensure pipeline yaml files are valid YAML and can be rendered with JSON-e\n\nPart of #613",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": 0.006340669933706522,
    "answer_correctness": 0.182468872623688
  },
  {
    "diff": "diff --git a/pontoon/teams/views.py b/pontoon/teams/views.py\nindex 2290f697e3..bb8146a0dc 100644\n--- a/pontoon/teams/views.py\n+++ b/pontoon/teams/views.py\n@@ -53,6 +53,7 @@ def ajax_projects(request, locale):\n \n     projects = (\n         Project.objects.available()\n+        .filter(can_be_requested=True)\n         .prefetch_latest_translation(locale)\n         .order_by('name')\n     )\n@@ -132,7 +133,10 @@ def request_projects(request, locale):\n     locale = get_object_or_404(Locale, code__iexact=locale)\n \n     # Validate projects\n-    project_list = Project.objects.available().filter(slug__in=slug_list)\n+    project_list = (\n+        Project.objects.available()\n+        .filter(slug__in=slug_list, can_be_requested=True)\n+    )\n     if not project_list:\n         return HttpResponseBadRequest('Bad Request: Non-existent projects specified')\n \n",
    "generated_commit": "fixing the filtering logic to filter for users on the same name",
    "label_commit": "Fix bug 1309287: Add the ability to disable Requests for projects",
    "bleu": 0.022416933501922302,
    "rouge_l": 0.34782608695652173,
    "meteor": 0.18018018018018017,
    "bert_score_f1": 0.12972062826156616,
    "answer_correctness": 0.18188716594701468
  },
  {
    "diff": "diff --git a/app/selenium_ui/bamboo/modules.py b/app/selenium_ui/bamboo/modules.py\nindex ef21ab4bf..a07a8d0be 100644\n--- a/app/selenium_ui/bamboo/modules.py\n+++ b/app/selenium_ui/bamboo/modules.py\n@@ -27,7 +27,6 @@ def sub_measure():\n             login_page.go_to()\n \n         sub_measure()\n-\n         login_page.set_credentials(username=datasets['username'], password=datasets['password'])\n         login_page.click_login_button()\n \n@@ -128,8 +127,10 @@ def measure():\n \n \n def log_out(webdriver, datasets):\n+    logout = Logout(webdriver)\n+\n     @print_timing(\"selenium_log_out\")\n     def measure():\n-        Logout(webdriver)\n-\n+        logout.go_to()\n+        logout.wait_for_page_loaded()\n     measure()\ndiff --git a/app/selenium_ui/bamboo/pages/pages.py b/app/selenium_ui/bamboo/pages/pages.py\nindex 9126e4155..d28d38503 100644\n--- a/app/selenium_ui/bamboo/pages/pages.py\n+++ b/app/selenium_ui/bamboo/pages/pages.py\n@@ -2,15 +2,15 @@\n \n from selenium_ui.bamboo.pages.selectors import UrlManager, LoginPageLocators, AllProjectsLocators, AllBuildsLocators, \\\n     PlanConfigurationLocators, BuildActivityLocators, PlanSummaryLocators, PlanHistoryLocators, BuildSummaryLocators, \\\n-    BuildLogsLocators, JobConfigLocators\n+    BuildLogsLocators, JobConfigLocators, LogoutLocators\n \n \n class Login(BasePage):\n     page_url = LoginPageLocators.login_page_url\n \n     def click_login_button(self):\n-        self.wait_until_visible(LoginPageLocators.login_button).click()\n-        self.wait_until_invisible(LoginPageLocators.login_button)\n+        self.wait_until_visible(LoginPageLocators.login_submit_button).click()\n+        self.wait_until_invisible(LoginPageLocators.login_submit_button)\n \n     def set_credentials(self, username, password):\n         self.get_element(LoginPageLocators.login_username_field).send_keys(username)\n@@ -102,4 +102,7 @@ def click_job_config_button(self):\n \n \n class Logout(BasePage):\n-    UrlManager().logout_url()\n+    page_url = LogoutLocators.logout_url\n+\n+    def wait_for_page_loaded(self):\n+        self.wait_until_visible(LogoutLocators.login_button_link)\ndiff --git a/app/selenium_ui/bamboo/pages/selectors.py b/app/selenium_ui/bamboo/pages/selectors.py\nindex dc46e9395..8902a724e 100644\n--- a/app/selenium_ui/bamboo/pages/selectors.py\n+++ b/app/selenium_ui/bamboo/pages/selectors.py\n@@ -6,7 +6,7 @@ class UrlManager:\n \n     def __init__(self, build_plan_id=None):\n         self.host = BAMBOO_SETTINGS.server_url\n-        self.login_params = '/userlogin!doDefault.action?os_destination=%2FallPlans.action'\n+        self.login_params = '/userlogin!doDefault.action'\n         self.logout_params = '/userLogout.action'\n         self.all_projects_params = '/allProjects.action'\n         self.plan_summary_params = f'/browse/{build_plan_id}'\n@@ -34,9 +34,9 @@ def logout_url(self):\n \n class LoginPageLocators:\n     login_page_url = UrlManager().login_url()\n-    login_button = (By.ID, \"loginForm_save\")\n     login_username_field = (By.ID, \"loginForm_os_username\")\n     login_password_field = (By.ID, \"loginForm_os_password\")\n+    login_submit_button = (By.ID, \"loginForm_save\")\n \n \n class AllProjectsLocators:\n@@ -89,4 +89,5 @@ class JobConfigLocators:\n \n \n class LogoutLocators:\n-    logout = (By.XPATH, \"//a[@href='/userLogout.action']\")\n+    logout_url = UrlManager().logout_url()\n+    login_button_link = (By.ID, \"login\")\n",
    "generated_commit": "fix: fix the docs in telemetry data to avoid issues with the same name",
    "label_commit": "Added check of the selenium logout action (#784)\n\n* Added check of the selenium logout action\n\n* Selenium login and logout actions modified after review\n\n* Changes after review\n\n* fix of sleector after comment",
    "bleu": 0.004194471627170932,
    "rouge_l": 0.08888888888888889,
    "meteor": 0.04559270516717325,
    "bert_score_f1": 0.034862738102674484,
    "answer_correctness": 0.17771445295903543
  },
  {
    "diff": "diff --git a/celery/log.py b/celery/log.py\nindex daf7095a0dd..5128abb94b9 100644\n--- a/celery/log.py\n+++ b/celery/log.py\n@@ -14,6 +14,7 @@\n from celery import current_app\n from celery.utils import LOG_LEVELS, isatty\n from celery.utils.compat import LoggerAdapter\n+from celery.utils.encoding import safe_str\n from celery.utils.patch import ensure_process_aware_logger\n from celery.utils.term import colored\n \n@@ -40,10 +41,10 @@ def format(self, record):\n \n         if self.use_color and color:\n             try:\n-                record.msg = color(unicode(record.msg, \"utf-8\", \"replace\"))\n+                record.msg = color(safe_str(record.msg))\n             except Exception, exc:\n                 record.msg = \"<Unrepresentable %r: %r>\" % (type(record.msg),\n-                                                           exc)\n+                                                           traceback.format_stack())\n \n         # Very ugly, but have to make sure processName is supported\n         # by foreign logger instances.\ndiff --git a/celery/utils/encoding.py b/celery/utils/encoding.py\nindex 5a038c3c00b..86a6406c025 100644\n--- a/celery/utils/encoding.py\n+++ b/celery/utils/encoding.py\n@@ -1,4 +1,5 @@\n import sys\n+import traceback\n \n \n def default_encoding():\n@@ -13,8 +14,8 @@ def safe_str(s, errors=\"replace\"):\n         if isinstance(s, unicode):\n             return s.encode(encoding, errors)\n         return unicode(s, encoding, errors)\n-    except Exception, exc:\n-        return \"<Unrepresentable %r: %r>\" % (type(s), exc)\n+    except Exception:\n+        return \"<Unrepresentable %r: %r>\" % (type(s), traceback.format_stack())\n \n \n def safe_repr(o, errors=\"replace\"):\n",
    "generated_commit": "Fixes error message when logging level is not set",
    "label_commit": "celeryd: Errors could not be logged, because of unicode errors",
    "bleu": 0.02145851237135921,
    "rouge_l": 0.2105263157894737,
    "meteor": 0.15151515151515152,
    "bert_score_f1": 0.16912361979484558,
    "answer_correctness": 0.1960204095110215
  },
  {
    "diff": "diff --git a/airflow/ti_deps/deps/trigger_rule_dep.py b/airflow/ti_deps/deps/trigger_rule_dep.py\nindex 6203b2a79b6d1..ca2a6100a2784 100644\n--- a/airflow/ti_deps/deps/trigger_rule_dep.py\n+++ b/airflow/ti_deps/deps/trigger_rule_dep.py\n@@ -27,7 +27,6 @@\n from airflow.models.taskinstance import PAST_DEPENDS_MET\n from airflow.ti_deps.deps.base_ti_dep import BaseTIDep\n from airflow.utils.state import TaskInstanceState\n-from airflow.utils.task_group import MappedTaskGroup\n from airflow.utils.trigger_rule import TriggerRule as TR\n \n if TYPE_CHECKING:\n@@ -133,20 +132,6 @@ def _get_expanded_ti_count() -> int:\n             \"\"\"\n             return ti.task.get_mapped_ti_count(ti.run_id, session=session)\n \n-        def _iter_expansion_dependencies() -> Iterator[str]:\n-            from airflow.models.mappedoperator import MappedOperator\n-\n-            if isinstance(ti.task, MappedOperator):\n-                for op in ti.task.iter_mapped_dependencies():\n-                    yield op.task_id\n-            task_group = ti.task.task_group\n-            if task_group and task_group.iter_mapped_task_groups():\n-                yield from (\n-                    op.task_id\n-                    for tg in task_group.iter_mapped_task_groups()\n-                    for op in tg.iter_mapped_dependencies()\n-                )\n-\n         @functools.lru_cache\n         def _get_relevant_upstream_map_indexes(upstream_id: str) -> int | range | None:\n             \"\"\"Get the given task's map indexes relevant to the current ti.\n@@ -157,9 +142,6 @@ def _get_relevant_upstream_map_indexes(upstream_id: str) -> int | range | None:\n             \"\"\"\n             if TYPE_CHECKING:\n                 assert isinstance(ti.task.dag, DAG)\n-            if isinstance(ti.task.task_group, MappedTaskGroup):\n-                if upstream_id not in set(_iter_expansion_dependencies()):\n-                    return None\n             try:\n                 expanded_ti_count = _get_expanded_ti_count()\n             except (NotFullyPopulated, NotMapped):\ndiff --git a/tests/models/test_mappedoperator.py b/tests/models/test_mappedoperator.py\nindex 5c2e23c1f9e30..7244c55774840 100644\n--- a/tests/models/test_mappedoperator.py\n+++ b/tests/models/test_mappedoperator.py\n@@ -1305,8 +1305,8 @@ def file_transforms(filename):\n         states = self.get_states(dr)\n         expected = {\n             \"file_transforms.my_setup\": {0: \"success\", 1: \"failed\", 2: \"skipped\"},\n-            \"file_transforms.my_work\": {2: \"upstream_failed\", 1: \"upstream_failed\", 0: \"upstream_failed\"},\n-            \"file_transforms.my_teardown\": {2: \"success\", 1: \"success\", 0: \"success\"},\n+            \"file_transforms.my_work\": {0: \"success\", 1: \"upstream_failed\", 2: \"skipped\"},\n+            \"file_transforms.my_teardown\": {0: \"success\", 1: \"upstream_failed\", 2: \"skipped\"},\n         }\n \n         assert states == expected\ndiff --git a/tests/ti_deps/deps/test_trigger_rule_dep.py b/tests/ti_deps/deps/test_trigger_rule_dep.py\nindex 1bc8808cb8b2b..00cbcd449af3e 100644\n--- a/tests/ti_deps/deps/test_trigger_rule_dep.py\n+++ b/tests/ti_deps/deps/test_trigger_rule_dep.py\n@@ -1165,23 +1165,19 @@ def _one_scheduling_decision_iteration() -> dict[tuple[str, int], TaskInstance]:\n     tis = _one_scheduling_decision_iteration()\n     assert sorted(tis) == [(\"tg.t1\", 0), (\"tg.t1\", 1), (\"tg.t1\", 2)]\n \n-    # After running the first t1, the remaining t1 must be run before t2 is available.\n+    # After running the first t1, the first t2 becomes immediately available.\n     tis[\"tg.t1\", 0].run()\n     tis = _one_scheduling_decision_iteration()\n-    assert sorted(tis) == [(\"tg.t1\", 1), (\"tg.t1\", 2)]\n+    assert sorted(tis) == [(\"tg.t1\", 1), (\"tg.t1\", 2), (\"tg.t2\", 0)]\n \n-    # After running all t1, t2 is available.\n-    tis[\"tg.t1\", 1].run()\n+    # Similarly for the subsequent t2 instances.\n     tis[\"tg.t1\", 2].run()\n     tis = _one_scheduling_decision_iteration()\n-    assert sorted(tis) == [(\"tg.t2\", 0), (\"tg.t2\", 1), (\"tg.t2\", 2)]\n-\n-    # Similarly for t2 instances. They both have to complete before t3 is available\n-    tis[\"tg.t2\", 0].run()\n-    tis = _one_scheduling_decision_iteration()\n-    assert sorted(tis) == [(\"tg.t2\", 1), (\"tg.t2\", 2)]\n+    assert sorted(tis) == [(\"tg.t1\", 1), (\"tg.t2\", 0), (\"tg.t2\", 2)]\n \n     # But running t2 partially does not make t3 available.\n+    tis[\"tg.t1\", 1].run()\n+    tis[\"tg.t2\", 0].run()\n     tis[\"tg.t2\", 2].run()\n     tis = _one_scheduling_decision_iteration()\n     assert sorted(tis) == [(\"tg.t2\", 1)]\n@@ -1411,34 +1407,3 @@ def w2():\n             (status,) = self.get_dep_statuses(dr, \"w2\", flag_upstream_failed=True, session=session)\n         assert status.reason.startswith(\"All setup tasks must complete successfully\")\n         assert self.get_ti(dr, \"w2\").state == expected\n-\n-\n-def test_mapped_tasks_in_mapped_task_group_waits_for_upstreams_to_complete(dag_maker, session):\n-    \"\"\"Test that one failed trigger rule works well in mapped task group\"\"\"\n-    with dag_maker() as dag:\n-\n-        @dag.task\n-        def t1():\n-            return [1, 2, 3]\n-\n-        @task_group(\"tg1\")\n-        def tg1(a):\n-            @dag.task()\n-            def t2(a):\n-                return a\n-\n-            @dag.task(trigger_rule=TriggerRule.ONE_FAILED)\n-            def t3(a):\n-                return a\n-\n-            t2(a) >> t3(a)\n-\n-        t = t1()\n-        tg1.expand(a=t)\n-\n-    dr = dag_maker.create_dagrun()\n-    ti = dr.get_task_instance(task_id=\"t1\")\n-    ti.run()\n-    dr.task_instance_scheduling_decisions()\n-    ti3 = dr.get_task_instance(task_id=\"tg1.t3\")\n-    assert not ti3.state\n",
    "generated_commit": "Remove some tests (#6032)",
    "label_commit": "Revert \"Fix pre-mature evaluation of tasks in mapped task group (#34337)\" (#35651)\n\nThis reverts commit 69938fd163045d750b8c218500d79bc89858f9c1.",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": -0.1354803442955017,
    "answer_correctness": 0.20190521878272014
  },
  {
    "diff": "diff --git a/apps/amo/tests/test_amo_utils.py b/apps/amo/tests/test_amo_utils.py\nindex ce0a72c3e63..d8e8befdc57 100644\n--- a/apps/amo/tests/test_amo_utils.py\n+++ b/apps/amo/tests/test_amo_utils.py\n@@ -272,6 +272,23 @@ def test_nested(self):\n         eq_(res['list'], [expected])\n         eq_(res['bool'], True)\n \n+    def test_without_linkify(self):\n+        value = '<button>http://firefox.com</button>'\n+        expected = '&lt;button&gt;http://firefox.com&lt;/button&gt;'\n+\n+        test = {\n+            'string': value,\n+            'dict': {'x': value},\n+            'list': [value],\n+            'bool': True,\n+        }\n+        res = escape_all(test, linkify=False)\n+\n+        eq_(res['string'], expected)\n+        eq_(res['dict'], {'x': expected})\n+        eq_(res['list'], [expected])\n+        eq_(res['bool'], True)\n+\n \n class TestAttachTransDict(amo.tests.TestCase):\n     \"\"\"\ndiff --git a/apps/amo/utils.py b/apps/amo/utils.py\nindex 5e896b2d33a..7eabaedf8df 100644\n--- a/apps/amo/utils.py\n+++ b/apps/amo/utils.py\n@@ -714,18 +714,19 @@ def no_translation(lang=None):\n     translation.trans_real.activate(old_lang)\n \n \n-def escape_all(v):\n+def escape_all(v, linkify=True):\n     \"\"\"Escape html in JSON value, including nested items.\"\"\"\n     if isinstance(v, basestring):\n         v = jinja2.escape(smart_unicode(v))\n-        v = linkify_with_outgoing(v)\n+        if linkify:\n+            v = linkify_with_outgoing(v)\n         return v\n     elif isinstance(v, list):\n         for i, lv in enumerate(v):\n-            v[i] = escape_all(lv)\n+            v[i] = escape_all(lv, linkify=linkify)\n     elif isinstance(v, dict):\n         for k, lv in v.iteritems():\n-            v[k] = escape_all(lv)\n+            v[k] = escape_all(lv, linkify=linkify)\n     elif isinstance(v, Translation):\n         v = jinja2.escape(smart_unicode(v.localized_string))\n     return v\ndiff --git a/mkt/developers/tests/test_views.py b/mkt/developers/tests/test_views.py\nindex f60d26d48bb..d46ad0376d6 100644\n--- a/mkt/developers/tests/test_views.py\n+++ b/mkt/developers/tests/test_views.py\n@@ -928,6 +928,62 @@ def test_detail_view(self):\n                     args=['hosted', upload.uuid]))\n         eq_(suite('#suite-results-tier-2').length, 1)\n \n+    def test_detail_view_linkification(self):\n+        uid = '9b1b3898db8a4d99a049829a46969ab4'\n+        upload = FileUpload.objects.create(\n+            name='something.zip',\n+            validation=json.dumps({\n+                u'ending_tier': 1,\n+                u'success': False,\n+                u'warnings': 0,\n+                u'errors': 1,\n+                u'notices': 0,\n+                u'feature_profile': [],\n+                u'messages': [\n+                    {\n+                        u'column': None,\n+                        u'context': [\n+                            u'',\n+                            u'<button on-click=\"{{ port.name }}\">uh</button>',\n+                            u''\n+                        ],\n+                        u'description': [\n+                            u'http://www.firefox.com'\n+                            u'<script>alert(\"hi\");</script>',\n+                        ],\n+                        u'file': u'index.html',\n+                        u'id': [u'csp', u'script_attribute'],\n+                        u'line': 1638,\n+                        u'message': u'CSP Violation Detected',\n+                        u'tier': 2,\n+                        u'type': u'error',\n+                        u'uid': uid,\n+                    },\n+                ],\n+                u'metadata': {'ran_js_tests': 'yes'},\n+                u'manifest': {},\n+                u'feature_usage': [],\n+                u'permissions': [],\n+\n+            }),\n+        )\n+        r = self.client.get(reverse('mkt.developers.standalone_upload_detail',\n+                                    args=['packaged', upload.uuid]))\n+        eq_(r.status_code, 200)\n+        data = json.loads(r.content)\n+        message = data['validation']['messages'][0]\n+        description = message['description'][0]\n+        eq_(description,\n+            '<a rel=\"nofollow\" href=\"http://outgoing.mozilla.org/v1/'\n+            '680c0c59e4d87f3d21faf1d7365f0fec615076041466406aa3608fe6503aef43'\n+            '/http%3A//www.firefox.com\">'\n+            'http://www.firefox.com</a>'\n+            '&lt;script&gt;alert(\"hi\");&lt;/script&gt;')\n+        context = message['context'][1]\n+        eq_(context,\n+            '&lt;button on-click=&#34;{{ port.name }}&#34;&gt;'\n+            'uh&lt;/button&gt;')\n+\n \n def assert_json_error(request, field, msg):\n     eq_(request.status_code, 400)\ndiff --git a/mkt/developers/views.py b/mkt/developers/views.py\nindex 25bf9bea318..761e8f3078f 100644\n--- a/mkt/developers/views.py\n+++ b/mkt/developers/views.py\n@@ -680,7 +680,7 @@ def make_validation_result(data):\n     if data['validation']:\n         for msg in data['validation']['messages']:\n             for k, v in msg.items():\n-                msg[k] = escape_all(v)\n+                msg[k] = escape_all(v, linkify=k in ('message', 'description'))\n     return data\n \n \n",
    "generated_commit": "Add test to illustrate code header (#2254)",
    "label_commit": "Don't linkify the validation context (bug 1030849)",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": 0.1100827008485794,
    "answer_correctness": 0.1918067971199866
  },
  {
    "diff": "diff --git a/emails/tests/views_tests.py b/emails/tests/views_tests.py\nindex 8cf1721692..585dbd5fe0 100644\n--- a/emails/tests/views_tests.py\n+++ b/emails/tests/views_tests.py\n@@ -227,9 +227,9 @@ def setUp(self) -> None:\n         self.bucket = 'test-bucket'\n         self.key = '/emails/objectkey123'\n \n-        patcher = patch('emails.views._get_address', side_effect=ObjectDoesNotExist())\n-        patcher.start()\n-        self.addCleanup(patcher.stop)\n+        self.patcher = patch('emails.views._get_address', side_effect=ObjectDoesNotExist())\n+        self.patcher.start()\n+        self.addCleanup(self.patcher.stop)\n \n     @patch('emails.views.remove_message_from_s3')\n     @patch('emails.views._handle_reply')\n@@ -298,6 +298,155 @@ def test_address_does_not_exist_email_in_s3_deleted(\n         assert response.status_code == 404\n         assert response.content == b'Address does not exist'\n \n+    @patch('emails.views.remove_message_from_s3')\n+    def test_bounce_notification_not_in_s3_deleted_ignored(\n+        self, mocked_message_removed\n+    ):\n+        response = _sns_notification(BOUNCE_SNS_BODIES['soft'])\n+        mocked_message_removed.assert_called_once_with(None, None)\n+        assert response.status_code == 404\n+        assert response.content == b'Address does not exist'\n+\n+    @patch('emails.views.remove_message_from_s3')\n+    def test_email_without_commonheaders_in_s3_deleted(\n+        self, mocked_message_removed\n+    ):\n+        message_wo_commonheaders = EMAIL_SNS_BODIES['s3_stored']['Message'].replace(\n+            'commonHeaders', 'invalidHeaders'\n+        )\n+        notification_wo_commonheaders = EMAIL_SNS_BODIES['s3_stored'].copy()\n+        notification_wo_commonheaders['Message'] = message_wo_commonheaders\n+        response = _sns_notification(notification_wo_commonheaders)\n+        mocked_message_removed.assert_called_once_with(self.bucket, self.key)\n+        assert response.status_code == 400\n+        assert response.content == b'Received SNS notification without commonHeaders.'\n+\n+    @patch('emails.views.remove_message_from_s3')\n+    def test_email_to_non_relay_domain_in_s3_deleted(\n+        self, mocked_message_removed\n+    ):\n+        message_w_non_relay_as_recipient = EMAIL_SNS_BODIES['s3_stored']['Message'].replace(\n+            'sender@test.com', 'to@not-relay.com'\n+        )\n+        notification_w_non_relay_domain = EMAIL_SNS_BODIES['s3_stored'].copy()\n+        notification_w_non_relay_domain['Message'] = message_w_non_relay_as_recipient\n+        response = _sns_notification(notification_w_non_relay_domain)\n+        mocked_message_removed.assert_called_once_with(self.bucket, self.key)\n+        assert response.status_code == 404\n+        assert response.content == b'Address does not exist'\n+\n+    @patch('emails.views.remove_message_from_s3')\n+    def test_malformed_to_field_email_in_s3_deleted(\n+        self, mocked_message_removed\n+    ):\n+        message_w_malformed_to_field = EMAIL_SNS_BODIES['s3_stored']['Message'].replace(\n+            'sender@test.com', 'not-relay-test.com'\n+        )\n+        notification_w_malformed_to_field = EMAIL_SNS_BODIES['s3_stored'].copy()\n+        notification_w_malformed_to_field['Message'] = message_w_malformed_to_field\n+        response = _sns_notification(notification_w_malformed_to_field)\n+        mocked_message_removed.assert_called_once_with(self.bucket, self.key)\n+        assert response.status_code == 400\n+        assert response.content == b'Malformed to field.'\n+\n+    @patch('emails.views.remove_message_from_s3')\n+    def test_noreply_email_in_s3_deleted(\n+        self, mocked_message_removed\n+    ):\n+        message_w_email_to_noreply = EMAIL_SNS_BODIES['s3_stored']['Message'].replace(\n+            'sender@test.com', 'noreply@default.com'\n+        )\n+        notification_w_email_to_noreply = EMAIL_SNS_BODIES['s3_stored'].copy()\n+        notification_w_email_to_noreply['Message'] = message_w_email_to_noreply\n+        response = _sns_notification(notification_w_email_to_noreply)\n+        mocked_message_removed.assert_called_once_with(self.bucket, self.key)\n+        assert response.status_code == 200\n+        assert response.content == b'noreply address is not supported.'\n+\n+\n+class SNSNotificationValidUserEmailsInS3Test(TestCase):\n+    def setUp(self) -> None:\n+        self.bucket = 'test-bucket'\n+        self.key = '/emails/objectkey123'\n+        self.user = baker.make(\n+            User, email='sender@test.com', make_m2m=True\n+        )\n+        self.profile = self.user.profile_set.first()\n+        self.address = baker.make(\n+            RelayAddress, user=self.user, address='sender', domain=2\n+        )\n+\n+    @patch('emails.views.remove_message_from_s3')\n+    def test_auto_block_spam_true_email_in_s3_deleted(\n+        self, mocked_message_removed\n+    ):\n+        self.profile.auto_block_spam = True\n+        self.profile.save()\n+        message_spamverdict_failed = EMAIL_SNS_BODIES['s3_stored']['Message'].replace(\n+            '\"spamVerdict\":{\"status\":\"PASS\"}', '\"spamVerdict\":{\"status\":\"FAIL\"}'\n+        )\n+        notification_w_spamverdict_failed = EMAIL_SNS_BODIES['s3_stored'].copy()\n+        notification_w_spamverdict_failed['Message'] = message_spamverdict_failed\n+\n+        response = _sns_notification(notification_w_spamverdict_failed)\n+        mocked_message_removed.assert_called_once_with(self.bucket, self.key)\n+        assert response.status_code == 200\n+        assert response.content == b'Address rejects spam.'\n+\n+    @patch('emails.views.remove_message_from_s3')\n+    def test_user_bounce_paused_email_in_s3_deleted(\n+        self, mocked_message_removed\n+    ):\n+        self.profile.last_soft_bounce = datetime.now(timezone.utc)\n+        self.profile.save()\n+\n+        response = _sns_notification(EMAIL_SNS_BODIES['s3_stored'])\n+        mocked_message_removed.assert_called_once_with(self.bucket, self.key)\n+        assert response.status_code == 200\n+        assert response.content == b'Address is temporarily disabled.'\n+\n+    @patch('emails.views._reply_allowed')\n+    @patch('emails.views._get_reply_record_from_lookup_key')\n+    @patch('emails.views.remove_message_from_s3')\n+    def test_reply_not_allowed_email_in_s3_deleted(\n+        self, mocked_message_removed, mocked_reply_record, mocked_reply_allowed,\n+    ):\n+        # external user sending a reply to Relay user\n+        # where the replies were being exchanged but now the user\n+        # no longer has the premium subscription\n+        mocked_reply_allowed.return_value = False\n+\n+        response = _sns_notification(EMAIL_SNS_BODIES['s3_stored'])\n+        mocked_message_removed.assert_called_once_with(self.bucket, self.key)\n+        assert response.status_code == 403\n+        assert response.content == b'Relay replies require a premium account'\n+\n+    @patch('emails.views.remove_message_from_s3')\n+    def test_relay_address_disabled_email_in_s3_deleted(\n+        self, mocked_message_removed\n+    ):\n+        self.address.enabled = False\n+        self.address.save()\n+\n+        response = _sns_notification(EMAIL_SNS_BODIES['s3_stored'])\n+        mocked_message_removed.assert_called_once_with(self.bucket, self.key)\n+        assert response.status_code == 200\n+        assert response.content == b'Address is temporarily disabled.'\n+\n+    @patch('emails.views._check_email_from_list')\n+    @patch('emails.views.remove_message_from_s3')\n+    def test_blocked_list_email_in_s3_deleted(\n+        self, mocked_message_removed, mocked_email_is_from_list\n+    ):\n+        self.address.block_list_emails = True\n+        self.address.save()\n+        mocked_email_is_from_list.return_value = True\n+\n+        response = _sns_notification(EMAIL_SNS_BODIES['s3_stored'])\n+        mocked_message_removed.assert_called_once_with(self.bucket, self.key)\n+        assert response.status_code == 200\n+        assert response.content == b'Address is not accepting list emails.'\n+\n \n class SnsMessageTest(TestCase):\n     def setUp(self) -> None:\ndiff --git a/emails/utils.py b/emails/utils.py\nindex 4386b1a446..be6f22eac9 100644\n--- a/emails/utils.py\n+++ b/emails/utils.py\n@@ -284,12 +284,16 @@ def _get_bucket_and_key_from_s3_json(message_json):\n     if 'receipt' in message_json and 'action' in message_json['receipt']:\n         message_json_receipt = message_json['receipt']\n     else:\n-        # TODO: sns inbound notification does not have 'receipt'\n-        # we need to look into this more\n-        logger.error(\n-            'sns_inbound_message_without_receipt',\n-            extra={'message_json_keys': message_json.keys()}\n-        )\n+        notification_type = message_json.get('notificationType')\n+        event_type = message_json.get('eventType')\n+        is_bounce_notification = notification_type == 'Bounce' or event_type == 'Bounce'\n+        if not is_bounce_notification:\n+            # TODO: sns inbound notification does not have 'receipt'\n+            # we need to look into this more\n+            logger.error(\n+                'sns_inbound_message_without_receipt',\n+                extra={'message_json_keys': message_json.keys()}\n+            )\n         return None, None\n \n     try:\ndiff --git a/emails/views.py b/emails/views.py\nindex 5caa004de7..dbf0fe07bf 100644\n--- a/emails/views.py\n+++ b/emails/views.py\n@@ -346,7 +346,7 @@ def _sns_message(message_json):\n         [to_local_portion, to_domain_portion] = to_address.split('@')\n     except ValueError:\n         # TODO: Add metric\n-        return HttpResponse('Malformed to field.', 400)\n+        return HttpResponse('Malformed to field.', status=400)\n \n     if to_local_portion == 'noreply':\n         incr_if_enabled('email_for_noreply_address', 1)\n",
    "generated_commit": "A few more fixes for the fact that it's now defined",
    "label_commit": "Test sns_message response and how it affects email removal from S3\nBounce notification type is being checked on _get_bucket_and_key_from_s3_json since by design the notification message will not have receipt",
    "bleu": 0.0036699065615375323,
    "rouge_l": 0.041666666666666664,
    "meteor": 0.01838235294117647,
    "bert_score_f1": -0.046130016446113586,
    "answer_correctness": 0.17152049259305197
  },
  {
    "diff": "diff --git a/scripts/regressor_finder.py b/scripts/regressor_finder.py\nindex 4ca01d5b33..8997507c00 100644\n--- a/scripts/regressor_finder.py\n+++ b/scripts/regressor_finder.py\n@@ -41,21 +41,21 @@\n db.register(\n     BUG_FIXING_COMMITS_DB,\n     \"https://index.taskcluster.net/v1/task/project.relman.bugbug_annotate.regressor_finder.latest/artifacts/public/bug_fixing_commits.json.zst\",\n-    2,\n+    3,\n )\n \n BUG_INTRODUCING_COMMITS_DB = \"data/bug_introducing_commits.json\"\n db.register(\n     BUG_INTRODUCING_COMMITS_DB,\n     \"https://index.taskcluster.net/v1/task/project.relman.bugbug_annotate.regressor_finder.latest/artifacts/public/bug_introducing_commits.json.zst\",\n-    1,\n+    3,\n )\n \n TOKENIZED_BUG_INTRODUCING_COMMITS_DB = \"data/tokenized_bug_introducing_commits.json\"\n db.register(\n     TOKENIZED_BUG_INTRODUCING_COMMITS_DB,\n     \"https://index.taskcluster.net/v1/task/project.relman.bugbug_annotate.regressor_finder.latest/artifacts/public/tokenized_bug_introducing_commits.json.zst\",\n-    1,\n+    3,\n )\n \n \n",
    "generated_commit": "Update eval script to use latest version of the latest version (#861)",
    "label_commit": "Increase version number of the DBs, as we changed the format",
    "bleu": 0.03986357128268015,
    "rouge_l": 0.2608695652173913,
    "meteor": 0.13513513513513514,
    "bert_score_f1": 0.11818458884954453,
    "answer_correctness": 0.19573430383785084
  },
  {
    "diff": "diff --git a/pontoon/base/models.py b/pontoon/base/models.py\nindex a73cc2773f..095f773b35 100644\n--- a/pontoon/base/models.py\n+++ b/pontoon/base/models.py\n@@ -308,7 +308,9 @@ def get_projects_with_stats(projects, locale=None):\n     for project in projects:\n         r = Entity.objects.filter(obsolete=False).values('resource')\n         resources = Resource.objects.filter(project=project, pk__in=r)\n-        stats = Stats.objects.filter(resource__in=resources)\n+        locales = project.locales.all()\n+        stats = Stats.objects.filter(\n+            resource__in=resources, locale__in=locales)\n \n         if locale:\n             stats = stats.filter(locale=locale)\n",
    "generated_commit": "Fixing issue with filtering by users of SQLAlchemy (#241)",
    "label_commit": "Bugfix: display aggregated stats only for project locales, ommit removed ones",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": 0.0194602832198143,
    "answer_correctness": 0.439056551316537
  },
  {
    "diff": "diff --git a/app/util/api/abstract_clients.py b/app/util/api/abstract_clients.py\nindex 6fa0bedd2..a218b9f76 100644\n--- a/app/util/api/abstract_clients.py\n+++ b/app/util/api/abstract_clients.py\n@@ -71,10 +71,13 @@ def get(self, url: str,\n             allow_redirect: bool = False,\n             headers: dict = None,\n             auth: tuple = None):\n+\n         response = self.session.get(url, verify=self.verify, timeout=self.requests_timeout,\n                                     allow_redirects=allow_redirect, headers=headers if headers else self.headers,\n                                     auth=auth if auth else self.base_auth)\n+\n         self.__verify_response(response, error_msg, expected_status_codes)\n+\n         return response\n \n     def delete(self, url: str, error_msg: str, expected_status_codes: list = None, allow_redirect=False):\ndiff --git a/app/util/api/jira_clients.py b/app/util/api/jira_clients.py\nindex 004fc0351..8dd690d68 100644\n--- a/app/util/api/jira_clients.py\n+++ b/app/util/api/jira_clients.py\n@@ -6,6 +6,8 @@\n BATCH_SIZE_USERS = 1000\n BATCH_SIZE_ISSUES = 1000\n \n+REINDEX_ERROR_MESSAGE = 'A task could not be found for the given task id'\n+\n \n class JiraRestClient(RestClient):\n \n@@ -245,3 +247,13 @@ def get_service_desk_info(self):\n         api_url = f'{self.host}/rest/plugins/applications/1.0/installed/jira-servicedesk'\n         service_desk_info = self.get(api_url, \"Could not retrieve JSM info\", headers=JSM_EXPERIMENTAL_HEADERS)\n         return service_desk_info.json()\n+\n+    def get_reindex_info(self):\n+        api_url = f'{self.host}/rest/api/2/reindex'\n+        reindex_info = self.get(api_url, 'Could not retrieve reindex info.', expected_status_codes=[404])\n+        if REINDEX_ERROR_MESSAGE in reindex_info.text:\n+            raise SystemExit('Jira is unable to perform a background re-index at this time because the index files '\n+                             'are either missing or corrupted.\\nPlease navigate to System > Indexing. '\n+                             'Select the Full re-index option. Click Re-Index and wait until re-indexing is completed.')\n+        elif not reindex_info.ok:\n+            raise SystemExit('Could not retrieve reindex info.')\ndiff --git a/app/util/data_preparation/jira_prepare_data.py b/app/util/data_preparation/jira_prepare_data.py\nindex 8f9acb17a..8f47e097b 100644\n--- a/app/util/data_preparation/jira_prepare_data.py\n+++ b/app/util/data_preparation/jira_prepare_data.py\n@@ -181,6 +181,10 @@ def __check_for_admin_permissions(jira_api):\n         raise SystemExit(f\"The '{jira_api.user}' user does not have admin permissions.\")\n \n \n+def __check_reindex_info(jira_api):\n+    jira_api.get_reindex_info()\n+\n+\n def main():\n     print(\"Started preparing data\")\n \n@@ -191,6 +195,7 @@ def main():\n \n     __check_for_admin_permissions(client)\n     __check_current_language(client)\n+    __check_reindex_info(client)\n     dataset = __create_data_set(client)\n     write_test_data_to_files(dataset)\n \ndiff --git a/app/util/data_preparation/jsm_prepare_data.py b/app/util/data_preparation/jsm_prepare_data.py\nindex 6c8713b01..3e8b0087e 100644\n--- a/app/util/data_preparation/jsm_prepare_data.py\n+++ b/app/util/data_preparation/jsm_prepare_data.py\n@@ -474,6 +474,10 @@ def __write_test_data_to_files(datasets):\n     __write_to_file(JSM_DATASET_CUSTOM_ISSUES, issues)\n \n \n+def __check_reindex_info(jira_api):\n+    jira_api.get_reindex_info()\n+\n+\n @print_timing('JSM full prepare data', sep='=')\n def main():\n     print(\"Started preparing data\")\n@@ -482,6 +486,7 @@ def main():\n     jsm_client = JsmRestClient(url, JSM_SETTINGS.admin_login, JSM_SETTINGS.admin_password, verify=JSM_SETTINGS.secure,\n                                headers=JSM_EXPERIMENTAL_HEADERS)\n     jira_client = JiraRestClient(url, JSM_SETTINGS.admin_login, JSM_SETTINGS.admin_password, JSM_SETTINGS.secure)\n+    __check_reindex_info(jira_client)\n     dataset = __create_data_set(jira_client=jira_client, jsm_client=jsm_client)\n     __write_test_data_to_files(dataset)\n \n",
    "generated_commit": "Add function to check for getting zoo file (#2866)",
    "label_commit": "[JIRA/JSM] DCA-1116:Add reindex info check (#609)\n\n* DCA-1116:Add reindex info check\n\n* DCA-1116:Refacotr code - add expected_status_codes for get reindex info request\n\n* DCA-1116:Move error message into constant",
    "bleu": 0.003453569068596916,
    "rouge_l": 0.1818181818181818,
    "meteor": 0.12092911877394635,
    "bert_score_f1": 0.021228564903140068,
    "answer_correctness": 0.17611378729361188
  },
  {
    "diff": "diff --git a/airflow/utils/db.py b/airflow/utils/db.py\nindex 2ebc1e51ed04e..bc4dd9ba912d5 100644\n--- a/airflow/utils/db.py\n+++ b/airflow/utils/db.py\n@@ -1159,7 +1159,7 @@ def _check_migration_errors(session: Session = NEW_SESSION) -> Iterable[str]:\n     :rtype: list[str]\n     \"\"\"\n     check_functions: Tuple[Callable[..., Iterable[str]], ...] = (\n-        check_task_fail_for_duplicates,\n+        # todo: check task fail for duplicates\n         check_conn_id_duplicates,\n         check_conn_type_null,\n         check_run_id_null,\n",
    "generated_commit": "Fixing typo in error message",
    "label_commit": "Temporarily disable task_fail pre-upgrade duplicates check (#22839)\n\nI am reworking it to actually move the rows but for now we can disable it.",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": -0.06311534345149994,
    "answer_correctness": 0.18523461797585714
  },
  {
    "diff": "diff --git a/sky/backends/backend_utils.py b/sky/backends/backend_utils.py\nindex 95e80d843e6..235c2ed9e60 100644\n--- a/sky/backends/backend_utils.py\n+++ b/sky/backends/backend_utils.py\n@@ -242,8 +242,11 @@ def _optimize_file_mounts(yaml_path: str) -> None:\n \n     # (For local) Move all runtime files, including the just-written yaml, to\n     # local_runtime_files_dir/.\n-    all_local_sources = ' '.join(\n-        local_src for local_src in file_mounts.values())\n+    all_local_sources = ''\n+    for local_src in file_mounts.values():\n+        full_local_src = str(pathlib.Path(local_src).expanduser())\n+        # Add quotes for paths containing spaces.\n+        all_local_sources += f'{full_local_src!r} '\n     # Takes 10-20 ms on laptop incl. 3 clouds' credentials.\n     subprocess.run(f'cp -r {all_local_sources} {local_runtime_files_dir}/',\n                    shell=True,\n",
    "generated_commit": "Fix path lookup (#2187)\n\nCo-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>",
    "label_commit": "Fix bug when .sky absolute path contains space (#1247)\n\nFix ~/.sky filepath with space",
    "bleu": 0.015603043420373072,
    "rouge_l": 0.14285714285714285,
    "meteor": 0.07462686567164178,
    "bert_score_f1": 0.024546168744564056,
    "answer_correctness": 0.200197225862001
  },
  {
    "diff": "diff --git a/lambda_functions/analyzer/main.py b/lambda_functions/analyzer/main.py\nindex b033d49..eb12f72 100644\n--- a/lambda_functions/analyzer/main.py\n+++ b/lambda_functions/analyzer/main.py\n@@ -7,6 +7,7 @@\n # Expects a binary YARA rules file to be at './compiled_yara_rules.bin'\n import logging\n import os\n+import urllib\n \n from yara import Error as YaraError\n from botocore.exceptions import ClientError as BotoError\n@@ -71,7 +72,9 @@ def analyze_lambda_handler(event_data, lambda_context):\n \n     LOGGER.info('Processing %d record(s)', len(event_data['S3Objects']))\n     for s3_key in event_data['S3Objects']:\n-        LOGGER.info('Analyzing %s', s3_key)\n+        # S3 keys in event notifications are url-encoded.\n+        s3_key = urllib.parse.unquote_plus(s3_key)\n+        LOGGER.info('Analyzing \"%s\"', s3_key)\n \n         with binary_info.BinaryInfo(os.environ['S3_BUCKET_NAME'], s3_key, ANALYZER) as binary:\n             result[binary.s3_identifier] = binary.summary()\ndiff --git a/tests/lambda_functions/analyzer/main_test.py b/tests/lambda_functions/analyzer/main_test.py\nindex c1401c9..370301f 100644\n--- a/tests/lambda_functions/analyzer/main_test.py\n+++ b/tests/lambda_functions/analyzer/main_test.py\n@@ -4,6 +4,7 @@\n import os\n import unittest\n from unittest import mock\n+import urllib\n \n import boto3\n from pyfakefs import fake_filesystem_unittest\n@@ -18,7 +19,7 @@\n     'observed_path': '/path/to/mock-evil.exe',\n     'reported_md5': 'REPORTED MD5'\n }\n-MOCK_S3_OBJECT_KEY = 'random-uuid'\n+MOCK_S3_OBJECT_KEY = 'space plus+file.test'\n \n # Mimics minimal parts of S3:ObjectAdded event that triggers the lambda function.\n LAMBDA_VERSION = 1\n@@ -77,7 +78,10 @@ def setUp(self):\n         boto3.client = mock.MagicMock(side_effect=self._boto3_client_mock)\n \n         # Create test event.\n-        self._test_event = {'S3Objects': [MOCK_S3_OBJECT_KEY], 'SQSReceipts': MOCK_SQS_RECEIPTS}\n+        self._test_event = {\n+            'S3Objects': [urllib.parse.quote_plus(MOCK_S3_OBJECT_KEY)],\n+            'SQSReceipts': MOCK_SQS_RECEIPTS\n+        }\n \n     def tearDown(self):\n         \"\"\"Restore boto3.client to its original.\"\"\"\n",
    "generated_commit": "fix: Updated CLI parsing error message (#2285)",
    "label_commit": "Url-decode S3 keys from the event notification (#54)",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": 0.199046328663826,
    "answer_correctness": 0.1822875118769634
  },
  {
    "diff": "diff --git a/mkt/feed/views.py b/mkt/feed/views.py\nindex dd0d75dccb0..f6f6e6873c1 100644\n--- a/mkt/feed/views.py\n+++ b/mkt/feed/views.py\n@@ -483,6 +483,10 @@ def filter_feed_element(self, request, feed_element, item_type):\n         If a feed element does not have enough apps, return None.\n         Else return the feed element.\n         \"\"\"\n+        if not feed_element:\n+            # Handle edge case where the ES index might get stale.\n+            return None\n+\n         if request.QUERY_PARAMS.get('filtering', '1') == '0':\n             # Without filtering\n             return feed_element\n",
    "generated_commit": "Fixing bug in filter_mel_element method (#707)",
    "label_commit": "handle feed items in ES that don't have feed element data (bug 1088001)",
    "bleu": 0.012712983380807008,
    "rouge_l": 0.18181818181818182,
    "meteor": 0.04065040650406504,
    "bert_score_f1": 0.14070042967796326,
    "answer_correctness": 0.18967832216186964
  },
  {
    "diff": "diff --git a/pontoon/translate/views.py b/pontoon/translate/views.py\nindex 5d7cf906ac..c0752ba7f6 100644\n--- a/pontoon/translate/views.py\n+++ b/pontoon/translate/views.py\n@@ -56,9 +56,10 @@ def translate(request, locale, project, resource):\n         )\n         if isinstance(project, HttpResponseRedirect):\n             return project\n-    # Validate ProjectLocale\n-    if locale not in project.locales.all():\n-        raise Http404\n+\n+        # Validate ProjectLocale\n+        if locale not in project.locales.all():\n+            raise Http404\n \n     context = {\n         \"locale\": get_preferred_locale(request),\n",
    "generated_commit": "Fixing typo in pull request (#233)",
    "label_commit": "Restore access to the All Projects Translate view (#2914)\n\nBy restoring dedentation.",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": 0.12852516770362854,
    "answer_correctness": 0.1929518284012505
  },
  {
    "diff": "diff --git a/celery/app/base.py b/celery/app/base.py\nindex ad89dea3a38..bd457f81d87 100644\n--- a/celery/app/base.py\n+++ b/celery/app/base.py\n@@ -20,6 +20,7 @@\n from contextlib import contextmanager\n from copy import deepcopy\n from functools import wraps\n+from pprint import pformat\n \n from kombu.clocks import LamportClock\n \n@@ -27,7 +28,7 @@\n from .. import platforms\n from ..exceptions import AlwaysEagerIgnored\n from ..local import maybe_evaluate\n-from ..utils import cached_property, instantiate, lpmerge\n+from ..utils import cached_property, instantiate, lpmerge, qualname\n \n from .defaults import DEFAULTS, find_deprecated_settings, find\n \n@@ -35,10 +36,16 @@\n if kombu.VERSION < (2, 0):\n     raise ImportError(\"Celery requires Kombu version 1.1.0 or higher.\")\n \n+SETTINGS_INFO = \"\"\"%s %s\"\"\"\n+\n BUGREPORT_INFO = \"\"\"\n platform -> system:%(system)s arch:%(arch)s imp:%(py_i)s\n software -> celery:%(celery_v)s kombu:%(kombu_v)s py:%(py_v)s\n+loader   -> %(loader)s\n settings -> transport:%(transport)s results:%(results)s\n+\n+%(human_settings)s\n+\n \"\"\"\n \n \n@@ -350,7 +357,32 @@ def bugreport(self):\n                                  \"kombu_v\": kombu.__version__,\n                                  \"py_v\": _platform.python_version(),\n                                  \"transport\": self.conf.BROKER_TRANSPORT,\n-                                 \"results\": self.conf.CELERY_RESULT_BACKEND}\n+                                 \"results\": self.conf.CELERY_RESULT_BACKEND,\n+                                 \"human_settings\": self.human_settings(),\n+                                 \"loader\": qualname(self.loader.__class__)}\n+\n+    def _pformat(self, value, width=80, nl_width=80, **kw):\n+\n+        if isinstance(value, dict):\n+            return \"{\\n %s\" % (\n+                                pformat(value, width=nl_width, indent=4, **kw)[1:])\n+        elif isinstance(value, tuple):\n+            return \"\\n%s%s\" % (' ' * 4,\n+                                pformat(value, width=nl_width, **kw))\n+        else:\n+            return pformat(value, width=width, **kw)\n+\n+    def human_settings(self):\n+        return \"\\n\".join(SETTINGS_INFO % (key + ':',\n+                                          self._pformat(value, width=50))\n+                    for key, value in self.filter_user_settings().iteritems())\n+\n+    def filter_user_settings(self):\n+        user_settings = {}\n+        # the last stash is the default settings, so just skip that\n+        for stash in self.conf._order[:-1]:\n+            user_settings.update(stash)\n+        return user_settings\n \n     @property\n     def pool(self):\ndiff --git a/celery/app/control.py b/celery/app/control.py\nindex ee81f8fa8f0..ba17a8644fa 100644\n--- a/celery/app/control.py\n+++ b/celery/app/control.py\n@@ -49,6 +49,9 @@ def _request(self, command, **kwargs):\n                                       callback=self.callback,\n                                       timeout=self.timeout, reply=True))\n \n+    def report(self):\n+        return self._request(\"report\")\n+\n     def active(self, safe=False):\n         return self._request(\"dump_active\", safe=safe)\n \ndiff --git a/celery/bin/celeryctl.py b/celery/bin/celeryctl.py\nindex 057fd18250a..cd7de1f8150 100644\n--- a/celery/bin/celeryctl.py\n+++ b/celery/bin/celeryctl.py\n@@ -291,7 +291,8 @@ class inspect(Command):\n                \"disable_events\": 1.0,\n                \"ping\": 0.2,\n                \"add_consumer\": 1.0,\n-               \"cancel_consumer\": 1.0}\n+               \"cancel_consumer\": 1.0,\n+               \"report\": 1.0}\n     option_list = Command.option_list + (\n                 Option(\"--timeout\", \"-t\", type=\"float\", dest=\"timeout\",\n                     default=None,\n@@ -497,6 +498,14 @@ def run(self, *args, **kwargs):\n help = command(help)\n \n \n+class report(Command):\n+\n+    def run(self, *args, **kwargs):\n+        print(self.app.bugreport())\n+        return EX_OK\n+report = command(report)\n+\n+\n class celeryctl(CeleryCommand):\n     commands = commands\n     enable_config_from_cmdline = True\ndiff --git a/celery/worker/control.py b/celery/worker/control.py\nindex f32413fe017..b8411646e18 100644\n--- a/celery/worker/control.py\n+++ b/celery/worker/control.py\n@@ -50,6 +50,11 @@ def revoke(panel, task_id, terminate=False, signal=None, **kwargs):\n     return {\"ok\": \"task %s %s\" % (task_id, action)}\n \n \n+@Panel.register\n+def report(panel):\n+    return {\"ok\": panel.app.bugreport()}\n+\n+\n @Panel.register\n def enable_events(panel):\n     dispatcher = panel.consumer.event_dispatcher\n",
    "generated_commit": "Pidbox now uses the new signals API to report on errors",
    "label_commit": "Bugreport now available as a command and broadcast command\n\n- Get it from a Python repl:\n\n    >>> import celery\n    >>> print(celery.bugreport())\n\n- Use celeryctl:\n\n    $ celeryctl report\n\n- Get it from remote workers:\n\n    $ celeryctl inspect report",
    "bleu": 0.002108929071392202,
    "rouge_l": 0.13953488372093023,
    "meteor": 0.0436046511627907,
    "bert_score_f1": -0.1674715280532837,
    "answer_correctness": 0.18960435553264365
  },
  {
    "diff": "diff --git a/knowledge_repo/app/routes/web_editor.py b/knowledge_repo/app/routes/web_editor.py\nindex 99aa4cfd0..e6dccfb6b 100644\n--- a/knowledge_repo/app/routes/web_editor.py\n+++ b/knowledge_repo/app/routes/web_editor.py\n@@ -139,8 +139,8 @@ def save_post():\n     kp = None\n     if path in current_repo:\n         kp = current_repo.post(path)\n-        if g.user.username not in kp.headers['authors']:\n-            return json.dumps({'msg': (\"Post with path {} already exists and you are not an author!\",\n+        if g.user.username not in kp.headers['authors'] and g.user.username not in current_repo.config.editors:\n+            return json.dumps({'msg': (\"Post with path {} already exists and you are not an author!\"\n                                        \"\\nPlease try a different path\").format(path),\n                                'success': False})\n \n",
    "generated_commit": "Fix web template rendering (#392)",
    "label_commit": "web editors can save even if not author (editors are admin)",
    "bleu": 0.016182712188007015,
    "rouge_l": 0.12500000000000003,
    "meteor": 0.04807692307692308,
    "bert_score_f1": 0.08293019980192184,
    "answer_correctness": 0.18289533104115602
  },
  {
    "diff": "diff --git a/airflow/providers/google/cloud/operators/bigquery.py b/airflow/providers/google/cloud/operators/bigquery.py\nindex 6fb94f21d7496..2fd6aacf5c618 100644\n--- a/airflow/providers/google/cloud/operators/bigquery.py\n+++ b/airflow/providers/google/cloud/operators/bigquery.py\n@@ -58,7 +58,7 @@\n     from airflow.models.taskinstancekey import TaskInstanceKey\n     from airflow.utils.context import Context\n \n-BIGQUERY_JOB_DETAILS_LINK_FMT = \"https://console.cloud.google.com/bigquery?j={project_id}:{location}:{job_id}\"\n+BIGQUERY_JOB_DETAILS_LINK_FMT = \"https://console.cloud.google.com/bigquery?j={job_id}\"\n \n \n class BigQueryUIColors(enum.Enum):\n@@ -90,17 +90,8 @@ def get_link(\n         *,\n         ti_key: TaskInstanceKey,\n     ):\n-        job_id_params = XCom.get_value(key=\"job_id_params\", ti_key=ti_key)\n-\n-        return (\n-            BIGQUERY_JOB_DETAILS_LINK_FMT.format(\n-                job_id=job_id_params[\"job_id\"],\n-                project_id=job_id_params[\"project_id\"],\n-                location=job_id_params[\"location\"],\n-            )\n-            if job_id_params\n-            else \"\"\n-        )\n+        job_id = XCom.get_value(key=\"job_id\", ti_key=ti_key)\n+        return BIGQUERY_JOB_DETAILS_LINK_FMT.format(job_id=job_id) if job_id else \"\"\n \n \n @attr.s(auto_attribs=True)\n@@ -119,16 +110,13 @@ def get_link(\n         *,\n         ti_key: TaskInstanceKey,\n     ):\n-        job_ids_params = XCom.get_value(key=\"job_id_params\", ti_key=ti_key)\n-        job_ids = job_ids_params[\"job_id\"]\n+        job_ids = XCom.get_value(key=\"job_id\", ti_key=ti_key)\n         if not job_ids:\n             return None\n         if len(job_ids) < self.index:\n             return None\n         job_id = job_ids[self.index]\n-        return BIGQUERY_JOB_DETAILS_LINK_FMT.format(\n-            job_id=job_id, project_id=job_ids_params[\"project_id\"], location=job_ids_params[\"location\"]\n-        )\n+        return BIGQUERY_JOB_DETAILS_LINK_FMT.format(job_id=job_id)\n \n \n class _BigQueryDbHookMixin:\n@@ -1196,13 +1184,7 @@ def execute(self, context: Context):\n             ]\n         else:\n             raise AirflowException(f\"argument 'sql' of type {type(str)} is neither a string nor an iterable\")\n-        job_id_params = {\n-            \"job_id\": job_id,\n-            \"project_id\": self.hook.project_id,\n-            \"location\": self.location if self.location else \"US\",\n-        }\n-        context[\"task_instance\"].xcom_push(key=\"job_id_params\", value=job_id_params)\n-        return job_id\n+        context[\"task_instance\"].xcom_push(key=\"job_id\", value=job_id)\n \n     def on_kill(self) -> None:\n         super().on_kill()\n@@ -2745,13 +2727,9 @@ def execute(self, context: Any):\n                                 persist_kwargs[\"dataset_id\"] = table[\"datasetId\"]\n                                 persist_kwargs[\"project_id\"] = table[\"projectId\"]\n                             BigQueryTableLink.persist(**persist_kwargs)\n+\n         self.job_id = job.job_id\n-        job_id_params = {\n-            \"job_id\": job_id,\n-            \"project_id\": self.project_id or self.hook.project_id,\n-            \"location\": self.location if self.location else \"US\",\n-        }\n-        context[\"ti\"].xcom_push(key=\"job_id_params\", value=job_id_params)\n+        context[\"ti\"].xcom_push(key=\"job_id\", value=self.job_id)\n         # Wait for the job to complete\n         if not self.deferrable:\n             job.result(timeout=self.result_timeout, retry=self.result_retry)\ndiff --git a/tests/providers/google/cloud/operators/test_bigquery.py b/tests/providers/google/cloud/operators/test_bigquery.py\nindex 367ef99cbbe1b..25b341a4c38a5 100644\n--- a/tests/providers/google/cloud/operators/test_bigquery.py\n+++ b/tests/providers/google/cloud/operators/test_bigquery.py\n@@ -83,10 +83,6 @@\n }\n TEST_TABLE = \"test-table\"\n GCP_CONN_ID = \"google_cloud_default\"\n-TEST_JOB_ID_1 = \"test-job-id\"\n-TEST_JOB_ID_2 = \"test-123\"\n-TEST_FULL_JOB_ID = f\"{TEST_GCP_PROJECT_ID}:{TEST_DATASET_LOCATION}:{TEST_JOB_ID_1}\"\n-TEST_FULL_JOB_ID_2 = f\"{TEST_GCP_PROJECT_ID}:{TEST_DATASET_LOCATION}:{TEST_JOB_ID_2}\"\n \n \n class TestBigQueryCreateEmptyTableOperator:\n@@ -676,15 +672,11 @@ def test_bigquery_operator_extra_serialized_field_when_single_query(\n \n         # Check DeSerialized version of operator link\n         assert isinstance(list(simple_task.operator_extra_links)[0], BigQueryConsoleLink)\n-        test_job_id_params = {\n-            \"job_id\": TEST_JOB_ID_1,\n-            \"project_id\": TEST_GCP_PROJECT_ID,\n-            \"location\": TEST_DATASET_LOCATION,\n-        }\n-        ti.xcom_push(\"job_id_params\", test_job_id_params)\n+\n+        ti.xcom_push(\"job_id\", 12345)\n \n         url = simple_task.get_extra_links(ti, BigQueryConsoleLink.name)\n-        assert url == f\"https://console.cloud.google.com/bigquery?j={TEST_FULL_JOB_ID}\"\n+        assert url == \"https://console.cloud.google.com/bigquery?j=12345\"\n \n     @pytest.mark.need_serialized_dag\n     def test_bigquery_operator_extra_serialized_field_when_multiple_queries(\n@@ -719,23 +711,17 @@ def test_bigquery_operator_extra_serialized_field_when_multiple_queries(\n         # Check DeSerialized version of operator link\n         assert isinstance(list(simple_task.operator_extra_links)[0], BigQueryConsoleIndexableLink)\n \n-        test_job_id_params = {\n-            \"job_id\": [TEST_JOB_ID_1, TEST_JOB_ID_2],\n-            \"project_id\": TEST_GCP_PROJECT_ID,\n-            \"location\": TEST_DATASET_LOCATION,\n-        }\n-        ti.xcom_push(key=\"job_id_params\", value=test_job_id_params)\n+        job_id = [\"123\", \"45\"]\n+        ti.xcom_push(key=\"job_id\", value=job_id)\n \n         assert {\"BigQuery Console #1\", \"BigQuery Console #2\"} == simple_task.operator_extra_link_dict.keys()\n \n-        assert (\n-            f\"https://console.cloud.google.com/bigquery?j={TEST_FULL_JOB_ID}\"\n-            == simple_task.get_extra_links(ti, \"BigQuery Console #1\")\n+        assert \"https://console.cloud.google.com/bigquery?j=123\" == simple_task.get_extra_links(\n+            ti, \"BigQuery Console #1\"\n         )\n \n-        assert (\n-            f\"https://console.cloud.google.com/bigquery?j={TEST_FULL_JOB_ID_2}\"\n-            == simple_task.get_extra_links(ti, \"BigQuery Console #2\")\n+        assert \"https://console.cloud.google.com/bigquery?j=45\" == simple_task.get_extra_links(\n+            ti, \"BigQuery Console #2\"\n         )\n \n     @mock.patch(\"airflow.providers.google.cloud.operators.bigquery.BigQueryHook\")\n@@ -754,9 +740,7 @@ def test_bigquery_operator_extra_link_when_missing_job_id(\n \n     @mock.patch(\"airflow.providers.google.cloud.operators.bigquery.BigQueryHook\")\n     def test_bigquery_operator_extra_link_when_single_query(\n-        self,\n-        mock_hook,\n-        create_task_instance_of_operator,\n+        self, mock_hook, create_task_instance_of_operator\n     ):\n         ti = create_task_instance_of_operator(\n             BigQueryExecuteQueryOperator,\n@@ -767,15 +751,11 @@ def test_bigquery_operator_extra_link_when_single_query(\n         )\n         bigquery_task = ti.task\n \n-        test_job_id_params = {\n-            \"job_id\": TEST_JOB_ID_1,\n-            \"project_id\": TEST_GCP_PROJECT_ID,\n-            \"location\": TEST_DATASET_LOCATION,\n-        }\n-        ti.xcom_push(key=\"job_id_params\", value=test_job_id_params)\n-        assert (\n-            f\"https://console.cloud.google.com/bigquery?j={TEST_FULL_JOB_ID}\"\n-            == bigquery_task.get_extra_links(ti, BigQueryConsoleLink.name)\n+        job_id = \"12345\"\n+        ti.xcom_push(key=\"job_id\", value=job_id)\n+\n+        assert f\"https://console.cloud.google.com/bigquery?j={job_id}\" == bigquery_task.get_extra_links(\n+            ti, BigQueryConsoleLink.name\n         )\n \n     @mock.patch(\"airflow.providers.google.cloud.operators.bigquery.BigQueryHook\")\n@@ -791,22 +771,17 @@ def test_bigquery_operator_extra_link_when_multiple_query(\n         )\n         bigquery_task = ti.task\n \n-        test_job_id_params = {\n-            \"job_id\": [TEST_JOB_ID_1, TEST_JOB_ID_2],\n-            \"project_id\": TEST_GCP_PROJECT_ID,\n-            \"location\": TEST_DATASET_LOCATION,\n-        }\n-        ti.xcom_push(key=\"job_id_params\", value=test_job_id_params)\n+        job_id = [\"123\", \"45\"]\n+        ti.xcom_push(key=\"job_id\", value=job_id)\n+\n         assert {\"BigQuery Console #1\", \"BigQuery Console #2\"} == bigquery_task.operator_extra_link_dict.keys()\n \n-        assert (\n-            f\"https://console.cloud.google.com/bigquery?j={TEST_FULL_JOB_ID}\"\n-            == bigquery_task.get_extra_links(ti, \"BigQuery Console #1\")\n+        assert \"https://console.cloud.google.com/bigquery?j=123\" == bigquery_task.get_extra_links(\n+            ti, \"BigQuery Console #1\"\n         )\n \n-        assert (\n-            f\"https://console.cloud.google.com/bigquery?j={TEST_FULL_JOB_ID_2}\"\n-            == bigquery_task.get_extra_links(ti, \"BigQuery Console #2\")\n+        assert \"https://console.cloud.google.com/bigquery?j=45\" == bigquery_task.get_extra_links(\n+            ti, \"BigQuery Console #2\"\n         )\n \n \n",
    "generated_commit": "[AIRFLOW-3264] Fix incorrect `ValueError` for GCP extra link when using GCS URL (#4346)",
    "label_commit": "Revert \"Fix `BIGQUERY_JOB_DETAILS_LINK_FMT` in `BigQueryConsoleLink` (#31457)\" (#31935)\n\nThis reverts commit c7072c0490cb80b448622a27eb1056576d6b92a4.",
    "bleu": 0,
    "rouge_l": 0.1379310344827586,
    "meteor": 0.0,
    "bert_score_f1": -0.12859651446342468,
    "answer_correctness": 0.19569561247713674
  },
  {
    "diff": "diff --git a/airflow/providers/microsoft/mssql/hooks/mssql.py b/airflow/providers/microsoft/mssql/hooks/mssql.py\nindex 10416aae4a698..55c1f2b188c8d 100644\n--- a/airflow/providers/microsoft/mssql/hooks/mssql.py\n+++ b/airflow/providers/microsoft/mssql/hooks/mssql.py\n@@ -16,10 +16,7 @@\n # specific language governing permissions and limitations\n # under the License.\n \"\"\"\n-This module is deprecated due to the discontinuation of the pymssql project.\n-See https://github.com/pymssql/pymssql/issues/668.\n-Support for pymssql will be removed in Airflow 2.0.\n-Please use :mod:`~airflow.providers.odbc.hooks.odbc`.\n+Microsoft SQLServer hook module\n \"\"\"\n \n import warnings\n@@ -28,25 +25,9 @@\n \n from airflow.hooks.dbapi_hook import DbApiHook\n \n-warnings.warn(\n-    (\n-        \"This module is deprecated due to the discontinuation of the pymssql project.\\n\"\n-        \"See https://github.com/pymssql/pymssql/issues/668.\\n\"\n-        \"Support for pymssql will be removed in airflow 2.0.\\n\"\n-        \"Please use `airflow.providers.odbc.hooks.odbc`.\\n\"\n-    ),\n-    DeprecationWarning,\n-    stacklevel=2,\n-)\n-\n \n class MsSqlHook(DbApiHook):\n     \"\"\"\n-    .. warning::\n-\n-        This class is deprecated.\n-        Please use :py:class:`~airflow.providers.odbc.hooks.odbc.OdbcHook`.\n-\n     Interact with Microsoft SQL Server.\n     \"\"\"\n \n",
    "generated_commit": "[AIRFLOW-XXX] Remove deprecated docstring (#3936)",
    "label_commit": "Pymssql is maintained again (#11537)\n\nSee https://github.com/pymssql/pymssql/commit/20457589b2878800cb40c1dd4f3045a4bb259675",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": -0.203483447432518,
    "answer_correctness": 0.1866578676608388
  },
  {
    "diff": "diff --git a/airflow/models.py b/airflow/models.py\nindex 3cf7942c24fa0..59aae74707ab0 100644\n--- a/airflow/models.py\n+++ b/airflow/models.py\n@@ -233,7 +233,7 @@ def collect_dags(\n             self.process_file(dag_folder, only_if_updated=only_if_updated)\n         elif os.path.isdir(dag_folder):\n             patterns = []\n-            for root, dirs, files in os.walk(dag_folder):\n+            for root, dirs, files in os.walk(dag_folder, followlinks=True):\n                 ignore_file = [f for f in files if f == '.airflowignore']\n                 if ignore_file:\n                     f = open(os.path.join(root, ignore_file[0]), 'r')\ndiff --git a/airflow/plugins_manager.py b/airflow/plugins_manager.py\nindex 60ac4432d31be..9dc114108c704 100644\n--- a/airflow/plugins_manager.py\n+++ b/airflow/plugins_manager.py\n@@ -35,7 +35,7 @@ def validate(cls):\n plugins = []\n \n # Crawl through the plugins folder to find AirflowPlugin derivatives\n-for root, dirs, files in os.walk(plugins_folder):\n+for root, dirs, files in os.walk(plugins_folder, followlinks=True):\n     for f in files:\n         try:\n             filepath = os.path.join(root, f)\n",
    "generated_commit": "Fixing minor issues in airflow",
    "label_commit": "Follow symlinks in DAG and Plugin folders",
    "bleu": 0.036015288308423515,
    "rouge_l": 0.16666666666666666,
    "meteor": 0.07352941176470587,
    "bert_score_f1": 0.07847823947668076,
    "answer_correctness": 0.18768054088020336
  },
  {
    "diff": "diff --git a/apps/forums/es_search.py b/apps/forums/es_search.py\nindex b6845c47f4c..2b55fbe9dc8 100644\n--- a/apps/forums/es_search.py\n+++ b/apps/forums/es_search.py\n@@ -18,6 +18,8 @@\n TYPE = 'type'\n ANALYZER = 'analyzer'\n INDEX = 'index'\n+TERM_VECTOR = 'term_vector'\n+STORE = 'store'\n \n INTEGER = 'integer'\n STRING = 'string'\n@@ -26,8 +28,12 @@\n \n ANALYZED = 'analyzed'\n \n+YES = 'yes'\n+\n SNOWBALL = 'snowball'\n \n+WITH_POS_OFFSETS = 'with_positions_offsets'\n+\n \n def setup_mapping(index):\n     from forums.models import Post\n@@ -46,7 +52,8 @@ def setup_mapping(index):\n             'is_locked': {TYPE: BOOLEAN},\n             'author_id': {TYPE: INTEGER},\n             'author_ord': {TYPE: STRING},\n-            'content': {TYPE: STRING, INDEX: ANALYZED, ANALYZER: SNOWBALL},\n+            'content': {TYPE: STRING, INDEX: ANALYZED, ANALYZER: SNOWBALL,\n+                        STORE: YES, TERM_VECTOR: WITH_POS_OFFSETS},\n             'created': {TYPE: DATE},\n             'updated': {TYPE: DATE},\n             'age': {TYPE: INTEGER},\ndiff --git a/apps/questions/es_search.py b/apps/questions/es_search.py\nindex 08d897602ac..7b8d315c5ee 100644\n--- a/apps/questions/es_search.py\n+++ b/apps/questions/es_search.py\n@@ -18,6 +18,8 @@\n TYPE = 'type'\n ANALYZER = 'analyzer'\n INDEX = 'index'\n+STORE = 'store'\n+TERM_VECTOR = 'term_vector'\n \n LONG = 'long'\n INTEGER = 'integer'\n@@ -29,6 +31,10 @@\n \n SNOWBALL = 'snowball'\n \n+YES = 'yes'\n+\n+WITH_POS_OFFSETS = 'with_positions_offsets'\n+\n \n def setup_mapping(index):\n     from questions.models import Question\n@@ -43,7 +49,8 @@ def setup_mapping(index):\n             'question_id': {TYPE: LONG},\n             'title': {TYPE: STRING, INDEX: ANALYZED, ANALYZER: SNOWBALL},\n             'question_content':\n-                {TYPE: STRING, INDEX: ANALYZED, ANALYZER: SNOWBALL},\n+                {TYPE: STRING, INDEX: ANALYZED, ANALYZER: SNOWBALL,\n+                 STORE: YES, TERM_VECTOR: WITH_POS_OFFSETS},\n             'answer_content':\n                 {TYPE: STRING, INDEX: ANALYZED, ANALYZER: SNOWBALL},\n             'replies': {TYPE: INTEGER},\n",
    "generated_commit": "Merge branch 'master' of https://github.com/openxla/intelligence",
    "label_commit": "Add term_vector to mapping for highlighted content",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": -0.09070505946874619,
    "answer_correctness": 0.17285314295149068
  },
  {
    "diff": "diff --git a/infra/spawn_data_pipeline.py b/infra/spawn_data_pipeline.py\nindex 3d3cb637be..a38b76fc52 100644\n--- a/infra/spawn_data_pipeline.py\n+++ b/infra/spawn_data_pipeline.py\n@@ -69,6 +69,8 @@ def main():\n \n     id_mapping = {}\n \n+    docker_tag = os.getenv(\"TAG\", None)\n+\n     # First pass, do the template rendering and dependencies resolution\n     tasks = []\n \n@@ -105,6 +107,20 @@ def main():\n \n         payload[\"dependencies\"] = new_dependencies\n \n+        # Override the Docker image tag if needed\n+        if docker_tag:\n+            base_image = payload[\"payload\"][\"image\"]\n+            splitted_image = base_image.rsplit(\":\", 1)\n+\n+            if len(splitted_image) > 1:\n+                err_msg = \"Docker tag should be None or 'latest', not {!r}\"\n+                assert splitted_image[1] == \"latest\", err_msg.format(splitted_image[1])\n+\n+            tagless_image = splitted_image[0]\n+\n+            new_image = \"{}:{}\".format(tagless_image, docker_tag)\n+            payload[\"payload\"][\"image\"] = new_image\n+\n         tasks.append((task_id, payload))\n \n     # Now sends them\n",
    "generated_commit": "fix: Updated container label to make it possible for running images (#1870)",
    "label_commit": "Add support for specific Docker tag in spawn_data_pipeline.py (#489)",
    "bleu": 0.017033186037639283,
    "rouge_l": 0.08333333333333333,
    "meteor": 0.10752688172043011,
    "bert_score_f1": 0.24147379398345947,
    "answer_correctness": 0.45263123843346653
  },
  {
    "diff": "diff --git a/privaterelay/models.py b/privaterelay/models.py\nindex 39764f3bc7..5ac0bd6189 100644\n--- a/privaterelay/models.py\n+++ b/privaterelay/models.py\n@@ -378,7 +378,7 @@ def date_phone_registered(self) -> datetime | None:\n         from phones.models import RealPhone, RelayNumber\n \n         try:\n-            real_phone = RealPhone.objects.get(user=self.user)\n+            real_phone = RealPhone.objects.get(user=self.user, verified=True)\n             relay_number = RelayNumber.objects.get(user=self.user)\n         except RealPhone.DoesNotExist:\n             return None\ndiff --git a/privaterelay/tests/model_tests.py b/privaterelay/tests/model_tests.py\nindex e97640fa82..c552dcfac2 100644\n--- a/privaterelay/tests/model_tests.py\n+++ b/privaterelay/tests/model_tests.py\n@@ -263,6 +263,20 @@ def test_real_phone_no_relay_number_returns_verified_date(self) -> None:\n         )\n         assert self.profile.date_phone_registered == datetime_now\n \n+    def test_two_real_phones_returns_verified_date(self) -> None:\n+        self.upgrade_to_phone()\n+        datetime_now = datetime.now(UTC)\n+        RealPhone.objects.create(\n+            user=self.profile.user, number=\"+12223335555\", verified=False\n+        )\n+        RealPhone.objects.create(\n+            user=self.profile.user,\n+            number=\"+12223334444\",\n+            verified=True,\n+            verified_date=datetime_now,\n+        )\n+        assert self.profile.date_phone_registered == datetime_now\n+\n     def test_real_phone_and_relay_number_w_created_at_returns_created_at_date(\n         self,\n     ) -> None:\n",
    "generated_commit": "Fix bug in some places where the wrong type is used by hamminging the year",
    "label_commit": "Fix /api/v1/profile for user with mult. RealPhones",
    "bleu": 0.013217947626377298,
    "rouge_l": 0.08333333333333334,
    "meteor": 0.06410256410256411,
    "bert_score_f1": -0.08924614638090134,
    "answer_correctness": 0.1799630198956741
  },
  {
    "diff": "diff --git a/bugbug/phabricator.py b/bugbug/phabricator.py\nindex 5f5929b4be..6af772af9d 100644\n--- a/bugbug/phabricator.py\n+++ b/bugbug/phabricator.py\n@@ -8,7 +8,7 @@\n \n PHABRICATOR_API = None\n \n-PROJECTS = {\n+TESTING_PROJECTS = {\n     \"PHID-PROJ-h7y4cs7m2o67iczw62pp\": \"testing-approved\",\n     \"PHID-PROJ-e4fcjngxcws3egiecv3r\": \"testing-exception-elsewhere\",\n     \"PHID-PROJ-iciyosoekrczpf2a4emw\": \"testing-exception-other\",\n@@ -46,3 +46,11 @@ def get(rev_ids):\n             progress_bar.update(len(rev_ids_group))\n \n     return data\n+\n+\n+def get_testing_projects(rev):\n+    return (\n+        TESTING_PROJECTS[projectPHID]\n+        for projectPHID in rev[\"attachments\"][\"projects\"][\"projectPHIDs\"]\n+        if projectPHID in TESTING_PROJECTS\n+    )\n",
    "generated_commit": "Add missing import and add tests (#609)",
    "label_commit": "Add a method to retrieve the testing tag associated to a Phabricator revision",
    "bleu": 0.014017732413740807,
    "rouge_l": 0.2,
    "meteor": 0.08064516129032259,
    "bert_score_f1": 0.11358324438333511,
    "answer_correctness": 0.18403603895603843
  },
  {
    "diff": "diff --git a/alerts/geomodel/locality.py b/alerts/geomodel/locality.py\nindex 97ed24fb0..e120c35b8 100644\n--- a/alerts/geomodel/locality.py\n+++ b/alerts/geomodel/locality.py\n@@ -5,7 +5,6 @@\n from mozdef_util.query_models import SearchQuery, TermMatch\n \n import alerts.geomodel.config as config\n-import alerts.geomodel.query as query\n \n \n # Default radius (in Kilometres) that a locality should have.\n@@ -13,10 +12,6 @@\n \n # TODO: Switch to dataclasses when we move to Python3.7+\n \n-def _dict_take(dictionary, keys):\n-    return {key: dictionary[key] for key in keys}\n-\n-\n class Locality(NamedTuple):\n     '''Represents a specific locality.\n     '''\n@@ -29,7 +24,6 @@ class Locality(NamedTuple):\n     longitude: float\n     radius: int\n \n-\n class State(NamedTuple):\n     '''Represents the state tracked for each user regarding their localities.\n     '''\n@@ -49,23 +43,6 @@ class Entry(NamedTuple):\n     identifier: Optional[str]\n     state: State\n \n-JournalInterface = Callable[[List[Entry], str], None]\n-\n-def wrap_journal(client: ESClient) -> JournalInterface:\n-    '''Wrap an `ElasticsearchClient` in a closure of type `JournalInterface`.\n-    '''\n-\n-    def wrapper(entries: List[Entry], esindex: str):\n-        for entry in entries:\n-            document = dict(entry.state._asdict())\n-\n-            client.save_object(\n-                index=esindex,\n-                body=document,\n-                doc_id=entry.identifier)\n-\n-    return wrapper\n-\n class Update(NamedTuple):\n     '''Produced by calls to functions operating on lists of `State`s to\n     indicate when an update was applied without having to maintain distinct\n@@ -86,6 +63,12 @@ def flat_map(fn: Callable[[State], 'Update'], u: 'Update') -> 'Update':\n \n         return Update(new.state, u.did_update or new.did_update)\n \n+JournalInterface = Callable[[List[Entry], str], None]\n+QueryInterface = Callable[[SearchQuery, str], List[Entry]]\n+\n+def _dict_take(dictionary, keys):\n+    return {key: dictionary[key] for key in keys}\n+\n def _update(state: State, from_evt: State) -> Update:\n     did_update = False\n \n@@ -115,6 +98,52 @@ def _update(state: State, from_evt: State) -> Update:\n \n     return Update(state, did_update)\n \n+def wrap_journal(client: ESClient) -> JournalInterface:\n+    '''Wrap an `ElasticsearchClient` in a closure of type `JournalInterface`.\n+    '''\n+\n+    def wrapper(entries: List[Entry], esindex: str):\n+        for entry in entries:\n+            document = dict(entry.state._asdict())\n+\n+            client.save_object(\n+                index=esindex,\n+                body=document,\n+                doc_id=entry.identifier)\n+\n+    return wrapper\n+\n+def wrap_query(client: ESClient) -> QueryInterface:\n+    '''Wrap an `ElasticsearchClient` in a closure of type `QueryInterface`.\n+    '''\n+\n+    def to_state(result: Dict[str, Any]) -> Optional[State]:\n+        try:\n+            result['localities'] = [\n+                Locality(**_dict_take(loc, Locality._fields))\n+                for loc in result['localities']\n+            ]\n+\n+            return State(**_dict_take(result, State._fields))\n+        except TypeError:\n+            return None\n+        except KeyError:\n+            return None\n+\n+    def wrapper(query: SearchQuery, esindex: str) -> List[Entry]:\n+        results = query.execute(client, indices=[esindex]).get('hits', [])\n+\n+        entries = []\n+        for event in results:\n+            opt_state = to_state(event.get('_source', {}))\n+\n+            if opt_state is not None:\n+                entries.append(Entry(event['_id'], opt_state))\n+\n+        return entries\n+\n+    return wrapper\n+\n def from_event(\n         event: Dict[str, Any],\n         radius=_DEFAULT_RADIUS_KM\n@@ -151,38 +180,16 @@ def from_event(\n         radius)\n \n def find_all(\n-        query_es: query.QueryInterface,\n+        query_es: QueryInterface,\n         locality: config.Localities\n ) -> List[Entry]:\n     '''Retrieve all locality state from ElasticSearch.\n     '''\n \n-    def to_state(result: Dict[str, Any]) -> Optional[State]:\n-        try:\n-            result['localities'] = [\n-                Locality(**_dict_take(loc, Locality._fields))\n-                for loc in result['localities']\n-            ]\n-\n-            return State(**_dict_take(result, State._fields))\n-        except TypeError:\n-            return None\n-        except KeyError:\n-            return None\n-\n     search = SearchQuery()\n     search.add_must([TermMatch('type_', 'locality')])\n \n-    results = query_es(search, locality.es_index)\n-\n-    entries = []\n-    for result in results:\n-        state = to_state(result['_source'])\n-\n-        if state is not None:\n-            entries.append(Entry(result['_id'], state))\n-\n-    return entries\n+    return query_es(search, locality.es_index)\n \n def merge(persisted: List[State], event_sourced: List[State]) -> List[Update]:\n     '''Merge together a list of states already stored in ElasticSearch\ndiff --git a/alerts/geomodel/query.py b/alerts/geomodel/query.py\ndeleted file mode 100644\nindex 26a39d8c0..000000000\n--- a/alerts/geomodel/query.py\n+++ /dev/null\n@@ -1,23 +0,0 @@\n-'''To make GeoModel code more testable, we abstract interaction with\n-ElasticSearch away via a \"query interface\".  This is just a function that,\n-called with an ES index and a `SearchQuery`, produces a list of dictionaries\n-as output.\n-'''\n-\n-from typing import Any, Callable, Dict, List\n-\n-from mozdef_util.elasticsearch_client import ElasticsearchClient as ESClient\n-from mozdef_util.query_models import SearchQuery\n-\n-\n-QueryInterface = Callable[[SearchQuery, str], List[Dict[str, Any]]]\n-\n-\n-def wrap(client: ESClient) -> QueryInterface:\n-    '''Wrap an `ElasticsearchClient` in a closure of type `QueryInterface`.\n-    '''\n-\n-    def wrapper(query: SearchQuery, esindex: str) -> List[Dict[str, Any]]:\n-        return query.execute(client, indices=[esindex]).get('hits', [])\n-\n-    return wrapper\ndiff --git a/tests/alerts/geomodel/test_locality.py b/tests/alerts/geomodel/test_locality.py\nindex 0a47cf48b..5fa7d78db 100644\n--- a/tests/alerts/geomodel/test_locality.py\n+++ b/tests/alerts/geomodel/test_locality.py\n@@ -1,15 +1,25 @@\n from datetime import datetime, timedelta\n import pytz\n+from typing import Any, Dict, List\n import unittest\n \n+from mozdef_util.query_models import SearchQuery\n+\n import alerts.geomodel.config as config\n import alerts.geomodel.locality as locality\n-import alerts.geomodel.query as query\n \n-from tests.alerts.geomodel.util import query_interface\n from tests.unit_test_suite import UnitTestSuite\n \n \n+def query_interface(results: List[locality.Entry]) -> locality.QueryInterface:\n+    '''Produce a `QueryInterface` that just returns the provided results.\n+    '''\n+\n+    def closure(q: SearchQuery, esi: str) -> List[locality.Entry]:\n+        return results\n+\n+    return closure\n+\n class TestLocalityElasticSearch(UnitTestSuite):\n     '''Tests for the `locality` module that interact with ES.\n     '''\n@@ -38,7 +48,7 @@ def test_simple_query(self):\n \n         self.refresh(self.event_index_name)\n \n-        query_iface = query.wrap(self.es_client)\n+        query_iface = locality.wrap_query(self.es_client)\n         loc_cfg = config.Localities(self.event_index_name, 30, 50.0)\n \n         entries = locality.find_all(query_iface, loc_cfg)\n@@ -88,7 +98,7 @@ def test_journaling(self):\n         \n         self.refresh(self.event_index_name)\n \n-        query_iface = query.wrap(self.es_client)\n+        query_iface = locality.wrap_query(self.es_client)\n         loc_cfg = config.Localities(self.event_index_name, 30, 50.0)\n \n         retrieved = locality.find_all(query_iface, loc_cfg)\n@@ -102,42 +112,26 @@ class TestLocality(unittest.TestCase):\n \n     def test_find_all_retrieves_all_states(self):\n         query_iface = query_interface([\n-            {\n-                '_id': 'id1',\n-                '_source': {\n-                    'type_': 'locality',\n-                    'username': 'tester1',\n-                    'localities': [\n-                        {\n-                            'sourceipaddress': '1.2.3.4',\n-                            'city': 'Toronto',\n-                            'country': 'CA',\n-                            'lastaction': datetime.utcnow(),\n-                            'latitude': 43.6529,\n-                            'longitude': -79.3849,\n-                            'radius': 50\n-                        }\n-                    ]\n-                }\n-            },\n-            {\n-                '_id': 'id2',\n-                '_source': {\n-                    'type_': 'locality',\n-                    'username': 'tester2',\n-                    'localities': [\n-                        {\n-                            'sourceipaddress': '4.3.2.1',\n-                            'city': 'San Francisco',\n-                            'country': 'USA',\n-                            'lastaction': datetime.utcnow(),\n-                            'latitude': 37.773972,\n-                            'longitude': -122.431297,\n-                            'radius': 50\n-                        }\n-                    ]\n-                }\n-            }\n+            locality.Entry('id1', locality.State('locality', 'tester1', [\n+                locality.Locality(\n+                    sourceipaddress='1.2.3.4',\n+                    city='Toronto',\n+                    country='CA',\n+                    lastaction=datetime.utcnow() - timedelta(minutes=3),\n+                    latitude=43.6529,\n+                    longitude=-79.3849,\n+                    radius=50)\n+            ])),\n+            locality.Entry('id2', locality.State('locality', 'tester2', [\n+                locality.Locality(\n+                    sourceipaddress='4.3.2.1',\n+                    city='San Francisco',\n+                    country='USA',\n+                    lastaction=datetime.utcnow(),\n+                    latitude=37.773972,\n+                    longitude=-122.431297,\n+                    radius=50)\n+            ]))\n         ])\n         loc_cfg = config.Localities('localities', 30, 50.0)\n \n@@ -152,55 +146,6 @@ def test_find_all_retrieves_all_states(self):\n         assert len(entries[0].state.localities) == 1\n         assert len(entries[1].state.localities) == 1\n \n-    def test_find_all_ignores_invalid_data(self):\n-        query_iface = query_interface([\n-            # Invalid top-level State\n-            {\n-                '_id': 'id1',\n-                '_source': {\n-                    'type__': 'locality',  # Should have only one underscore (_)\n-                    'username': 'tester',\n-                    'localities': []\n-                }\n-            },\n-            # Valid State\n-            {\n-                '_id': 'id2',\n-                '_source': {\n-                    'type_': 'locality',\n-                    'username': 'validtester',\n-                    'localities': []\n-                }\n-            },\n-            # Invalid locality data\n-            {\n-                '_id': 'id3',\n-                '_source': {\n-                    'type_': 'locality',\n-                    'username': 'tester2',\n-                    'localities': [\n-                        {\n-                            # Should be sourceipaddress; missing a 'd'\n-                            'sourceipadress': '1.2.3.4',\n-                            'city': 'San Francisco',\n-                            'country': 'USA',\n-                            'lastaction': datetime.utcnow(),\n-                            'latitude': 37.773972,\n-                            'longitude': -122.431297,\n-                            'radius': 50\n-                        }\n-                    ]\n-                }\n-            }\n-        ])\n-        loc_cfg = config.Localities('localities', 30, 50.0)\n-\n-        entries = locality.find_all(query_iface, loc_cfg)\n-        usernames = [entry.state.username for entry in entries]\n-\n-        assert len(entries) == 1\n-        assert usernames == ['validtester']\n-\n     def test_merge_updates_localities(self):\n         from_es = [\n             locality.State('locality', 'user1', [\ndiff --git a/tests/alerts/geomodel/util.py b/tests/alerts/geomodel/util.py\ndeleted file mode 100644\nindex 9dd88df9a..000000000\n--- a/tests/alerts/geomodel/util.py\n+++ /dev/null\n@@ -1,15 +0,0 @@\n-from typing import Any, Dict, List\n-\n-from mozdef_util.query_models import SearchQuery\n-\n-import alerts.geomodel.query as query\n-\n-\n-def query_interface(results: List[Dict[str, Any]]) -> query.QueryInterface:\n-    '''Produce a `QueryInterface` that just returns the provided results.\n-    '''\n-\n-    def closure(q: SearchQuery, esi: str) -> List[Dict[str, Any]]:\n-        return results\n-\n-    return closure\n",
    "generated_commit": "Remove unused code path",
    "label_commit": "Adapt QueryInterface specifically for localities since it isn't used for events anymore",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": 0.12662658095359802,
    "answer_correctness": 0.17730434797782021
  },
  {
    "diff": "diff --git a/celery/task/base.py b/celery/task/base.py\nindex 06b0c2df671..2c7ceee5064 100644\n--- a/celery/task/base.py\n+++ b/celery/task/base.py\n@@ -1,7 +1,6 @@\n import sys\n import warnings\n from datetime import datetime, timedelta\n-from Queue import Queue\n \n from billiard.serialization import pickle\n \n@@ -128,12 +127,6 @@ class Task(object):\n         limit), ``\"100/s\"`` (hundred tasks a second), ``\"100/m\"`` (hundred\n         tasks a minute), ``\"100/h\"`` (hundred tasks an hour)\n \n-    .. attribute:: rate_limit_queue_type\n-\n-        Type of queue used by the rate limiter for this kind of tasks.\n-        Default is a :class:`Queue.Queue`, but you can change this to\n-        a :class:`Queue.LifoQueue` or an invention of your own.\n-\n     .. attribute:: ignore_result\n \n         Don't store the return value of this task.\n@@ -200,7 +193,6 @@ class Task(object):\n     default_retry_delay = 3 * 60\n     serializer = conf.TASK_SERIALIZER\n     rate_limit = conf.DEFAULT_RATE_LIMIT\n-    rate_limit_queue_type = Queue\n     backend = default_backend\n     exchange_type = conf.DEFAULT_EXCHANGE_TYPE\n     delivery_mode = conf.DEFAULT_DELIVERY_MODE\n",
    "generated_commit": "Task: Sentrate limiter as it was not being set",
    "label_commit": "Removed Task.rate_limit_queue_type, as it was not really useful and makes refactoring easier.",
    "bleu": 0.21386638885976572,
    "rouge_l": 0.4800000000000001,
    "meteor": 0.3392094017094017,
    "bert_score_f1": 0.15568898618221283,
    "answer_correctness": 0.19569308934556126
  },
  {
    "diff": "diff --git a/mkt/api/v1/urls.py b/mkt/api/v1/urls.py\nindex d2dae1540b2..fc091337452 100644\n--- a/mkt/api/v1/urls.py\n+++ b/mkt/api/v1/urls.py\n@@ -57,6 +57,7 @@\n \n urlpatterns = patterns('',\n     url('', include('mkt.fireplace.urls')),\n+    url('', include('mkt.darjeeling.urls')),\n     url(r'^apps/', include(apps.urls)),\n     url(r'^apps/app/', include(subapps.urls)),\n     url(r'^apps/search/featured/', FeaturedSearchView.as_view(),\ndiff --git a/mkt/darjeeling/__init__.py b/mkt/darjeeling/__init__.py\nnew file mode 100644\nindex 00000000000..e69de29bb2d\ndiff --git a/mkt/darjeeling/tests/__init__.py b/mkt/darjeeling/tests/__init__.py\nnew file mode 100644\nindex 00000000000..e69de29bb2d\ndiff --git a/mkt/darjeeling/urls.py b/mkt/darjeeling/urls.py\nnew file mode 100644\nindex 00000000000..d2e92549d9b\n--- /dev/null\n+++ b/mkt/darjeeling/urls.py\n@@ -0,0 +1,10 @@\n+from django.conf.urls import patterns, url\n+\n+from mkt.darjeeling.views import DarjeelingAppList\n+\n+\n+urlpatterns = patterns('',\n+    url(r'^darjeeling/list/',\n+        DarjeelingAppList.as_view(),\n+        name='darjeeling-list'),\n+)\ndiff --git a/mkt/darjeeling/views.py b/mkt/darjeeling/views.py\nnew file mode 100644\nindex 00000000000..68ac3559678\n--- /dev/null\n+++ b/mkt/darjeeling/views.py\n@@ -0,0 +1,79 @@\n+import json\n+\n+from django.http import HttpResponse\n+\n+from rest_framework.generics import ListAPIView\n+from rest_framework.serializers import SerializerMethodField\n+\n+from mkt.api.base import CORSMixin, MarketplaceView\n+from mkt.collections.models import Collection\n+from mkt.fireplace.api import (FireplaceCollectionMembershipField,\n+                               FireplaceESAppSerializer)\n+\n+\n+class FakeCollection(object):\n+    def __init__(self, pk):\n+        self.pk = pk\n+\n+\n+class DarjeelingESAppSerializer(FireplaceESAppSerializer):\n+    featured = SerializerMethodField('is_featured')\n+\n+    class Meta(FireplaceESAppSerializer.Meta):\n+        fields = sorted(FireplaceESAppSerializer.Meta.fields + ['featured'])\n+        exclude = FireplaceESAppSerializer.Meta.exclude\n+\n+    def is_featured(self, obj):\n+        collections = [c['id'] for c in obj.es_data.get('collection', [])]\n+        return self.context['featured_pk'] in collections\n+\n+\n+class DarjeelingCollectionMembershipField(FireplaceCollectionMembershipField):\n+    app_serializer_classes = {\n+        'es': DarjeelingESAppSerializer,\n+    }\n+\n+\n+class DarjeelingAppList(CORSMixin, MarketplaceView, ListAPIView):\n+    \"\"\"\n+    Endpoint that darjeeling client consumes to fetch its app list. The list is\n+    actually made of 2 things:\n+    - One collection called \"darjeeling-apps\" containg all apps;\n+    - One collection called \"darjeeling-featured\" containing all homepage apps.\n+\n+    The first list is returned directly (without pagination) and since the\n+    second one is just supposed to be a subset of the first, only the app ids\n+    are returned.\n+    \"\"\"\n+    cors_allowed_methods = ['get']\n+    authentication_classes = []\n+    permission_classes = []\n+\n+    def get_collection(self, slug):\n+        \"\"\"\n+        Return a Fake Collection object with only the pk, for use with\n+        CollectionMembershipField. We can't simply do a Collection.objects.only\n+        query, because transforms get in the way (no_transforms doesn't remove\n+        translations atm)\n+        \"\"\"\n+        pk = Collection.objects.filter(slug=slug).values_list('pk', flat=True)\n+        return FakeCollection(pk[0])\n+\n+    def get_queryset(self):\n+        \"\"\"\n+        Fetch (and directly serialize using fireplace serializer) all apps\n+        belonging to the 'all' collection by querying ES.\n+        \"\"\"\n+        collection_all = self.get_collection('darjeeling-apps')\n+        membership = DarjeelingCollectionMembershipField(many=True)\n+        membership.context = self.get_serializer_context()\n+        membership.context['use-es-for-apps'] = True\n+        membership.context['featured_pk'] = (\n+            self.get_collection('darjeeling-featured').pk)\n+        return membership.field_to_native_es(collection_all, self.request)\n+\n+    def list(self, request, *args, **kwargs):\n+        data = {}\n+        data['all'] = self.get_queryset()\n+        data['featured'] = [d['id'] for d in data['all'] if d['featured']]\n+        return HttpResponse(json.dumps(data), content_type='application/json')\n",
    "generated_commit": "Add CALP model stubs",
    "label_commit": "[WIP] Create API endpoint that aggregates Darjeeling collections (bug 994304)",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": 0.03923453018069267,
    "answer_correctness": 0.17661716783938067
  },
  {
    "diff": "diff --git a/bugbug/similarity.py b/bugbug/similarity.py\nindex 4c39427d23..e324a5d7d5 100644\n--- a/bugbug/similarity.py\n+++ b/bugbug/similarity.py\n@@ -82,7 +82,11 @@ def download_and_load_similarity_model(model_name):\n \n class BaseSimilarity(abc.ABC):\n     def __init__(\n-        self, cleanup_urls=True, nltk_tokenizer=False, confidence_threshold=0.8\n+        self,\n+        cleanup_urls=True,\n+        nltk_tokenizer=False,\n+        confidence_threshold=0.8,\n+        end_to_end=False,\n     ):\n         self.cleanup_functions = [\n             feature_cleanup.responses(),\n@@ -98,6 +102,10 @@ def __init__(\n         self.nltk_tokenizer = nltk_tokenizer\n         self.confidence_threshold = confidence_threshold\n \n+        self.duplicatemodel = (\n+            DuplicateModel.load(\"duplicatemodel\") if end_to_end else None\n+        )\n+\n     def get_text(self, bug, all_comments=False):\n         if all_comments:\n             comments = \" \".join(comment[\"text\"] for comment in bug[\"comments\"])\n@@ -106,6 +114,29 @@ def get_text(self, bug, all_comments=False):\n \n         return \"{} {}\".format(bug[\"summary\"], comments)\n \n+    def get_similar_bugs(self, bug):\n+        similar_bug_ids = self.search_similar_bugs(bug)\n+        if self.duplicatemodel:\n+            similar_bugs = [\n+                bug for bug in bugzilla.get_bugs() if bug[\"id\"] in similar_bug_ids\n+            ]\n+            bug_couples = [(bug, similar_bug) for similar_bug in similar_bugs]\n+            probs_bug_couples = sorted(\n+                zip(\n+                    self.duplicatemodel.classify(bug_couples, probabilities=True),\n+                    bug_couples,\n+                ),\n+                key=lambda v: -v[0][1],\n+            )\n+\n+            similar_bug_ids = [\n+                similar_bug[\"id\"]\n+                for prob, (bug, similar_bug) in probs_bug_couples\n+                if prob[1] > self.confidence_threshold\n+            ]\n+\n+        return similar_bug_ids\n+\n     def text_preprocess(self, text, stemming=True, lemmatization=False, join=False):\n \n         for func in self.cleanup_functions:\n@@ -134,7 +165,7 @@ def text_preprocess(self, text, stemming=True, lemmatization=False, join=False):\n             return \" \".join(word for word in text)\n         return text\n \n-    def evaluation(self, end_to_end=False):\n+    def evaluation(self):\n         # A map from bug ID to its duplicate IDs\n         duplicates = defaultdict(set)\n         all_ids = set(\n@@ -144,9 +175,6 @@ def evaluation(self, end_to_end=False):\n             and \"dupeme\" not in bug[\"keywords\"]\n         )\n \n-        if end_to_end:\n-            duplicatemodel = DuplicateModel.load(\"duplicatemodel\")\n-\n         for bug in bugzilla.get_bugs():\n             dupes = [entry for entry in bug[\"duplicates\"] if entry in all_ids]\n             if bug[\"dupe_of\"] in all_ids:\n@@ -176,17 +204,6 @@ def evaluation(self, end_to_end=False):\n                 num_hits = 0\n                 queries += 1\n                 similar_bugs = self.get_similar_bugs(bug)[:10]\n-                if end_to_end:\n-                    sim_bugs = [\n-                        bug for bug in bugzilla.get_bugs() if bug[\"id\"] in similar_bugs\n-                    ]\n-                    bug_couples = [(bug, sim_bugs[bug_id]) for bug_id in sim_bugs]\n-                    probs = duplicatemodel.classify(bug_couples, probabilities=True)\n-                    similar_bugs = [\n-                        similar_bugs[idx]\n-                        for idx, prob in enumerate(probs)\n-                        if prob[1] > self.confidence_threshold\n-                    ]\n \n                 # Recall\n                 for idx, item in enumerate(duplicates[bug[\"id\"]]):\n@@ -280,7 +297,7 @@ def __init__(\n             output_prefix=\"simdata.shdat\", corpus=corpus_lsi, num_features=300\n         )\n \n-    def get_similar_bugs(self, query, k=10):\n+    def search_similar_bugs(self, query, k=10):\n         query_summary = \"{} {}\".format(query[\"summary\"], query[\"comments\"][0][\"text\"])\n         query_summary = self.text_preprocess(query_summary)\n \n@@ -329,7 +346,7 @@ def __init__(\n         self.vectorizer.fit(text)\n         self.similarity_calculator.fit(self.vectorizer.transform(text))\n \n-    def get_similar_bugs(self, query):\n+    def search_similar_bugs(self, query):\n \n         processed_query = self.vectorizer.transform([self.get_text(query)])\n         _, indices = self.similarity_calculator.kneighbors(processed_query)\n@@ -387,7 +404,7 @@ def __init__(\n \n     # word2vec.wmdistance calculates only the euclidean distance. To get the cosine distance,\n     # we're using the function with a few subtle changes. We compute the cosine distances\n-    # in the get_similar_bugs method and use this inside the wmdistance method.\n+    # in the search_similar_bugs method and use this inside the wmdistance method.\n     def wmdistance(self, document1, document2, all_distances, distance_metric=\"cosine\"):\n         model = self.w2vmodel\n         if len(document1) == 0 or len(document2) == 0:\n@@ -446,7 +463,7 @@ def calculate_all_distances(self, words):\n             dtype=np.double,\n         )\n \n-    def get_similar_bugs(self, query):\n+    def search_similar_bugs(self, query):\n \n         words = self.text_preprocess(self.get_text(query))\n         words = [word for word in words if word in self.w2vmodel.wv.vocab]\n@@ -530,7 +547,7 @@ def __init__(\n         self.dictionary = Dictionary(self.corpus)\n         self.tfidf = TfidfModel(dictionary=self.dictionary)\n \n-    def get_similar_bugs(self, query):\n+    def search_similar_bugs(self, query):\n \n         query = self.text_preprocess(self.get_text(query))\n         words = [\n@@ -646,7 +663,7 @@ def __init__(\n             bow, similarity_matrix, num_best=10\n         )\n \n-    def get_similar_bugs(self, query):\n+    def search_similar_bugs(self, query):\n         similarities = self.softcosinesimilarity[\n             self.dictionary.doc2bow(self.text_preprocess(self.get_text(query)))\n         ]\n@@ -683,7 +700,7 @@ def __init__(\n \n         self.model = BM25(self.corpus)\n \n-    def get_similar_bugs(self, query):\n+    def search_similar_bugs(self, query):\n         distances = self.model.get_scores(self.text_preprocess(self.get_text(query)))\n         id_dist = zip(self.bug_ids, distances)\n \n@@ -719,7 +736,7 @@ def __init__(\n \n         self.model = LdaModel([self.dictionary.doc2bow(text) for text in self.corpus])\n \n-    def get_similar_bugs(self, query):\n+    def search_similar_bugs(self, query):\n         query = self.text_preprocess(self.get_text(query))\n \n         dense1 = sparse2full(\n@@ -775,7 +792,7 @@ def index(self):\n         self.elastic_search.indices.delete(index=\"bugbug\", ignore=[400, 404])\n         bulk(self.elastic_search, self.make_documents())\n \n-    def get_similar_bugs(self, query):\n+    def search_similar_bugs(self, query):\n         find_similar = self.text_preprocess(\n             self.get_text(query, all_comments=True), stemming=False, join=True\n         )\n",
    "generated_commit": "Search distance calculation to be compatible",
    "label_commit": "Refactor similarity classes moving the duplicate model evaluation directly in the function to find similar bugs (#1588)",
    "bleu": 0.006527063301683154,
    "rouge_l": 0.08695652173913045,
    "meteor": 0.031446540880503145,
    "bert_score_f1": 0.012363847345113754,
    "answer_correctness": 0.1926680321382655
  },
  {
    "diff": "diff --git a/kitsune/inproduct/middleware.py b/kitsune/inproduct/middleware.py\nindex 5da589f8291..d5f2b838689 100644\n--- a/kitsune/inproduct/middleware.py\n+++ b/kitsune/inproduct/middleware.py\n@@ -1,7 +1,4 @@\n-try:\n-    from django.utils.deprecation import MiddlewareMixin\n-except ImportError:\n-    MiddlewareMixin = object\n+from django.utils.deprecation import MiddlewareMixin\n \n \n class EuBuildMiddleware(MiddlewareMixin):\ndiff --git a/kitsune/sumo/anonymous.py b/kitsune/sumo/anonymous.py\nindex f7bd4fad1e9..534fbffc5e4 100644\n--- a/kitsune/sumo/anonymous.py\n+++ b/kitsune/sumo/anonymous.py\n@@ -27,14 +27,10 @@\n import time\n \n from django.conf import settings\n+from django.utils.deprecation import MiddlewareMixin\n from django.utils.http import cookie_date\n \n \n-try:\n-    from django.utils.deprecation import MiddlewareMixin\n-except ImportError:\n-    MiddlewareMixin = object\n-\n # Use the system (hardware-based) random number generator if it exists.\n if hasattr(random, 'SystemRandom'):\n     randrange = random.SystemRandom().randrange\ndiff --git a/kitsune/sumo/middleware.py b/kitsune/sumo/middleware.py\nindex 7f3f47ff71a..90c63470124 100644\n--- a/kitsune/sumo/middleware.py\n+++ b/kitsune/sumo/middleware.py\n@@ -16,8 +16,8 @@\n from django.http.request import split_domain_port\n from django.shortcuts import render\n from django.utils import translation\n-from django.utils.cache import (add_never_cache_headers,\n-                                patch_response_headers, patch_vary_headers)\n+from django.utils.cache import add_never_cache_headers, patch_response_headers, patch_vary_headers\n+from django.utils.deprecation import MiddlewareMixin\n from django.utils.encoding import iri_to_uri, smart_bytes, smart_text\n import commonware.middleware\n import commonware.request.middleware\n@@ -28,11 +28,6 @@\n from kitsune.sumo.urlresolvers import Prefixer, set_url_prefixer, split_path\n from kitsune.sumo.views import handle403\n \n-try:\n-    from django.utils.deprecation import MiddlewareMixin\n-except ImportError:\n-    MiddlewareMixin = object\n-\n \n class EnforceHostIPMiddleware(EnforceHostMiddleware):\n     \"\"\"Modify the `EnforceHostMiddleware` to allow IP addresses\"\"\"\ndiff --git a/kitsune/twitter/middleware.py b/kitsune/twitter/middleware.py\nindex 88633e34090..c0e7e3d8e5a 100644\n--- a/kitsune/twitter/middleware.py\n+++ b/kitsune/twitter/middleware.py\n@@ -3,6 +3,7 @@\n \n from django import http\n from django.conf import settings\n+from django.utils.deprecation import MiddlewareMixin\n \n from kitsune.twitter import url, Session, REQUEST_KEY_NAME, REQUEST_SECRET_NAME\n from twython import TwythonError, TwythonAuthError\n@@ -10,12 +11,6 @@\n from kitsune.twitter import get_twitter_api\n \n \n-try:\n-    from django.utils.deprecation import MiddlewareMixin\n-except ImportError:\n-    MiddlewareMixin = object\n-\n-\n log = logging.getLogger(\"k\")\n \n \ndiff --git a/kitsune/users/middleware.py b/kitsune/users/middleware.py\nindex be5a6676659..55cb1372a80 100644\n--- a/kitsune/users/middleware.py\n+++ b/kitsune/users/middleware.py\n@@ -1,15 +1,11 @@\n from django.contrib.auth import logout\n from django.http import HttpResponseRedirect\n+from django.utils.deprecation import MiddlewareMixin\n+from django.utils.translation import ugettext_lazy as _lazy\n \n from kitsune.sumo.urlresolvers import reverse\n \n \n-try:\n-    from django.utils.deprecation import MiddlewareMixin\n-except ImportError:\n-    MiddlewareMixin = object\n-\n-\n class LogoutDeactivatedUsersMiddleware(MiddlewareMixin):\n     \"\"\"Verifies that user.is_active == True.\n \n",
    "generated_commit": "Remove unused imports and fix pylint warnings",
    "label_commit": "Remove unnecessary try/except around MiddlewareMixin\n\nThat was only needed if supporting multiple older versions of Django.",
    "bleu": 0.009131698392329843,
    "rouge_l": 0.08333333333333333,
    "meteor": 0.033112582781456956,
    "bert_score_f1": 0.07481065392494202,
    "answer_correctness": 0.19051828005614407
  },
  {
    "diff": "diff --git a/kitsune/questions/tests/test_templates.py b/kitsune/questions/tests/test_templates.py\nindex 35e73a5b1e5..bca4b0a9d5e 100644\n--- a/kitsune/questions/tests/test_templates.py\n+++ b/kitsune/questions/tests/test_templates.py\n@@ -121,8 +121,8 @@ def test_long_answer(self):\n         doc = pq(response.content)\n         error_msg = doc('ul.errorlist li a')[0]\n         eq_(error_msg.text, 'Please keep the length of your content to ' +\n-                            '10,000 characters or less. It is currently ' +\n-                            '10,001 characters.')\n+                            '10000 characters or less. It is currently ' +\n+                            '10001 characters.')\n \n     def test_solve_unsolve(self):\n         \"\"\"Test accepting a solution and undoing.\"\"\"\ndiff --git a/kitsune/sumo/form_fields.py b/kitsune/sumo/form_fields.py\nindex e9ee88d4901..b7dea3509b9 100644\n--- a/kitsune/sumo/form_fields.py\n+++ b/kitsune/sumo/form_fields.py\n@@ -1,14 +1,9 @@\n from django import forms\n-from django.conf import settings\n from django.contrib.auth.models import User\n from django.core import validators\n from django.core.exceptions import ValidationError\n-from django.utils import translation\n from django.utils.translation import ugettext as _\n \n-from babel import Locale, localedata\n-from babel.support import Format\n-\n \n class TypedMultipleChoiceField(forms.MultipleChoiceField):\n     \"\"\"Coerce choices to a specific type and don't validate them.\n@@ -78,32 +73,3 @@ def to_python(self, value):\n                     raise forms.ValidationError(msg.format(username=username))\n \n         return users\n-\n-\n-class BaseValidator(validators.BaseValidator):\n-    \"\"\"Override the BaseValidator from django to format numbers.\"\"\"\n-    def __call__(self, value):\n-        cleaned = self.clean(value)\n-        params = {'limit_value': _format_decimal(self.limit_value),\n-                  'show_value': _format_decimal(cleaned)}\n-        if self.compare(cleaned, self.limit_value):\n-            raise ValidationError(\n-                self.message % params,\n-                code=self.code,\n-                params=params,\n-            )\n-\n-\n-def _format_decimal(num, format=None):\n-    \"\"\"Returns the string of a number formatted for the current language.\n-\n-    Uses django's translation.get_language() to find the current language from\n-    the request.\n-    Falls back to the default language if babel does not support the current.\n-\n-    \"\"\"\n-    lang = translation.get_language()\n-    if not localedata.exists(lang):\n-        lang = settings.LANGUAGE_CODE\n-    locale = Locale(translation.to_locale(lang))\n-    return Format(locale).decimal(num, format)\ndiff --git a/kitsune/sumo/tests/test_form_fields.py b/kitsune/sumo/tests/test_form_fields.py\nindex d3a7466f7e7..86ece2e0476 100644\n--- a/kitsune/sumo/tests/test_form_fields.py\n+++ b/kitsune/sumo/tests/test_form_fields.py\n@@ -1,41 +1,11 @@\n from django.core.exceptions import ValidationError\n-from django.utils import translation\n \n from nose.tools import eq_\n \n-from kitsune.sumo.form_fields import _format_decimal, TypedMultipleChoiceField\n+from kitsune.sumo.form_fields import TypedMultipleChoiceField\n from kitsune.sumo.tests import TestCase\n \n \n-class TestFormatDecimal(TestCase):\n-\n-    def test_default_locale(self):\n-        \"\"\"Default locale just works\"\"\"\n-        num = _format_decimal(1234.567)\n-        eq_('1,234.567', num)\n-\n-    def test_fr_locale(self):\n-        \"\"\"French locale returns french format\"\"\"\n-        translation.activate('fr')\n-        num = _format_decimal(1234.567)\n-        eq_(u'1\\xa0234,567', num)\n-\n-    def test_xx_YY_locale(self):\n-        \"\"\"Falls back to English-like formatting for unknown locales\"\"\"\n-        translation.activate('xx-YY')\n-        num = _format_decimal(1234.567)\n-        eq_('1,234.567', num)\n-\n-    def test_fy_NL_locale(self):\n-        \"\"\"Falls back to English for unknown babel locales\"\"\"\n-        # Note: if this starts to fail for no apparent reason, it's probably\n-        # because babel learned about fy-NL since this test was written.\n-        translation.activate('fy-NL')\n-        eq_('fy-nl', translation.get_language())\n-        num = _format_decimal(1234.567)\n-        eq_('1,234.567', num)\n-\n-\n class TypedMultipleChoiceFieldTestCase(TestCase):\n     \"\"\"TypedMultipleChoiceField is just like MultipleChoiceField\n     except, instead of validating, it coerces types.\"\"\"\n",
    "generated_commit": "Remove super method",
    "label_commit": "Fix test for to match removed validator max_length decimal localization\n\nThis doesn't seem important enough to have to monkey patch the field.",
    "bleu": 0,
    "rouge_l": 0.07407407407407407,
    "meteor": 0.02487562189054727,
    "bert_score_f1": 0.11294190585613251,
    "answer_correctness": 0.17748758300573808
  },
  {
    "diff": "diff --git a/tests/providers/amazon/aws/hooks/test_secrets_manager.py b/tests/providers/amazon/aws/hooks/test_secrets_manager.py\nindex 788bbb5e043a4..c5af2001ecb77 100644\n--- a/tests/providers/amazon/aws/hooks/test_secrets_manager.py\n+++ b/tests/providers/amazon/aws/hooks/test_secrets_manager.py\n@@ -33,20 +33,15 @@ def test_get_conn_returns_a_boto3_connection(self):\n \n     def test_get_secret_string(self):\n         secret_name = \"arn:aws:secretsmanager:us-east-2:999999999999:secret:db_cluster-YYYYYYY\"\n-        secret_value = '{\"user\": \"test\"}'\n+        secret_value = \"test\"\n         hook = SecretsManagerHook(aws_conn_id=\"aws_default\")\n \n         create_param = {\n             \"Name\": secret_name,\n-        }\n-\n-        put_param = {\n-            \"SecretId\": secret_name,\n             \"SecretString\": secret_value,\n         }\n \n         hook.get_conn().create_secret(**create_param)\n-        hook.get_conn().put_secret_value(**put_param)\n \n         secret = hook.get_secret(secret_name)\n         assert secret == secret_value\n@@ -58,15 +53,10 @@ def test_get_secret_dict(self):\n \n         create_param = {\n             \"Name\": secret_name,\n-        }\n-\n-        put_param = {\n-            \"SecretId\": secret_name,\n             \"SecretString\": secret_value,\n         }\n \n         hook.get_conn().create_secret(**create_param)\n-        hook.get_conn().put_secret_value(**put_param)\n \n         secret = hook.get_secret_as_dict(secret_name)\n         assert secret == json.loads(secret_value)\n@@ -75,17 +65,9 @@ def test_get_secret_binary(self):\n         secret_name = \"arn:aws:secretsmanager:us-east-2:999999999999:secret:db_cluster-YYYYYYY\"\n         secret_value_binary = base64.b64encode(b'{\"username\": \"test\"}')\n         hook = SecretsManagerHook(aws_conn_id=\"aws_default\")\n-        create_param = {\n-            \"Name\": secret_name,\n-        }\n-\n-        put_param = {\n-            \"SecretId\": secret_name,\n-            \"SecretBinary\": secret_value_binary,\n-        }\n+        create_param = {\"Name\": secret_name, \"SecretBinary\": secret_value_binary}\n \n         hook.get_conn().create_secret(**create_param)\n-        hook.get_conn().put_secret_value(**put_param)\n \n         secret = hook.get_secret(secret_name)\n         assert secret == base64.b64decode(secret_value_binary)\ndiff --git a/tests/providers/amazon/aws/secrets/test_secrets_manager.py b/tests/providers/amazon/aws/secrets/test_secrets_manager.py\nindex fa70a12fcbf41..cb5dbd0210992 100644\n--- a/tests/providers/amazon/aws/secrets/test_secrets_manager.py\n+++ b/tests/providers/amazon/aws/secrets/test_secrets_manager.py\n@@ -37,16 +37,11 @@ def test_get_conn_value_full_url_mode(self):\n         secret_id = \"airflow/connections/test_postgres\"\n         create_param = {\n             \"Name\": secret_id,\n-        }\n-\n-        param = {\n-            \"SecretId\": secret_id,\n             \"SecretString\": \"postgresql://airflow:airflow@host:5432/airflow\",\n         }\n \n         secrets_manager_backend = SecretsManagerBackend()\n         secrets_manager_backend.client.create_secret(**create_param)\n-        secrets_manager_backend.client.put_secret_value(**param)\n \n         returned_uri = secrets_manager_backend.get_conn_value(conn_id=\"test_postgres\")\n         assert \"postgresql://airflow:airflow@host:5432/airflow\" == returned_uri\n@@ -63,10 +58,6 @@ def test_get_connection_broken_field_mode_url_encoding(self, are_secret_values_u\n         secret_id = \"airflow/connections/test_postgres\"\n         create_param = {\n             \"Name\": secret_id,\n-        }\n-\n-        param = {\n-            \"SecretId\": secret_id,\n             \"SecretString\": json.dumps(\n                 {\n                     \"conn_type\": \"postgresql\",\n@@ -82,7 +73,6 @@ def test_get_connection_broken_field_mode_url_encoding(self, are_secret_values_u\n             are_secret_values_urlencoded=are_secret_values_urlencoded\n         )\n         secrets_manager_backend.client.create_secret(**create_param)\n-        secrets_manager_backend.client.put_secret_value(**param)\n \n         conn = secrets_manager_backend.get_connection(conn_id=\"test_postgres\")\n \n@@ -97,10 +87,6 @@ def test_get_connection_broken_field_mode_extra_allows_nested_json(self):\n         secret_id = \"airflow/connections/test_postgres\"\n         create_param = {\n             \"Name\": secret_id,\n-        }\n-\n-        param = {\n-            \"SecretId\": secret_id,\n             \"SecretString\": json.dumps(\n                 {\n                     \"conn_type\": \"postgresql\",\n@@ -114,7 +100,6 @@ def test_get_connection_broken_field_mode_extra_allows_nested_json(self):\n \n         secrets_manager_backend = SecretsManagerBackend(full_url_mode=False)\n         secrets_manager_backend.client.create_secret(**create_param)\n-        secrets_manager_backend.client.put_secret_value(**param)\n \n         conn = secrets_manager_backend.get_connection(conn_id=\"test_postgres\")\n         assert conn.extra_dejson[\"foo\"] == \"bar\"\n@@ -124,17 +109,14 @@ def test_get_conn_value_broken_field_mode(self):\n         secret_id = \"airflow/connections/test_postgres\"\n         create_param = {\n             \"Name\": secret_id,\n-        }\n-\n-        param = {\n-            \"SecretId\": secret_id,\n-            \"SecretString\": '{\"user\": \"airflow\", \"pass\": \"airflow\", \"host\": \"host\", '\n-            '\"port\": 5432, \"schema\": \"airflow\", \"engine\": \"postgresql\"}',\n+            \"SecretString\": (\n+                '{\"user\": \"airflow\", \"pass\": \"airflow\", \"host\": \"host\", '\n+                '\"port\": 5432, \"schema\": \"airflow\", \"engine\": \"postgresql\"}'\n+            ),\n         }\n \n         secrets_manager_backend = SecretsManagerBackend(full_url_mode=False)\n         secrets_manager_backend.client.create_secret(**create_param)\n-        secrets_manager_backend.client.put_secret_value(**param)\n \n         conn = secrets_manager_backend.get_connection(conn_id=\"test_postgres\")\n         returned_uri = conn.get_uri()\n@@ -145,19 +127,16 @@ def test_get_conn_value_broken_field_mode_extra_words_added(self):\n         secret_id = \"airflow/connections/test_postgres\"\n         create_param = {\n             \"Name\": secret_id,\n-        }\n-\n-        param = {\n-            \"SecretId\": secret_id,\n-            \"SecretString\": '{\"usuario\": \"airflow\", \"pass\": \"airflow\", \"host\": \"host\", '\n-            '\"port\": 5432, \"schema\": \"airflow\", \"engine\": \"postgresql\"}',\n+            \"SecretString\": (\n+                '{\"usuario\": \"airflow\", \"pass\": \"airflow\", \"host\": \"host\", '\n+                '\"port\": 5432, \"schema\": \"airflow\", \"engine\": \"postgresql\"}'\n+            ),\n         }\n \n         secrets_manager_backend = SecretsManagerBackend(\n             full_url_mode=False, extra_conn_words={\"user\": [\"usuario\"]}\n         )\n         secrets_manager_backend.client.create_secret(**create_param)\n-        secrets_manager_backend.client.put_secret_value(**param)\n \n         conn = secrets_manager_backend.get_connection(conn_id=\"test_postgres\")\n         returned_uri = conn.get_uri()\n@@ -174,16 +153,11 @@ def test_get_conn_value_non_existent_key(self):\n         secret_id = \"airflow/connections/test_postgres\"\n         create_param = {\n             \"Name\": secret_id,\n-        }\n-\n-        param = {\n-            \"SecretId\": secret_id,\n             \"SecretString\": \"postgresql://airflow:airflow@host:5432/airflow\",\n         }\n \n         secrets_manager_backend = SecretsManagerBackend()\n         secrets_manager_backend.client.create_secret(**create_param)\n-        secrets_manager_backend.client.put_secret_value(**param)\n \n         assert secrets_manager_backend.get_conn_value(conn_id=conn_id) is None\n         assert secrets_manager_backend.get_connection(conn_id=conn_id) is None\n@@ -192,15 +166,10 @@ def test_get_conn_value_non_existent_key(self):\n     def test_get_variable(self):\n \n         secret_id = \"airflow/variables/hello\"\n-        create_param = {\n-            \"Name\": secret_id,\n-        }\n-\n-        param = {\"SecretId\": secret_id, \"SecretString\": \"world\"}\n+        create_param = {\"Name\": secret_id, \"SecretString\": \"world\"}\n \n         secrets_manager_backend = SecretsManagerBackend()\n         secrets_manager_backend.client.create_secret(**create_param)\n-        secrets_manager_backend.client.put_secret_value(**param)\n \n         returned_uri = secrets_manager_backend.get_variable(\"hello\")\n         assert \"world\" == returned_uri\n@@ -212,14 +181,10 @@ def test_get_variable_non_existent_key(self):\n         SystemsManagerParameterStoreBackend.get_variables should return None\n         \"\"\"\n         secret_id = \"airflow/variables/hello\"\n-        create_param = {\n-            \"Name\": secret_id,\n-        }\n-        param = {\"SecretId\": secret_id, \"SecretString\": \"world\"}\n+        create_param = {\"Name\": secret_id, \"SecretString\": \"world\"}\n \n         secrets_manager_backend = SecretsManagerBackend()\n         secrets_manager_backend.client.create_secret(**create_param)\n-        secrets_manager_backend.client.put_secret_value(**param)\n \n         assert secrets_manager_backend.get_variable(\"test_mysql\") is None\n \n@@ -230,14 +195,10 @@ def test_get_config_non_existent_key(self):\n         SystemsManagerParameterStoreBackend.get_config should return None\n         \"\"\"\n         secret_id = \"airflow/config/hello\"\n-        create_param = {\n-            \"Name\": secret_id,\n-        }\n-        param = {\"SecretId\": secret_id, \"SecretString\": \"world\"}\n+        create_param = {\"Name\": secret_id, \"SecretString\": \"world\"}\n \n         secrets_manager_backend = SecretsManagerBackend()\n         secrets_manager_backend.client.create_secret(**create_param)\n-        secrets_manager_backend.client.put_secret_value(**param)\n \n         assert secrets_manager_backend.get_config(\"test\") is None\n \n",
    "generated_commit": "Remove the way of using `Client` as a parameter",
    "label_commit": "Make AWS SecretsManager tests compatible with `moto>=4.2.1` (#34046)",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": 0.09907358884811401,
    "answer_correctness": 0.17880870903335544
  },
  {
    "diff": "diff --git a/celery/tests/test_buckets.py b/celery/tests/test_buckets.py\nindex 4eaf8051013..6764884674a 100644\n--- a/celery/tests/test_buckets.py\n+++ b/celery/tests/test_buckets.py\n@@ -5,10 +5,15 @@\n import unittest\n from itertools import chain, izip\n \n-from celery.worker import buckets\n+from billiard.utils.functional import curry\n+\n+from celery.task.base import Task\n from celery.utils import gen_unique_id\n+from celery.tests.utils import skip_if_environ\n+from celery.worker import buckets\n from celery.registry import TaskRegistry\n-from celery.task.base import Task\n+\n+skip_if_disabled = curry(skip_if_environ(\"SKIP_RLIMITS\"))\n \n \n class MockJob(object):\n@@ -35,10 +40,12 @@ def __repr__(self):\n \n class TestTokenBucketQueue(unittest.TestCase):\n \n+    @skip_if_disabled\n     def empty_queue_yields_QueueEmpty(self):\n         x = buckets.TokenBucketQueue(fill_rate=10)\n         self.assertRaises(buckets.QueueEmpty, x.get)\n \n+    @skip_if_disabled\n     def test_bucket__put_get(self):\n         x = buckets.TokenBucketQueue(fill_rate=10)\n         x.put(\"The quick brown fox\")\n@@ -48,6 +55,7 @@ def test_bucket__put_get(self):\n         time.sleep(0.2)\n         self.assertEquals(x.get_nowait(), \"The lazy dog\")\n \n+    @skip_if_disabled\n     def test_fill_rate(self):\n         x = buckets.TokenBucketQueue(fill_rate=10)\n         # 20 items should take at least one second to complete\n@@ -58,6 +66,7 @@ def test_fill_rate(self):\n             x.wait()\n         self.assertTrue(time.time() - time_start > 1.5)\n \n+    @skip_if_disabled\n     def test_can_consume(self):\n         x = buckets.TokenBucketQueue(fill_rate=1)\n         x.put(\"The quick brown fox\")\n@@ -67,12 +76,14 @@ def test_can_consume(self):\n         x.put(\"The lazy dog\")\n         self.assertRaises(x.RateLimitExceeded, x.get)\n \n+    @skip_if_disabled\n     def test_expected_time(self):\n         x = buckets.TokenBucketQueue(fill_rate=1)\n         x.put_nowait(\"The quick brown fox\")\n         self.assertEqual(x.get_nowait(), \"The quick brown fox\")\n         self.assertTrue(x.expected_time())\n \n+    @skip_if_disabled\n     def test_qsize(self):\n         x = buckets.TokenBucketQueue(fill_rate=1)\n         x.put(\"The quick brown fox\")\n@@ -82,6 +93,7 @@ def test_qsize(self):\n \n class TestRateLimitString(unittest.TestCase):\n \n+    @skip_if_disabled\n     def test_conversion(self):\n         self.assertEquals(buckets.parse_ratelimit_string(999), 999)\n         self.assertEquals(buckets.parse_ratelimit_string(\"1456/s\"), 1456)\n@@ -123,6 +135,7 @@ def setUp(self):\n         for task_cls in self.task_classes:\n             self.registry.register(task_cls)\n \n+    @skip_if_disabled\n     def test_auto_add_on_missing(self):\n         b = buckets.TaskBucket(task_registry=self.registry)\n         for task_cls in self.task_classes:\n@@ -132,6 +145,7 @@ def test_auto_add_on_missing(self):\n         self.assertTrue(TaskD.name in b.buckets.keys())\n         self.registry.unregister(TaskD)\n \n+    @skip_if_disabled\n     def test_has_rate_limits(self):\n         b = buckets.TaskBucket(task_registry=self.registry)\n         self.assertEqual(b.buckets[TaskA.name].fill_rate, 10)\n@@ -144,17 +158,20 @@ def test_has_rate_limits(self):\n         finally:\n             self.registry.unregister(TaskD)\n \n+    @skip_if_disabled\n     def test_on_empty_buckets__get_raises_empty(self):\n         b = buckets.TaskBucket(task_registry=self.registry)\n         self.assertRaises(buckets.QueueEmpty, b.get)\n         self.assertEqual(b.qsize(), 0)\n \n+    @skip_if_disabled\n     def test_put__get(self):\n         b = buckets.TaskBucket(task_registry=self.registry)\n         job = MockJob(gen_unique_id(), TaskA.name, [\"theqbf\"], {\"foo\": \"bar\"})\n         b.put(job)\n         self.assertEquals(b.get(), job)\n \n+    @skip_if_disabled\n     def test_fill_rate(self):\n         b = buckets.TaskBucket(task_registry=self.registry)\n \n@@ -171,6 +188,7 @@ def test_fill_rate(self):\n             self.assertEqual(b.get(), job)\n         self.assertTrue(time.time() - time_start > 1.5)\n \n+    @skip_if_disabled\n     def test__very_busy_queue_doesnt_block_others(self):\n         b = buckets.TaskBucket(task_registry=self.registry)\n \n@@ -187,6 +205,7 @@ def test__very_busy_queue_doesnt_block_others(self):\n \n         self.assertTrue(got_ajobs > 2)\n \n+    @skip_if_disabled\n     def test_thorough__multiple_types(self):\n         self.registry.register(TaskD)\n         try:\ndiff --git a/celery/tests/utils.py b/celery/tests/utils.py\nindex 8154c7a01a0..837088f5591 100644\n--- a/celery/tests/utils.py\n+++ b/celery/tests/utils.py\n@@ -1,19 +1,37 @@\n from __future__ import with_statement\n \n import sys\n+import os\n import __builtin__\n from StringIO import StringIO\n from functools import wraps\n from contextlib import contextmanager\n \n \n+def skip_if_environ(env_var_name):\n+\n+    def _wrap_test(fun):\n+\n+        @wraps(fun)\n+        def _skips_if_environ(*args, **kwargs):\n+            if os.environ.get(env_var_name):\n+                sys.stderr.write(\"SKIP %s: %s set\\n\" % (\n+                    fun.__name__, env_var_name))\n+                return\n+            return fun(*args, **kwargs)\n+\n+        return _skips_if_environ\n+\n+    return _wrap_test\n+\n+\n def _skip_test(reason, sign):\n \n     def _wrap_test(fun):\n \n         @wraps(fun)\n         def _skipped_test(*args, **kwargs):\n-            sys.stderr.write(\"(%s: %s) \" % (sign, reason))\n+            sys.stderr.write(\"%s: %s \" % (sign, reason))\n \n         return _skipped_test\n     return _wrap_test\n",
    "generated_commit": "Improve error handling of errors when called",
    "label_commit": "Skip heavy rate limit unittests if SKIP_RLIMITS is set",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": -0.03730449825525284,
    "answer_correctness": 0.17840551020393064
  },
  {
    "diff": "diff --git a/tests/seta/test_update_job_priority.py b/tests/seta/test_update_job_priority.py\nindex 8b8b08f1426..a6a9eacb160 100644\n--- a/tests/seta/test_update_job_priority.py\n+++ b/tests/seta/test_update_job_priority.py\n@@ -4,7 +4,6 @@\n from mock import patch\n \n from treeherder.seta.models import JobPriority\n-from treeherder.seta.runnable_jobs import RunnableJobsClient\n from treeherder.seta.update_job_priority import (_initialize_values,\n                                                  _sanitize_data,\n                                                  _unique_key,\n@@ -37,9 +36,9 @@ def test_sanitize_data(runnable_jobs_data):\n     assert tc_jobs == 2\n \n \n-@patch.object(RunnableJobsClient, 'query_runnable_jobs')\n-def test_query_sanitized_data(query_runnable_jobs, runnable_jobs_data, sanitized_data):\n-    query_runnable_jobs.return_value = runnable_jobs_data\n+@patch('treeherder.seta.update_job_priority.list_runnable_jobs')\n+def test_query_sanitized_data(list_runnable_jobs, runnable_jobs_data, sanitized_data):\n+    list_runnable_jobs.return_value = runnable_jobs_data\n     data = query_sanitized_data()\n     assert data == sanitized_data\n \ndiff --git a/treeherder/seta/runnable_jobs.py b/treeherder/seta/runnable_jobs.py\ndeleted file mode 100644\nindex 32f597fa3ba..00000000000\n--- a/treeherder/seta/runnable_jobs.py\n+++ /dev/null\n@@ -1,40 +0,0 @@\n-# TODO: Get rid of this module (bug 1330649)\n-import logging\n-\n-from treeherder.etl.runnable_jobs import (list_runnable_jobs,\n-                                          query_latest_gecko_decision_task_id)\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-class RunnableJobsClient(object):\n-    def __init__(self):\n-        self.cache = {}\n-\n-    def query_runnable_jobs(self, repo_name, task_id=None):\n-        '''Return runnable_jobs data\n-\n-        Calling this repo without task_id is optimal as we then only cache the latest runnable jobs.\n-\n-        We do caching because the data does not really change and because bug 1288028 makes calls to\n-        the runnable_jobs API very slow.\n-        '''\n-        if repo_name not in self.cache:\n-            self.cache[repo_name] = {}\n-\n-        if not task_id:\n-            task_id = query_latest_gecko_decision_task_id(repo_name)\n-            self.cache[repo_name]['latest'] = self._query_runnable_jobs(repo_name=repo_name, task_id=task_id)\n-            return self.cache[repo_name]['latest']\n-        else:\n-            if task_id in self.cache:\n-                # XXX: In previous code, we were returning None; what should we do for this case?\n-                logger.info(\"We have already processed the data from this task (%s).\", task_id)\n-                return self.cache[repo_name][task_id]\n-            else:\n-                logger.info(\"We're going to fetch new runnable jobs data.\")\n-                self.cache[repo_name][task_id] = self._query_runnable_jobs(repo_name=repo_name, task_id=task_id)\n-                return self.cache[repo_name][task_id]\n-\n-    def _query_runnable_jobs(self, repo_name, task_id):\n-        return list_runnable_jobs(repo_name, task_id)\ndiff --git a/treeherder/seta/update_job_priority.py b/treeherder/seta/update_job_priority.py\nindex 6ff69be85a0..aee605f1c74 100644\n--- a/treeherder/seta/update_job_priority.py\n+++ b/treeherder/seta/update_job_priority.py\n@@ -12,12 +12,12 @@\n import datetime\n import logging\n \n+from treeherder.etl.runnable_jobs import list_runnable_jobs\n from treeherder.etl.seta import (parse_testtype,\n                                  valid_platform)\n from treeherder.seta.common import (job_priority_index,\n                                     unique_key)\n from treeherder.seta.models import JobPriority\n-from treeherder.seta.runnable_jobs import RunnableJobsClient\n from treeherder.seta.settings import (SETA_HIGH_VALUE_PRIORITY,\n                                       SETA_LOW_VALUE_PRIORITY)\n \n@@ -102,10 +102,7 @@ def _sanitize_data(runnable_jobs_data):\n def query_sanitized_data(repo_name='mozilla-inbound'):\n     \"\"\"Return sanitized jobs data based on runnable api. None if failed to obtain or no new data.\n \n-     We need to find the latest gecko decision task ID (by querying the index [1][2])\n-     in order to know which task ID to pass to the runnable api [3].\n-\n-     It stores the minimal sanitized data from runnable apis under ~/.mozilla/seta/<task_id>.json\n+     We need to find the latest gecko decision task ID (by querying the index [1][2]).\n \n      [1] https://index.taskcluster.net/v1/task/gecko.v2.%s.latest.taskgraph.decision/\n      [2] Index's data structure:\n@@ -116,9 +113,8 @@ def query_sanitized_data(repo_name='mozilla-inbound'):\n         \"data\": {},\n         \"expires\": \"2017-10-06T18:30:18.428Z\"\n       }\n-     [3] https://treeherder.mozilla.org/api/project/mozilla-inbound/runnable_jobs/?decision_task_id=Pp7ZxoH0SKyU6wnhX_Fp0g&format=json\n     \"\"\"\n-    runnable_jobs = RunnableJobsClient().query_runnable_jobs(repo_name)\n+    runnable_jobs = list_runnable_jobs(repo_name)\n     return _sanitize_data(runnable_jobs)\n \n \n",
    "generated_commit": "refactor: remove deps of completed jobs (#4321)",
    "label_commit": "Bug 1513486 - Remove RunnableJobsClient\n\nSince it's never called with the `task_id` parameter, and when the task ID\nis not set, the return value is identical to calling `list_runnable_jobs`\ndirectly.",
    "bleu": 0,
    "rouge_l": 0.1,
    "meteor": 0.036101083032490974,
    "bert_score_f1": 0.034675560891628265,
    "answer_correctness": 0.19042757451903614
  },
  {
    "diff": "diff --git a/celery/exceptions.py b/celery/exceptions.py\nindex 87c1afcd8dd..f4ce8449a44 100644\n--- a/celery/exceptions.py\n+++ b/celery/exceptions.py\n@@ -25,6 +25,10 @@ class SecurityError(Exception):\n     \"\"\"\n \n \n+class Ignore(Exception):\n+    \"\"\"A task can raise this to ignore doing state updates.\"\"\"\n+\n+\n class SystemTerminate(SystemExit):\n     \"\"\"Signals that the worker should terminate.\"\"\"\n \ndiff --git a/celery/states.py b/celery/states.py\nindex a1d787f6e8c..4563fb72f7c 100644\n--- a/celery/states.py\n+++ b/celery/states.py\n@@ -126,6 +126,7 @@ def __le__(self, other):\n FAILURE = 'FAILURE'\n REVOKED = 'REVOKED'\n RETRY = 'RETRY'\n+IGNORED = 'IGNORED'\n \n READY_STATES = frozenset([SUCCESS, FAILURE, REVOKED])\n UNREADY_STATES = frozenset([PENDING, RECEIVED, STARTED, RETRY])\ndiff --git a/celery/task/trace.py b/celery/task/trace.py\nindex 85977abb65f..7263e7c568a 100644\n--- a/celery/task/trace.py\n+++ b/celery/task/trace.py\n@@ -29,7 +29,7 @@\n from celery.app import set_default_app\n from celery.app.task import Task as BaseTask, Context\n from celery.datastructures import ExceptionInfo\n-from celery.exceptions import RetryTaskError\n+from celery.exceptions import Ignore, RetryTaskError\n from celery.utils.serialization import get_pickleable_exception\n from celery.utils.log import get_logger\n \n@@ -43,6 +43,7 @@\n success_receivers = signals.task_success.receivers\n STARTED = states.STARTED\n SUCCESS = states.SUCCESS\n+IGNORED = states.IGNORED\n RETRY = states.RETRY\n FAILURE = states.FAILURE\n EXCEPTION_STATES = states.EXCEPTION_STATES\n@@ -222,6 +223,8 @@ def trace_task(uuid, args, kwargs, request=None):\n                 try:\n                     R = retval = fun(*args, **kwargs)\n                     state = SUCCESS\n+                except Ignore, exc:\n+                    I, R = Info(IGNORED, exc), ExceptionInfo(internal=True)\n                 except RetryTaskError, exc:\n                     I = Info(RETRY, exc)\n                     state, retval = I.state, I.retval\ndiff --git a/celery/worker/job.py b/celery/worker/job.py\nindex 64e6c70c378..9739369cdd4 100644\n--- a/celery/worker/job.py\n+++ b/celery/worker/job.py\n@@ -23,7 +23,7 @@\n from celery import signals\n from celery.app import app_or_default\n from celery.datastructures import ExceptionInfo\n-from celery.exceptions import TaskRevokedError\n+from celery.exceptions import Ignore, TaskRevokedError\n from celery.platforms import signals as _signals\n from celery.task.trace import (\n     trace_task,\n@@ -64,8 +64,9 @@ class Request(object):\n                  'eventer', 'connection_errors',\n                  'task', 'eta', 'expires',\n                  'request_dict', 'acknowledged', 'success_msg',\n-                 'error_msg', 'retry_msg', 'time_start', 'worker_pid',\n-                 '_already_revoked', '_terminate_on_ack', '_tzlocal')\n+                 'error_msg', 'retry_msg', 'ignore_msg',\n+                 'time_start', 'worker_pid', '_already_revoked',\n+                 '_terminate_on_ack', '_tzlocal')\n \n     #: Format string used to log task success.\n     success_msg = \"\"\"\\\n@@ -82,6 +83,10 @@ class Request(object):\n         Task %(name)s[%(id)s] INTERNAL ERROR: %(exc)s\n     \"\"\"\n \n+    ignored_msg = \"\"\"\\\n+        Task %(name)s[%(id)s] ignored\n+    \"\"\"\n+\n     #: Format string used to log task retry.\n     retry_msg = \"\"\"Task %(name)s[%(id)s] retry: %(exc)s\"\"\"\n \n@@ -380,9 +385,15 @@ def _log_error(self, einfo):\n                          traceback=traceback)\n \n         if internal:\n-            format = self.internal_error_msg\n-            description = 'INTERNAL ERROR'\n-            severity = logging.CRITICAL\n+            if isinstance(einfo.exception, Ignore):\n+                format = self.ignored_msg\n+                description = 'ignored'\n+                severity = logging.INFO\n+                exc_info = None\n+            else:\n+                format = self.internal_error_msg\n+                description = 'INTERNAL ERROR'\n+                severity = logging.CRITICAL\n \n         context = {\n             'hostname': self.hostname,\n",
    "generated_commit": "A few more fixes for bug in #731",
    "label_commit": "Tasks can now raise celery.exceptions.Ignore to not update any state",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": -0.01594844087958336,
    "answer_correctness": 0.1783647550571103
  },
  {
    "diff": "diff --git a/mkt/submit/forms.py b/mkt/submit/forms.py\nindex 0000c338e6a..94ce2bcfc30 100644\n--- a/mkt/submit/forms.py\n+++ b/mkt/submit/forms.py\n@@ -186,7 +186,8 @@ def clean(self):\n             origin = pkg.get('origin')\n             if origin:\n                 try:\n-                    origin = verify_app_domain(origin, packaged=True)\n+                    origin = verify_app_domain(origin, packaged=True,\n+                                               exclude=self.addon)\n                 except forms.ValidationError, e:\n                     self._errors['upload'] = self.error_class(e.messages)\n                     return\ndiff --git a/mkt/submit/tests/test_forms.py b/mkt/submit/tests/test_forms.py\nindex e8cb8c0631b..b4018baf0e3 100644\n--- a/mkt/submit/tests/test_forms.py\n+++ b/mkt/submit/tests/test_forms.py\n@@ -146,6 +146,26 @@ def test_verify_app_domain_called(self, _verify):\n         assert form.is_valid(), form.errors\n         assert _verify.called\n \n+    @mock.patch('mkt.submit.forms.parse_addon',\n+                lambda *args: {\"origin\": \"app://hy.fr\"})\n+    def test_verify_app_domain_exclude_same(self):\n+        app = amo.tests.app_factory(app_domain='app://hy.fr')\n+        form = forms.NewWebappVersionForm(\n+            {'upload': self.file.uuid}, request=self.request, is_packaged=True,\n+            addon=app)\n+        assert form.is_valid(), form.errors\n+\n+    @mock.patch('mkt.submit.forms.parse_addon',\n+                lambda *args: {\"origin\": \"app://hy.fr\"})\n+    def test_verify_app_domain_exclude_different(self):\n+        app = amo.tests.app_factory(app_domain='app://yo.lo')\n+        amo.tests.app_factory(app_domain='app://hy.fr')\n+        form = forms.NewWebappVersionForm(\n+            {'upload': self.file.uuid}, request=self.request, is_packaged=True,\n+            addon=app)\n+        assert not form.is_valid(), form.errors\n+        assert 'An app already exists' in ''.join(form.errors['upload'])\n+\n \n class TestAppDetailsBasicForm(amo.tests.TestCase):\n     fixtures = fixture('user_999', 'webapp_337141')\n",
    "generated_commit": "Add missing comma to the error message for invalid user",
    "label_commit": "Ignore current app's app_domain  when uploading new versions (bug 892694)",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": 0.023510344326496124,
    "answer_correctness": 0.16962698154268577
  },
  {
    "diff": "diff --git a/airflow/providers/amazon/aws/hooks/s3.py b/airflow/providers/amazon/aws/hooks/s3.py\nindex 0489fa63128ad..044dc2c9a9eaa 100644\n--- a/airflow/providers/amazon/aws/hooks/s3.py\n+++ b/airflow/providers/amazon/aws/hooks/s3.py\n@@ -212,11 +212,16 @@ def parse_s3_url(s3url: str) -> tuple[str, str]:\n         :param s3url: The S3 Url to parse.\n         :return: the parsed bucket name and key\n         \"\"\"\n+        valid_s3_format = \"S3://bucket-name/key-name\"\n+        valid_s3_virtual_hosted_format = \"https://bucket-name.s3.region-code.amazonaws.com/key-name\"\n         format = s3url.split(\"//\")\n         if re.match(r\"s3[na]?:\", format[0], re.IGNORECASE):\n             parsed_url = urlsplit(s3url)\n             if not parsed_url.netloc:\n-                raise S3HookUriParseFailure(f'Please provide a bucket name using a valid format: \"{s3url}\"')\n+                raise S3HookUriParseFailure(\n+                    \"Please provide a bucket name using a valid format of the form: \"\n+                    + f'{valid_s3_format} or {valid_s3_virtual_hosted_format} but provided: \"{s3url}\"'\n+                )\n \n             bucket_name = parsed_url.netloc\n             key = parsed_url.path.lstrip(\"/\")\n@@ -229,8 +234,16 @@ def parse_s3_url(s3url: str) -> tuple[str, str]:\n             elif temp_split[1] == \"s3\":\n                 bucket_name = temp_split[0]\n                 key = \"/\".join(format[1].split(\"/\")[1:])\n+            else:\n+                raise S3HookUriParseFailure(\n+                    \"Please provide a bucket name using a valid virtually hosted format which should\"\n+                    + f' be of the form: {valid_s3_virtual_hosted_format} but provided: \"{s3url}\"'\n+                )\n         else:\n-            raise S3HookUriParseFailure(f'Please provide a bucket name using a valid format: \"{s3url}\"')\n+            raise S3HookUriParseFailure(\n+                \"Please provide a bucket name using a valid format of the form: \"\n+                + f'{valid_s3_format} or {valid_s3_virtual_hosted_format} but provided: \"{s3url}\"'\n+            )\n         return bucket_name, key\n \n     @staticmethod\ndiff --git a/tests/providers/amazon/aws/hooks/test_s3.py b/tests/providers/amazon/aws/hooks/test_s3.py\nindex 001584ccbacc3..4a42d828da8ba 100644\n--- a/tests/providers/amazon/aws/hooks/test_s3.py\n+++ b/tests/providers/amazon/aws/hooks/test_s3.py\n@@ -34,6 +34,7 @@\n \n from airflow.exceptions import AirflowException\n from airflow.models import Connection\n+from airflow.providers.amazon.aws.exceptions import S3HookUriParseFailure\n from airflow.providers.amazon.aws.hooks.s3 import (\n     S3Hook,\n     provide_bucket_name,\n@@ -94,6 +95,15 @@ def test_parse_s3_url_virtual_hosted_style(self):\n         parsed = S3Hook.parse_s3_url(\"https://DOC-EXAMPLE-BUCKET1.s3.us-west-2.amazonaws.com/test.png\")\n         assert parsed == (\"DOC-EXAMPLE-BUCKET1\", \"test.png\"), \"Incorrect parsing of the s3 url\"\n \n+    def test_parse_invalid_s3_url_virtual_hosted_style(self):\n+        with pytest.raises(\n+            S3HookUriParseFailure,\n+            match=\"Please provide a bucket name using a valid virtually hosted format which should\"\n+            + \" be of the form: https://bucket-name.s3.region-code.amazonaws.com/key-name but \"\n+            + 'provided: \"https://DOC-EXAMPLE-BUCKET1.us-west-2.amazonaws.com/test.png\"',\n+        ):\n+            S3Hook.parse_s3_url(\"https://DOC-EXAMPLE-BUCKET1.us-west-2.amazonaws.com/test.png\")\n+\n     def test_parse_s3_object_directory(self):\n         parsed = S3Hook.parse_s3_url(\"s3://test/this/is/not/a-real-s3-directory/\")\n         assert parsed == (\"test\", \"this/is/not/a-real-s3-directory/\"), \"Incorrect parsing of the s3 url\"\n",
    "generated_commit": "Fix error message for invalid s3 URL (#4332)",
    "label_commit": "Handle `UnboundLocalError` while parsing invalid `s3_url` (#32120)",
    "bleu": 0.02777619034011792,
    "rouge_l": 0.375,
    "meteor": 0.07042253521126761,
    "bert_score_f1": 0.4253712296485901,
    "answer_correctness": 0.22045779553483524
  },
  {
    "diff": "diff --git a/mkt/collections/models.py b/mkt/collections/models.py\nindex ad6efa8d010..271d3042d26 100644\n--- a/mkt/collections/models.py\n+++ b/mkt/collections/models.py\n@@ -5,7 +5,7 @@\n \n import amo.models\n import mkt.regions\n-from addons.models import Category, clean_slug\n+from addons.models import Addon, Category, clean_slug\n from amo.decorators import use_master\n from amo.utils import to_language\n from mkt.webapps.models import Webapp\n@@ -76,7 +76,15 @@ def image_path(self):\n                             'app_collection_%s.png' % (self.pk,))\n \n     def apps(self):\n-        return self._apps.order_by('collectionmembership')\n+        \"\"\"\n+        Public apps on the collection, ordered by their position in the\n+        CollectionMembership model.\n+\n+        Use this method everytime you want to display apps for a collection to\n+        an user.\n+        \"\"\"\n+        return self._apps.filter(disabled_by_user=False,\n+            status=amo.STATUS_PUBLIC).order_by('collectionmembership')\n \n     def add_app(self, app, order=None):\n         \"\"\"\n@@ -159,5 +167,16 @@ class Meta:\n         ordering = ('order',)\n \n \n+def remove_deleted_apps(*args, **kwargs):\n+    instance = kwargs.get('instance')\n+    CollectionMembership.objects.filter(app_id=instance.pk).delete()\n+\n+\n+# Save translations when saving a Collection.\n models.signals.pre_save.connect(save_signal, sender=Collection,\n                                 dispatch_uid='collection_translations')\n+\n+# Delete collection membership when deleting an app (sender needs to be Addon,\n+# not Webapp, because that's the real model underneath).\n+models.signals.post_delete.connect(remove_deleted_apps, sender=Addon,\n+                                   dispatch_uid='apps_collections_cleanup')\ndiff --git a/mkt/collections/serializers.py b/mkt/collections/serializers.py\nindex 30c056f1ee3..72b7452a8a4 100644\n--- a/mkt/collections/serializers.py\n+++ b/mkt/collections/serializers.py\n@@ -34,14 +34,15 @@\n \n class CollectionMembershipField(serializers.RelatedField):\n     \"\"\"\n-    RelatedField subclass that serializes an M2M to CollectionMembership into\n-    a list of apps, rather than a list of CollectionMembership objects.\n+    RelatedField subclass that serializes apps in a Collection, taking into\n+    account feature profile and optionally relying on ElasticSearch to find\n+    the apps instead of making a DB query.\n \n     Specifically created for use with CollectionSerializer; you probably don't\n     want to use this elsewhere.\n     \"\"\"\n     def to_native(self, value):\n-        return AppSerializer(value.app, context=self.context).data\n+        return AppSerializer(value, context=self.context).data\n \n     def field_to_native(self, obj, field_name):\n         if not hasattr(self, 'context') or not 'request' in self.context:\n@@ -56,15 +57,15 @@ def field_to_native(self, obj, field_name):\n             and waffle.switch_is_active('collections-use-es-for-apps')):\n             return self.field_to_native_es(obj, request)\n \n-        value = get_component(obj, self.source)\n+        qs = get_component(obj, self.source)\n \n         # Filter apps based on feature profiles.\n         profile = get_feature_profile(request)\n         if profile:\n-            value = value.filter(**profile.to_kwargs(\n-                prefix='app___current_version__features__has_'))\n+            qs = qs.filter(**profile.to_kwargs(\n+                prefix='_current_version__features__has_'))\n \n-        return [self.to_native(item) for item in value.all()]\n+        return [self.to_native(app) for app in qs]\n \n     def field_to_native_es(self, obj, request):\n         \"\"\"\n@@ -114,8 +115,7 @@ class CollectionSerializer(serializers.ModelSerializer):\n     description = TranslationSerializerField()\n     slug = serializers.CharField(required=False)\n     collection_type = serializers.IntegerField()\n-    apps = CollectionMembershipField(many=True,\n-                                     source='collectionmembership_set')\n+    apps = CollectionMembershipField(many=True, source='apps')\n     image = HyperlinkedRelatedOrNullField(\n         source='*',\n         view_name='collection-image-detail',\ndiff --git a/mkt/collections/tests/test_models.py b/mkt/collections/tests/test_models.py\nindex 786108ead06..4fcfac9a21d 100644\n--- a/mkt/collections/tests/test_models.py\n+++ b/mkt/collections/tests/test_models.py\n@@ -64,6 +64,53 @@ def test_apps(self):\n         eq_(list(CollectionMembership.objects.values_list('order', flat=True)),\n             [0, 1, 2, 3])\n \n+    def test_app_deleted(self):\n+        collection = self.collection\n+        app = amo.tests.app_factory()\n+        collection.add_app(app)\n+        self.assertSetEqual(collection.apps(), [app])\n+        self.assertSetEqual(collection.collectionmembership_set.all(),\n+            [CollectionMembership.objects.get(collection=collection, app=app)])\n+\n+        app.delete()\n+\n+        self.assertSetEqual(collection.apps(), [])\n+        self.assertSetEqual(collection.collectionmembership_set.all(), [])\n+\n+    def test_app_disabled_by_user(self):\n+        collection = self.collection\n+        app = amo.tests.app_factory()\n+        collection.add_app(app)\n+        self.assertSetEqual(collection.apps(), [app])\n+        self.assertSetEqual(collection.collectionmembership_set.all(),\n+            [CollectionMembership.objects.get(collection=collection, app=app)])\n+\n+        app.update(disabled_by_user=True)\n+\n+        self.assertSetEqual(collection.apps(), [])\n+\n+        # The collection membership still exists here, the app is not deleted,\n+        # only disabled.\n+        self.assertSetEqual(collection.collectionmembership_set.all(),\n+            [CollectionMembership.objects.get(collection=collection, app=app)])\n+\n+    def test_app_pending(self):\n+        collection = self.collection\n+        app = amo.tests.app_factory()\n+        collection.add_app(app)\n+        self.assertSetEqual(collection.apps(), [app])\n+        self.assertSetEqual(collection.collectionmembership_set.all(),\n+            [CollectionMembership.objects.get(collection=collection, app=app)])\n+\n+        app.update(status=amo.STATUS_PENDING)\n+\n+        self.assertSetEqual(collection.apps(), [])\n+\n+        # The collection membership still exists here, the app is not deleted,\n+        # just not public.\n+        self.assertSetEqual(collection.collectionmembership_set.all(),\n+            [CollectionMembership.objects.get(collection=collection, app=app)])\n+\n     def test_mixed_ordering(self):\n         self._generate_apps()\n \ndiff --git a/mkt/collections/tests/test_serializers.py b/mkt/collections/tests/test_serializers.py\nindex af0d60bb10a..6efd07a8029 100644\n--- a/mkt/collections/tests/test_serializers.py\n+++ b/mkt/collections/tests/test_serializers.py\n@@ -54,7 +54,7 @@ def test_to_native(self):\n         resource = AppSerializer(self.app)\n         resource.context = {'request': request}\n         self.field.context['request'] = request\n-        native = self.field.to_native(self.membership)\n+        native = self.field.to_native(self.collection.apps()[0])\n         for key, value in native.iteritems():\n             if key == 'resource_uri':\n                 eq_(value, self.app.get_api_url(pk=self.app.pk))\n@@ -64,11 +64,10 @@ def test_to_native(self):\n     def _field_to_native_profile(self, profile='0.0'):\n         request = self.get_request({'pro': profile, 'dev': 'firefoxos'})\n         self.field.parent = self.collection\n-        self.field.source = 'collectionmembership_set'\n+        self.field.source = 'apps'\n         self.field.context['request'] = request\n \n-        return self.field.field_to_native(self.collection,\n-                                          'collectionmembership_set')\n+        return self.field.field_to_native(self.collection, 'apps')\n \n     def test_ordering(self):\n         self.app2 = amo.tests.app_factory()\n@@ -78,6 +77,21 @@ def test_ordering(self):\n         eq_(int(result[0]['id']), self.app2.id)\n         eq_(int(result[1]['id']), self.app.id)\n \n+    def test_app_delete(self):\n+        self.app.delete()\n+        result = self._field_to_native_profile()\n+        eq_(len(result), 0)\n+\n+    def test_app_disable(self):\n+        self.app.update(disabled_by_user=True)\n+        result = self._field_to_native_profile()\n+        eq_(len(result), 0)\n+\n+    def test_app_pending(self):\n+        self.app.update(status=amo.STATUS_PENDING)\n+        result = self._field_to_native_profile()\n+        eq_(len(result), 0)\n+\n     def test_field_to_native_profile(self):\n         result = self._field_to_native_profile(self.profile)\n         eq_(len(result), 1)\n@@ -135,6 +149,24 @@ def test_ordering(self):\n         eq_(int(result[0]['id']), self.app2.id)\n         eq_(int(result[1]['id']), self.app.id)\n \n+    def test_app_delete(self):\n+        self.app.delete()\n+        self.refresh('webapp')\n+        result = self._field_to_native_profile()\n+        eq_(len(result), 0)\n+\n+    def test_app_disable(self):\n+        self.app.update(disabled_by_user=True)\n+        self.refresh('webapp')\n+        result = self._field_to_native_profile()\n+        eq_(len(result), 0)\n+\n+    def test_app_pending(self):\n+        self.app.update(status=amo.STATUS_PENDING)\n+        self.refresh('webapp')\n+        result = self._field_to_native_profile()\n+        eq_(len(result), 0)\n+\n \n class TestCollectionSerializer(CollectionDataMixin, amo.tests.TestCase):\n \ndiff --git a/mkt/collections/views.py b/mkt/collections/views.py\nindex 9f6788da354..517b917cdc4 100644\n--- a/mkt/collections/views.py\n+++ b/mkt/collections/views.py\n@@ -193,18 +193,18 @@ def reorder(self, request, pk=None):\n         \"\"\"\n         Reorder the specified collection.\n         \"\"\"\n-        collection = self.get_object()\n         def membership(app):\n             f = CollectionMembershipField()\n             f.context = {'request': request}\n             return f.to_native(app)\n+\n+        collection = self.get_object()\n         try:\n             collection.reorder(request.DATA)\n         except ValueError:\n             return Response({\n                 'detail': self.exceptions['app_mismatch'],\n-                'apps': [membership(a) for a in\n-                         collection.collectionmembership_set.all()]\n+                'apps': [membership(a) for a in collection.apps()]\n             }, status=status.HTTP_400_BAD_REQUEST, exception=True)\n         return self.return_updated(status.HTTP_200_OK)\n \ndiff --git a/mkt/webapps/models.py b/mkt/webapps/models.py\nindex 155c669261e..8e4c0a16f4d 100644\n--- a/mkt/webapps/models.py\n+++ b/mkt/webapps/models.py\n@@ -1506,8 +1506,11 @@ def extract_document(cls, pk, obj=None):\n         d['app_type'] = obj.app_type_id\n         d['author'] = obj.developer_name\n         d['category'] = list(obj.categories.values_list('slug', flat=True))\n-        d['collection'] = [{'id': cms.collection_id, 'order': cms.order}\n-                           for cms in obj.collectionmembership_set.all()]\n+        if obj.is_public:\n+            d['collection'] = [{'id': cms.collection_id, 'order': cms.order}\n+                               for cms in obj.collectionmembership_set.all()]\n+        else:\n+            d['collection'] = []\n         d['content_ratings'] = (obj.get_content_ratings_by_region(es=True) or\n                                 None)\n         d['content_descriptors'] = obj.get_descriptors(es=True)\n",
    "generated_commit": "Fix app update",
    "label_commit": "Hide non-public apps inside collections (bug 941337)",
    "bleu": 0,
    "rouge_l": 0.18181818181818182,
    "meteor": 0.07575757575757576,
    "bert_score_f1": 0.18211150169372559,
    "answer_correctness": 0.18819564444047943
  },
  {
    "diff": "diff --git a/treeherder/webapp/api/performance_data.py b/treeherder/webapp/api/performance_data.py\nindex 7d05c8e0baf..5574c3ff089 100644\n--- a/treeherder/webapp/api/performance_data.py\n+++ b/treeherder/webapp/api/performance_data.py\n@@ -10,11 +10,13 @@\n from rest_framework.response import Response\n \n from performance_serializers import (PerformanceAlertSerializer,\n-                                     PerformanceAlertSummarySerializer)\n+                                     PerformanceAlertSummarySerializer,\n+                                     PerformanceFrameworkSerializer)\n from treeherder.model import models\n from treeherder.perf.models import (PerformanceAlert,\n                                     PerformanceAlertSummary,\n                                     PerformanceDatum,\n+                                    PerformanceFramework,\n                                     PerformanceSignature)\n from treeherder.webapp.api.permissions import IsStaffOrReadOnly\n \n@@ -50,13 +52,15 @@ def list(self, request, project):\n                 platform__in=platforms)\n \n         ret = {}\n-        for (signature_hash, option_collection_hash, platform, suite, test,\n-             lower_is_better, extra_properties) in signature_data.values_list(\n+        for (signature_hash, option_collection_hash, platform, framework,\n+             suite, test, lower_is_better,\n+             extra_properties) in signature_data.values_list(\n                  'signature_hash',\n                  'option_collection__option_collection_hash',\n-                 'platform__platform', 'suite',\n+                 'platform__platform', 'framework', 'suite',\n                  'test', 'lower_is_better', 'extra_properties').distinct():\n             ret[signature_hash] = {\n+                'framework_id': framework,\n                 'option_collection_hash': option_collection_hash,\n                 'machine_platform': platform,\n                 'suite': suite\n@@ -85,6 +89,13 @@ def list(self, request, project):\n                 'platform__platform', flat=True).distinct())\n \n \n+class PerformanceFrameworkViewSet(viewsets.ReadOnlyModelViewSet):\n+    queryset = PerformanceFramework.objects.all()\n+    serializer_class = PerformanceFrameworkSerializer\n+    filter_backends = [filters.OrderingFilter]\n+    ordering = 'id'\n+\n+\n class PerformanceDatumViewSet(viewsets.ViewSet):\n     \"\"\"\n     This view serves performance test result data\ndiff --git a/treeherder/webapp/api/performance_serializers.py b/treeherder/webapp/api/performance_serializers.py\nindex e0a4f79698e..0b57a6e2804 100644\n--- a/treeherder/webapp/api/performance_serializers.py\n+++ b/treeherder/webapp/api/performance_serializers.py\n@@ -2,9 +2,16 @@\n \n from treeherder.perf.models import (PerformanceAlert,\n                                     PerformanceAlertSummary,\n+                                    PerformanceFramework,\n                                     PerformanceSignature)\n \n \n+class PerformanceFrameworkSerializer(serializers.ModelSerializer):\n+    class Meta:\n+        model = PerformanceFramework\n+        fields = ['id', 'name']\n+\n+\n class TestOptionsSerializer(serializers.JSONField):\n     def to_representation(self, obj):\n         # if extra_propeties is blank, just return nothing\ndiff --git a/treeherder/webapp/api/urls.py b/treeherder/webapp/api/urls.py\nindex dd3ece7c045..2ddd0e5bafe 100644\n--- a/treeherder/webapp/api/urls.py\n+++ b/treeherder/webapp/api/urls.py\n@@ -105,6 +105,9 @@\n default_router.register(r'performance/alert',\n                         performance_data.PerformanceAlertViewSet,\n                         base_name='performance-alerts')\n+default_router.register(r'performance/framework',\n+                        performance_data.PerformanceFrameworkViewSet,\n+                        base_name='performance-frameworks')\n \n urlpatterns = [\n     url(r'^project/(?P<project>[\\w-]{0,50})/',\n",
    "generated_commit": "Add additional tests for GCP products (#241)",
    "label_commit": "Bug 1227622 - Add performance framework API endpoint\n\nAlso add information about performance frameworks to the list of performance\nsignatures we return.",
    "bleu": 0.0038752448326166435,
    "rouge_l": 0.07142857142857142,
    "meteor": 0.024390243902439025,
    "bert_score_f1": 0.15556643903255463,
    "answer_correctness": 0.17937251678158586
  },
  {
    "diff": "diff --git a/airflow/www/utils.py b/airflow/www/utils.py\nindex 8b7357a593586..e9a7993f7e251 100644\n--- a/airflow/www/utils.py\n+++ b/airflow/www/utils.py\n@@ -52,6 +52,13 @@ def limit_sql(sql, limit, conn_type):\n             {sql}\n             ) qry\n             \"\"\".format(**locals())\n+        elif conn_type in ['oracle']:\n+            sql = \"\"\"\\\n+            SELECT * FROM (\n+            {sql}\n+            ) qry\n+            WHERE ROWNUM <= {limit}\n+            \"\"\".format(**locals())\n         else:\n             sql = \"\"\"\\\n             SELECT * FROM (\n",
    "generated_commit": "[AIRFLOW-300] Add support for SQL expressions (#5184)",
    "label_commit": "Update utils.py\n\nadd support for Data Profiling with Oracle SQL",
    "bleu": 0.05035983350735684,
    "rouge_l": 0.4210526315789474,
    "meteor": 0.38659793814432986,
    "bert_score_f1": 0.13323917984962463,
    "answer_correctness": 0.19340658114535642
  },
  {
    "diff": "diff --git a/celery/worker.py b/celery/worker.py\nindex 36c5a49e7fb..1023c95c1bb 100644\n--- a/celery/worker.py\n+++ b/celery/worker.py\n@@ -30,7 +30,7 @@\n \n %%(traceback)s\n \n-%%(EMAIL_SIGNATURE_SEP)s\n+%(EMAIL_SIGNATURE_SEP)s\n Just thought I'd let you know!\n celeryd at %%(hostname)s.\n \"\"\" % {\"EMAIL_SIGNATURE_SEP\": EMAIL_SIGNATURE_SEP}\n",
    "generated_commit": "Fixed typo in celery task missing CELERYCHAR",
    "label_commit": "Fixed worker exception e-mail sending broken due to a bad body string and interpolation.",
    "bleu": 0.012151662434083676,
    "rouge_l": 0.09090909090909091,
    "meteor": 0.03759398496240601,
    "bert_score_f1": 0.028329594060778618,
    "answer_correctness": 0.19061641686408276
  },
  {
    "diff": "diff --git a/mkt/websites/models.py b/mkt/websites/models.py\nindex 1131bb9053d..443b69c3f05 100644\n--- a/mkt/websites/models.py\n+++ b/mkt/websites/models.py\n@@ -101,6 +101,10 @@ def devices(self):\n         # devices.\n         return [device.id for device in DEVICE_TYPE_LIST]\n \n+    @property\n+    def device_names(self):\n+        return [device.api_name for device in DEVICE_TYPE_LIST]\n+\n     def is_dummy_content_for_qa(self):\n         \"\"\"\n         Returns whether this app is a dummy app used for testing only or not.\ndiff --git a/mkt/websites/serializers.py b/mkt/websites/serializers.py\nindex 37125e5b5f8..393e85539e7 100644\n--- a/mkt/websites/serializers.py\n+++ b/mkt/websites/serializers.py\n@@ -11,6 +11,7 @@\n class WebsiteSerializer(serializers.ModelSerializer):\n     categories = ListField(serializers.CharField())\n     description = TranslationSerializerField()\n+    device_types = ListField(serializers.CharField(), source='device_names')\n     id = serializers.IntegerField(source='pk')\n     short_name = TranslationSerializerField()\n     keywords = serializers.SerializerMethodField('get_keywords')\n@@ -20,8 +21,9 @@ class WebsiteSerializer(serializers.ModelSerializer):\n \n     class Meta:\n         model = Website\n-        fields = ['categories', 'description', 'icons', 'id', 'keywords',\n-                  'mobile_url', 'name', 'short_name', 'title', 'url']\n+        fields = ['categories', 'description', 'device_types', 'icons', 'id',\n+                  'keywords', 'mobile_url', 'name', 'short_name', 'title',\n+                  'url']\n \n     def get_icons(self, obj):\n         return dict([(icon_size, obj.get_icon_url(icon_size))\n",
    "generated_commit": "Add device_name to the list of `DeviceType` types",
    "label_commit": "Set device_types in website API (bug 1173048)",
    "bleu": 0,
    "rouge_l": 0.23529411764705882,
    "meteor": 0.0,
    "bert_score_f1": 0.1465774029493332,
    "answer_correctness": 0.20874418531141511
  },
  {
    "diff": "diff --git a/airflow/providers/amazon/aws/example_dags/example_rds.py b/airflow/providers/amazon/aws/example_dags/example_rds.py\nindex 5cf6d85e42646..f30404b9d6867 100644\n--- a/airflow/providers/amazon/aws/example_dags/example_rds.py\n+++ b/airflow/providers/amazon/aws/example_dags/example_rds.py\n@@ -15,10 +15,6 @@\n # KIND, either express or implied.  See the License for the\n # specific language governing permissions and limitations\n # under the License.\n-\"\"\"\n-This is an example dag for using `RedshiftSQLOperator` to authenticate with Amazon Redshift\n-then execute a simple select statement\n-\"\"\"\n \n from datetime import datetime\n \n@@ -81,6 +77,7 @@\n         export_task_identifier='export-auth-db-snap-{{ ds }}',\n         source_arn='arn:aws:rds:<region>:<account number>:snapshot:auth-db-snap',\n         s3_bucket_name='my_s3_bucket',\n+        s3_prefix='some/prefix',\n         iam_role_arn='arn:aws:iam:<region>:<account number>:role/MyRole',\n         kms_key_id='arn:aws:kms:<region>:<account number>:key/*****-****-****-****-********',\n         aws_conn_id='aws_default',\n@@ -105,7 +102,7 @@\n     # [START howto_guide_rds_create_subscription]\n     create_subscription = RdsCreateEventSubscriptionOperator(\n         task_id='create_subscription',\n-        subscription_name='my_topic_subscription',\n+        subscription_name='my-topic-subscription',\n         sns_topic_arn='arn:aws:sns:<region>:<account number>:MyTopic',\n         source_type='db-instance',\n         source_ids=['auth-db'],\n@@ -118,7 +115,7 @@\n     # [START howto_guide_rds_delete_subscription]\n     delete_subscription = RdsDeleteEventSubscriptionOperator(\n         task_id='delete_subscription',\n-        subscription_name='my_topic_subscription',\n+        subscription_name='my-topic-subscription',\n         aws_conn_id='aws_default',\n         hook_params={'region_name': 'us-east-1'},\n     )\n@@ -144,6 +141,7 @@\n     export_sensor = RdsExportTaskExistenceSensor(\n         task_id='export_sensor',\n         export_task_identifier='export-auth-db-snap-{{ ds }}',\n+        target_statuses=['starting', 'in_progress', 'complete', 'canceling', 'canceled'],\n         aws_conn_id='aws_default',\n         hook_params={'region_name': 'us-east-1'},\n     )\ndiff --git a/airflow/providers/amazon/aws/operators/rds.py b/airflow/providers/amazon/aws/operators/rds.py\nindex e14df928dcbad..a527107e80a46 100644\n--- a/airflow/providers/amazon/aws/operators/rds.py\n+++ b/airflow/providers/amazon/aws/operators/rds.py\n@@ -82,14 +82,15 @@ def _await_status(\n             if len(items) > 1:\n                 raise AirflowException(f\"There are {len(items)} {item_type} with identifier {item_name}\")\n \n-            if wait_statuses and items[0]['Status'] in wait_statuses:\n+            if wait_statuses and items[0]['Status'].lower() in wait_statuses:\n+                time.sleep(self._await_interval)\n                 continue\n-            elif ok_statuses and items[0]['Status'] in ok_statuses:\n+            elif ok_statuses and items[0]['Status'].lower() in ok_statuses:\n                 break\n-            elif error_statuses and items[0]['Status'] in error_statuses:\n+            elif error_statuses and items[0]['Status'].lower() in error_statuses:\n                 raise AirflowException(f\"Item has error status ({error_statuses}): {items[0]}\")\n-\n-            time.sleep(self._await_interval)\n+            else:\n+                raise AirflowException(f\"Item has uncertain status: {items[0]}\")\n \n         return None\n \n@@ -118,7 +119,7 @@ class RdsCreateDbSnapshotOperator(RdsBaseOperator):\n         `USER Tagging <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Tagging.html>`__\n     \"\"\"\n \n-    template_fields = (\"db_snapshot_identifier\", \"db_instance_identifier\", \"tags\")\n+    template_fields = (\"db_snapshot_identifier\", \"db_identifier\", \"tags\")\n \n     def __init__(\n         self,\n@@ -257,7 +258,7 @@ def execute(self, context: 'Context') -> str:\n             self._await_status(\n                 'instance_snapshot',\n                 self.target_db_snapshot_identifier,\n-                wait_statuses=['copying'],\n+                wait_statuses=['creating'],\n                 ok_statuses=['available'],\n             )\n         else:\n@@ -392,7 +393,8 @@ def execute(self, context: 'Context') -> str:\n             'export_task',\n             self.export_task_identifier,\n             wait_statuses=['starting', 'in_progress'],\n-            ok_statuses=['available', 'complete'],\n+            ok_statuses=['complete'],\n+            error_statuses=['canceling', 'canceled'],\n         )\n \n         return json.dumps(start_export, default=str)\n@@ -506,7 +508,7 @@ def execute(self, context: 'Context') -> str:\n             'event_subscription',\n             self.subscription_name,\n             wait_statuses=['creating'],\n-            ok_statuses=['created', 'available'],\n+            ok_statuses=['active'],\n         )\n \n         return json.dumps(create_subscription, default=str)\ndiff --git a/airflow/providers/amazon/aws/sensors/rds.py b/airflow/providers/amazon/aws/sensors/rds.py\nindex da57e7030a29a..1c74d5ae8fc14 100644\n--- a/airflow/providers/amazon/aws/sensors/rds.py\n+++ b/airflow/providers/amazon/aws/sensors/rds.py\n@@ -62,7 +62,7 @@ def _check_item(self, item_type: str, item_name: str) -> bool:\n         except ClientError:\n             return False\n         else:\n-            return bool(items) and any(map(lambda s: items[0]['Status'] == s, self.target_statuses))\n+            return bool(items) and any(map(lambda s: items[0]['Status'].lower() == s, self.target_statuses))\n \n \n class RdsSnapshotExistenceSensor(RdsBaseSensor):\n@@ -80,7 +80,7 @@ class RdsSnapshotExistenceSensor(RdsBaseSensor):\n \n     template_fields: Sequence[str] = (\n         'db_snapshot_identifier',\n-        'target_status',\n+        'target_statuses',\n     )\n \n     def __init__(\n@@ -121,7 +121,7 @@ class RdsExportTaskExistenceSensor(RdsBaseSensor):\n \n     template_fields: Sequence[str] = (\n         'export_task_identifier',\n-        'target_status',\n+        'target_statuses',\n     )\n \n     def __init__(\n@@ -135,7 +135,13 @@ def __init__(\n         super().__init__(aws_conn_id=aws_conn_id, **kwargs)\n \n         self.export_task_identifier = export_task_identifier\n-        self.target_statuses = target_statuses or ['available']\n+        self.target_statuses = target_statuses or [\n+            'starting',\n+            'in_progress',\n+            'complete',\n+            'canceling',\n+            'canceled',\n+        ]\n \n     def poke(self, context: 'Context'):\n         self.log.info(\ndiff --git a/setup.py b/setup.py\nindex 2e4f3ef72fd69..667819e1fc521 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -612,7 +612,7 @@ def write_version(filename: str = os.path.join(*[my_dir, \"airflow\", \"git_version\n     'jira',\n     'jsondiff',\n     'mongomock',\n-    'moto>=3.0.7',\n+    'moto>=3.1.0',\n     'parameterized',\n     'paramiko',\n     'pipdeptree',\ndiff --git a/tests/providers/amazon/aws/operators/test_rds.py b/tests/providers/amazon/aws/operators/test_rds.py\nindex 0989736ff7981..d952fbc11a93c 100644\n--- a/tests/providers/amazon/aws/operators/test_rds.py\n+++ b/tests/providers/amazon/aws/operators/test_rds.py\n@@ -35,9 +35,9 @@\n from airflow.utils import timezone\n \n try:\n-    from moto import mock_rds2\n+    from moto import mock_rds\n except ImportError:\n-    mock_rds2 = None\n+    mock_rds = None\n \n \n DEFAULT_DATE = timezone.datetime(2019, 1, 1)\n@@ -165,7 +165,7 @@ def test_await_status_ok(self):\n         )\n \n \n-@pytest.mark.skipif(mock_rds2 is None, reason='mock_rds2 package not present')\n+@pytest.mark.skipif(mock_rds is None, reason='mock_rds package not present')\n class TestRdsCreateDbSnapshotOperator:\n     @classmethod\n     def setup_class(cls):\n@@ -177,7 +177,7 @@ def teardown_class(cls):\n         del cls.dag\n         del cls.hook\n \n-    @mock_rds2\n+    @mock_rds\n     def test_create_db_instance_snapshot(self):\n         _create_db_instance(self.hook)\n         instance_snapshot_operator = RdsCreateDbSnapshotOperator(\n@@ -196,7 +196,7 @@ def test_create_db_instance_snapshot(self):\n         assert instance_snapshots\n         assert len(instance_snapshots) == 1\n \n-    @mock_rds2\n+    @mock_rds\n     def test_create_db_cluster_snapshot(self):\n         _create_db_cluster(self.hook)\n         cluster_snapshot_operator = RdsCreateDbSnapshotOperator(\n@@ -216,7 +216,7 @@ def test_create_db_cluster_snapshot(self):\n         assert len(cluster_snapshots) == 1\n \n \n-@pytest.mark.skipif(mock_rds2 is None, reason='mock_rds2 package not present')\n+@pytest.mark.skipif(mock_rds is None, reason='mock_rds package not present')\n class TestRdsCopyDbSnapshotOperator:\n     @classmethod\n     def setup_class(cls):\n@@ -228,7 +228,7 @@ def teardown_class(cls):\n         del cls.dag\n         del cls.hook\n \n-    @mock_rds2\n+    @mock_rds\n     def test_copy_db_instance_snapshot(self):\n         _create_db_instance(self.hook)\n         _create_db_instance_snapshot(self.hook)\n@@ -248,7 +248,7 @@ def test_copy_db_instance_snapshot(self):\n         assert instance_snapshots\n         assert len(instance_snapshots) == 1\n \n-    @mock_rds2\n+    @mock_rds\n     def test_copy_db_cluster_snapshot(self):\n         _create_db_cluster(self.hook)\n         _create_db_cluster_snapshot(self.hook)\n@@ -271,7 +271,7 @@ def test_copy_db_cluster_snapshot(self):\n         assert len(cluster_snapshots) == 1\n \n \n-@pytest.mark.skipif(mock_rds2 is None, reason='mock_rds2 package not present')\n+@pytest.mark.skipif(mock_rds is None, reason='mock_rds package not present')\n class TestRdsDeleteDbSnapshotOperator:\n     @classmethod\n     def setup_class(cls):\n@@ -283,7 +283,7 @@ def teardown_class(cls):\n         del cls.dag\n         del cls.hook\n \n-    @mock_rds2\n+    @mock_rds\n     def test_delete_db_instance_snapshot(self):\n         _create_db_instance(self.hook)\n         _create_db_instance_snapshot(self.hook)\n@@ -300,7 +300,7 @@ def test_delete_db_instance_snapshot(self):\n         with pytest.raises(self.hook.conn.exceptions.ClientError):\n             self.hook.conn.describe_db_snapshots(DBSnapshotIdentifier=DB_CLUSTER_SNAPSHOT)\n \n-    @mock_rds2\n+    @mock_rds\n     def test_delete_db_cluster_snapshot(self):\n         _create_db_cluster(self.hook)\n         _create_db_cluster_snapshot(self.hook)\n@@ -318,7 +318,7 @@ def test_delete_db_cluster_snapshot(self):\n             self.hook.conn.describe_db_cluster_snapshots(DBClusterSnapshotIdentifier=DB_CLUSTER_SNAPSHOT)\n \n \n-@pytest.mark.skipif(mock_rds2 is None, reason='mock_rds2 package not present')\n+@pytest.mark.skipif(mock_rds is None, reason='mock_rds package not present')\n class TestRdsStartExportTaskOperator:\n     @classmethod\n     def setup_class(cls):\n@@ -330,7 +330,7 @@ def teardown_class(cls):\n         del cls.dag\n         del cls.hook\n \n-    @mock_rds2\n+    @mock_rds\n     def test_start_export_task(self):\n         _create_db_instance(self.hook)\n         _create_db_instance_snapshot(self.hook)\n@@ -352,10 +352,10 @@ def test_start_export_task(self):\n \n         assert export_tasks\n         assert len(export_tasks) == 1\n-        assert export_tasks[0]['Status'] == 'available'\n+        assert export_tasks[0]['Status'] == 'complete'\n \n \n-@pytest.mark.skipif(mock_rds2 is None, reason='mock_rds2 package not present')\n+@pytest.mark.skipif(mock_rds is None, reason='mock_rds package not present')\n class TestRdsCancelExportTaskOperator:\n     @classmethod\n     def setup_class(cls):\n@@ -367,7 +367,7 @@ def teardown_class(cls):\n         del cls.dag\n         del cls.hook\n \n-    @mock_rds2\n+    @mock_rds\n     def test_cancel_export_task(self):\n         _create_db_instance(self.hook)\n         _create_db_instance_snapshot(self.hook)\n@@ -389,7 +389,7 @@ def test_cancel_export_task(self):\n         assert export_tasks[0]['Status'] == 'canceled'\n \n \n-@pytest.mark.skipif(mock_rds2 is None, reason='mock_rds2 package not present')\n+@pytest.mark.skipif(mock_rds is None, reason='mock_rds package not present')\n class TestRdsCreateEventSubscriptionOperator:\n     @classmethod\n     def setup_class(cls):\n@@ -401,7 +401,7 @@ def teardown_class(cls):\n         del cls.dag\n         del cls.hook\n \n-    @mock_rds2\n+    @mock_rds\n     def test_create_event_subscription(self):\n         _create_db_instance(self.hook)\n \n@@ -421,10 +421,10 @@ def test_create_event_subscription(self):\n \n         assert subscriptions\n         assert len(subscriptions) == 1\n-        assert subscriptions[0]['Status'] == 'available'\n+        assert subscriptions[0]['Status'] == 'active'\n \n \n-@pytest.mark.skipif(mock_rds2 is None, reason='mock_rds2 package not present')\n+@pytest.mark.skipif(mock_rds is None, reason='mock_rds package not present')\n class TestRdsDeleteEventSubscriptionOperator:\n     @classmethod\n     def setup_class(cls):\n@@ -436,7 +436,7 @@ def teardown_class(cls):\n         del cls.dag\n         del cls.hook\n \n-    @mock_rds2\n+    @mock_rds\n     def test_delete_event_subscription(self):\n         _create_event_subscription(self.hook)\n \ndiff --git a/tests/providers/amazon/aws/sensors/test_rds.py b/tests/providers/amazon/aws/sensors/test_rds.py\nindex 1dbeeaea85eec..f93b93174eded 100644\n--- a/tests/providers/amazon/aws/sensors/test_rds.py\n+++ b/tests/providers/amazon/aws/sensors/test_rds.py\n@@ -28,9 +28,9 @@\n from airflow.utils import timezone\n \n try:\n-    from moto import mock_rds2\n+    from moto import mock_rds\n except ImportError:\n-    mock_rds2 = None\n+    mock_rds = None\n \n \n DEFAULT_DATE = timezone.datetime(2019, 1, 1)\n@@ -132,7 +132,7 @@ def test_check_item_false(self):\n         assert not self.base_sensor._check_item(item_type='instance_snapshot', item_name='')\n \n \n-@pytest.mark.skipif(mock_rds2 is None, reason='mock_rds2 package not present')\n+@pytest.mark.skipif(mock_rds is None, reason='mock_rds package not present')\n class TestRdsSnapshotExistenceSensor:\n     @classmethod\n     def setup_class(cls):\n@@ -144,7 +144,7 @@ def teardown_class(cls):\n         del cls.dag\n         del cls.hook\n \n-    @mock_rds2\n+    @mock_rds\n     def test_db_instance_snapshot_poke_true(self):\n         _create_db_instance_snapshot(self.hook)\n         op = RdsSnapshotExistenceSensor(\n@@ -156,7 +156,7 @@ def test_db_instance_snapshot_poke_true(self):\n         )\n         assert op.poke(None)\n \n-    @mock_rds2\n+    @mock_rds\n     def test_db_instance_snapshot_poke_false(self):\n         op = RdsSnapshotExistenceSensor(\n             task_id='test_instance_snap_false',\n@@ -167,7 +167,7 @@ def test_db_instance_snapshot_poke_false(self):\n         )\n         assert not op.poke(None)\n \n-    @mock_rds2\n+    @mock_rds\n     def test_db_instance_cluster_poke_true(self):\n         _create_db_cluster_snapshot(self.hook)\n         op = RdsSnapshotExistenceSensor(\n@@ -179,7 +179,7 @@ def test_db_instance_cluster_poke_true(self):\n         )\n         assert op.poke(None)\n \n-    @mock_rds2\n+    @mock_rds\n     def test_db_instance_cluster_poke_false(self):\n         op = RdsSnapshotExistenceSensor(\n             task_id='test_cluster_snap_false',\n@@ -191,7 +191,7 @@ def test_db_instance_cluster_poke_false(self):\n         assert not op.poke(None)\n \n \n-@pytest.mark.skipif(mock_rds2 is None, reason='mock_rds2 package not present')\n+@pytest.mark.skipif(mock_rds is None, reason='mock_rds package not present')\n class TestRdsExportTaskExistenceSensor:\n     @classmethod\n     def setup_class(cls):\n@@ -203,7 +203,7 @@ def teardown_class(cls):\n         del cls.dag\n         del cls.hook\n \n-    @mock_rds2\n+    @mock_rds\n     def test_export_task_poke_true(self):\n         _create_db_instance_snapshot(self.hook)\n         _start_export_task(self.hook)\n@@ -215,7 +215,7 @@ def test_export_task_poke_true(self):\n         )\n         assert op.poke(None)\n \n-    @mock_rds2\n+    @mock_rds\n     def test_export_task_poke_false(self):\n         _create_db_instance_snapshot(self.hook)\n         op = RdsExportTaskExistenceSensor(\n",
    "generated_commit": "fix flake8 tests",
    "label_commit": "AWS RDS integration fixes (#22125)\n\nCo-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>",
    "bleu": 0,
    "rouge_l": 0.125,
    "meteor": 0.05952380952380952,
    "bert_score_f1": -0.05309225246310234,
    "answer_correctness": 0.18321768072963965
  },
  {
    "diff": "diff --git a/cron/healthToMongo.py b/cron/healthToMongo.py\nnew file mode 100644\nindex 000000000..a399aaf65\n--- /dev/null\n+++ b/cron/healthToMongo.py\n@@ -0,0 +1,148 @@\n+#!/usr/bin/env python\n+\n+# This Source Code Form is subject to the terms of the Mozilla Public\n+# License, v. 2.0. If a copy of the MPL was not distributed with this\n+# file, You can obtain one at http://mozilla.org/MPL/2.0/.\n+# Copyright (c) 2014 Mozilla Corporation\n+#\n+# Contributors:\n+# Anthony Verez averez@mozilla.com\n+\n+import json\n+import logging\n+import os\n+import pyes\n+import pytz\n+import requests\n+import sys\n+from datetime import datetime\n+from datetime import timedelta\n+from configlib import getConfig, OptionParser\n+from logging.handlers import SysLogHandler\n+from dateutil.parser import parse\n+from pymongo import MongoClient\n+\n+logger = logging.getLogger(sys.argv[0])\n+\n+\n+def loggerTimeStamp(self, record, datefmt=None):\n+    return toUTC(datetime.now()).isoformat()\n+\n+\n+def initLogger():\n+    logger.level = logging.INFO\n+    formatter = logging.Formatter(\n+        '%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n+    formatter.formatTime = loggerTimeStamp\n+    if options.output == 'syslog':\n+        logger.addHandler(\n+            SysLogHandler(\n+                address=(options.sysloghostname,\n+                    options.syslogport)))\n+    else:\n+        sh = logging.StreamHandler(sys.stderr)\n+        sh.setFormatter(formatter)\n+        logger.addHandler(sh)\n+\n+\n+def toUTC(suspectedDate, localTimeZone=\"US/Pacific\"):\n+    '''make a UTC date out of almost anything'''\n+    utc = pytz.UTC\n+    objDate = None\n+    if type(suspectedDate) == str:\n+        objDate = parse(suspectedDate, fuzzy=True)\n+    elif type(suspectedDate) == datetime:\n+        objDate = suspectedDate\n+\n+    if objDate.tzinfo is None:\n+        objDate = pytz.timezone(localTimeZone).localize(objDate)\n+        objDate = utc.normalize(objDate)\n+    else:\n+        objDate = utc.normalize(objDate)\n+    if objDate is not None:\n+        objDate = utc.normalize(objDate)\n+\n+    return objDate\n+\n+\n+def getFrontendStats(es):\n+    begindateUTC = toUTC(datetime.now() - timedelta(minutes=1))\n+    enddateUTC = toUTC(datetime.now())\n+    qDate = pyes.RangeQuery(qrange=pyes.ESRange('utctimestamp',\n+        from_value=begindateUTC, to_value=enddateUTC))\n+    qType = pyes.TermFilter('_type', 'mozdefhealth')\n+    qMozdef = pyes.TermsFilter('category', ['mozdef'])\n+    pyesresults = es.search(pyes.ConstantScoreQuery(pyes.BoolFilter(\n+        must=[qType, qDate, qMozdef])),\n+        indices='events')\n+    return pyesresults._search_raw()['hits']['hits']\n+\n+\n+def writeFrontendStats(data, mongo):\n+    for host in data:\n+        for key in host['_source']['details'].keys():\n+            # remove unwanted data\n+            if '.' in key:\n+                del host['_source']['details'][key]\n+        mongo.healthfrontend.insert(host['_source'])\n+        # print host['_source']['hostname']\n+        # print host['_source']['details']['loadaverage']\n+        # for key in host['_source']['details'].keys():\n+            # if key not in ('username', 'loadaverage'):\n+                # print key\n+                # print host['_source']['details'][key]['publish_eps']\n+                # print host['_source']['details'][key]['messages_ready']\n+                # print host['_source']['details'][key]['messages_unacknowledged']\n+                # if 'deliver_eps' in host['_source']['details'][key].keys():\n+                    # print host['_source']['details'][key]['deliver_eps']\n+        # print ''\n+\n+\n+def main():\n+    logger.debug('starting')\n+    logger.debug(options)\n+    try:\n+        es = pyes.ES(server=(list('{0}'.format(s) for s in options.esservers)))\n+        client = MongoClient(options.mongohost, options.mongoport)\n+        # use meteor db\n+        mongo = client.meteor\n+        writeFrontendStats(getFrontendStats(es), mongo)\n+    except Exception as e:\n+        logger.error(\"Exception %r sending health to mongo\" % e)\n+\n+\n+def initConfig():\n+    # output our log to stdout or syslog\n+    options.output = getConfig('output', 'stdout', options.configfile)\n+    # syslog hostname\n+    options.sysloghostname = getConfig('sysloghostname', 'localhost',\n+        options.configfile)\n+    # syslog port\n+    options.syslogport = getConfig('syslogport', 514, options.configfile)\n+\n+    options.mqservers = list(getConfig('mqservers', 'localhost',\n+        options.configfile).split(','))\n+    options.mquser = getConfig('mquser', 'guest', options.configfile)\n+    options.mqpassword = getConfig('mqpassword', 'guest', options.configfile)\n+    # port of the rabbitmq json management interface\n+    options.mqapiport = getConfig('mqapiport', 15672, options.configfile)\n+\n+    # elastic search server settings\n+    options.esservers = list(getConfig('esservers', 'http://localhost:9200',\n+        options.configfile).split(','))\n+    options.mongohost = getConfig('mongohost', 'localhost', options.configfile)\n+    options.mongoport = getConfig('mongoport', 3001, options.configfile)\n+\n+\n+if __name__ == '__main__':\n+    parser = OptionParser()\n+    parser.add_option(\n+        \"-c\",\n+        dest='configfile',\n+        default=sys.argv[0].replace('.py', '.conf'),\n+        help=\"configuration file to use\")\n+    (options, args) = parser.parse_args()\n+    initConfig()\n+    initLogger()\n+    main()\n+\n",
    "generated_commit": "Add support for certain documentation",
    "label_commit": "averez-56-healthinfo: store frontend stats in mongo",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": -0.096074178814888,
    "answer_correctness": 0.17884987078306358
  },
  {
    "diff": "diff --git a/ichnaea/data/observation.py b/ichnaea/data/observation.py\nindex 88ca3ecb9..e3fda0fac 100644\n--- a/ichnaea/data/observation.py\n+++ b/ichnaea/data/observation.py\n@@ -66,7 +66,7 @@ def insert(self, entries, userid=None):\n         for key, observations in station_observations.items():\n             first_blacklisted = None\n             incomplete = False\n-            station = self.station_model.querykey(self.session, key).first()\n+            station = self.station_model.getkey(self.session, key)\n \n             if station is None:\n                 # Drop observations for blacklisted stations.\ndiff --git a/ichnaea/data/tasks.py b/ichnaea/data/tasks.py\nindex f31320989..650d1abfb 100644\n--- a/ichnaea/data/tasks.py\n+++ b/ichnaea/data/tasks.py\n@@ -35,7 +35,7 @@ def insert_measures(self, items=None, nickname='', email='',\n \n     reports = kombu_loads(items)\n     with self.db_session() as session:\n-        api_key = api_key_text and session.query(ApiKey).filter(ApiKey.valid_key==api_key_text).first()\n+        api_key = api_key_text and ApiKey.getkey(session, api_key_text)\n \n         queue = ReportQueueV1(self, session,\n                               api_key,\ndiff --git a/ichnaea/models/content.py b/ichnaea/models/content.py\nindex fa629de0a..ba84a333e 100644\n--- a/ichnaea/models/content.py\n+++ b/ichnaea/models/content.py\n@@ -73,7 +73,7 @@ class Score(IdMixin, HashKeyMixin, _Model):\n \n     @classmethod\n     def incr(cls, session, key, value):\n-        score = cls.querykey(session, key).first()\n+        score = cls.getkey(session, key)\n         if score is not None:\n             score.value += int(value)\n         else:\ndiff --git a/ichnaea/monitor/tasks.py b/ichnaea/monitor/tasks.py\nindex e5a11c46c..d980fc899 100644\n--- a/ichnaea/monitor/tasks.py\n+++ b/ichnaea/monitor/tasks.py\n@@ -1,4 +1,5 @@\n from sqlalchemy import func\n+from sqlalchemy.orm import load_only\n \n from ichnaea.async.app import celery_app\n from ichnaea.async.task import DatabaseTask\n@@ -15,14 +16,10 @@\n def monitor_api_key_limits(self):\n     result = {}\n     try:\n-        redis_client = self.app.redis_client\n-        stats_client = self.stats_client\n-        now = util.utcnow()\n-        today = now.strftime(\"%Y%m%d\")\n-\n-        keys = redis_client.keys('apilimit:*:' + today)\n+        today = util.utcnow().strftime(\"%Y%m%d\")\n+        keys = self.redis_client.keys('apilimit:*:' + today)\n         if keys:\n-            values = redis_client.mget(keys)\n+            values = self.redis_client.mget(keys)\n             keys = [k.split(':')[1] for k in keys]\n         else:\n             values = []\n@@ -30,18 +27,17 @@ def monitor_api_key_limits(self):\n         names = {}\n         if keys:\n             with self.db_session() as session:\n-                q = session.query(ApiKey.valid_key, ApiKey.shortname).filter(\n-                    ApiKey.valid_key.in_(keys))\n-                names = dict(q.all())\n+                query = (ApiKey.querykeys(session, keys)\n+                               .options(load_only('valid_key', 'shortname')))\n+                for api_key in query.all():\n+                    names[api_key.valid_key] = api_key.name\n \n         result = {}\n         for k, v in zip(keys, values):\n-            name = names.get(k)\n-            if not name:\n-                name = k\n+            name = names.get(k, k)\n             value = int(v)\n             result[name] = value\n-            stats_client.gauge('apilimit.' + name, value)\n+            self.stats_client.gauge('apilimit.' + name, value)\n     except Exception:  # pragma: no cover\n         # Log but ignore the exception\n         self.raven_client.captureException()\n",
    "generated_commit": "Fix bug in sample data collection",
    "label_commit": "Use the new getkey helper in more places.\n\nAlso use ApiKey.querykeys inside the monitoring task.",
    "bleu": 0.009109250643152446,
    "rouge_l": 0.09090909090909091,
    "meteor": 0.03546099290780142,
    "bert_score_f1": 0.13865527510643005,
    "answer_correctness": 0.18216647710103462
  },
  {
    "diff": "diff --git a/airflow/contrib/hooks/qubole_hook.py b/airflow/contrib/hooks/qubole_hook.py\nindex 833c1c7ccf4b4..f3bcc202ed7f1 100755\n--- a/airflow/contrib/hooks/qubole_hook.py\n+++ b/airflow/contrib/hooks/qubole_hook.py\n@@ -202,9 +202,6 @@ def create_cmd_args(self, context):\n         args.append(\"--tags={0}\".format(','.join(filter(None,tags))))\n \n         if inplace_args is not None:\n-            if cmd_type == 'hadoopcmd':\n-                args += inplace_args.split(' ', 1)\n-            else:\n-                args += inplace_args.split(' ')\n+            args += inplace_args.split(' ')\n \n         return args\ndiff --git a/tests/contrib/operators/test_qubole_operator.py b/tests/contrib/operators/test_qubole_operator.py\nindex 0e6e13dc2f020..9b5bf3a7f4be8 100644\n--- a/tests/contrib/operators/test_qubole_operator.py\n+++ b/tests/contrib/operators/test_qubole_operator.py\n@@ -89,6 +89,19 @@ def test_position_args_parameters(self):\n         self.assertEqual(task.get_hook().create_cmd_args({'run_id':'dummy'})[2],\n                          \"key2=value2\")\n \n-\n+        task = QuboleOperator(task_id=TASK_ID, command_type='hadoopcmd',\n+                          sub_command=\"s3distcp --src s3n://airflow/source_hadoopcmd \" +\n+                                      \"--dest s3n://airflow/destination_hadoopcmd\", dag=dag)\n+\n+        self.assertEqual(task.get_hook().create_cmd_args({'run_id': 'dummy'})[1],\n+                         \"s3distcp\")\n+        self.assertEqual(task.get_hook().create_cmd_args({'run_id': 'dummy'})[2],\n+                         \"--src\")\n+        self.assertEqual(task.get_hook().create_cmd_args({'run_id': 'dummy'})[3],\n+                         \"s3n://airflow/source_hadoopcmd\")\n+        self.assertEqual(task.get_hook().create_cmd_args({'run_id': 'dummy'})[4],\n+                         \"--dest\")\n+        self.assertEqual(task.get_hook().create_cmd_args({'run_id': 'dummy'})[5],\n+                         \"s3n://airflow/destination_hadoopcmd\")\n \n \n",
    "generated_commit": "[AIRFLOW-3297] Fix bug in qubole hook (#4836)",
    "label_commit": "[AIRFLOW-1657] Handle failing qubole operator\n\nCloses #2643 from rupesh92/AIRFLOW-1657",
    "bleu": 0.024822529802838878,
    "rouge_l": 0.2,
    "meteor": 0.05681818181818182,
    "bert_score_f1": 0.32112276554107666,
    "answer_correctness": 0.2135322361236499
  },
  {
    "diff": "diff --git a/kitsune/products/migrations/0010_auto_20240624_0616.py b/kitsune/products/migrations/0010_auto_20240624_0616.py\nindex 3815d1dd4ad..0ff39555914 100644\n--- a/kitsune/products/migrations/0010_auto_20240624_0616.py\n+++ b/kitsune/products/migrations/0010_auto_20240624_0616.py\n@@ -11,7 +11,7 @@\n # If it exists, the articles will be moved to the existing topic for each product.\n # If the topics is created, a redirect will be added to the old topic.\n TOPICS_TO_MIGRATE = {\n-    \"install-and-update:\": [\n+    \"install-and-update\": [\n         \"install-and-update-firefox\",\n         \"install-and-update-firefox-ios\",\n         \"download-and-install\",\n",
    "generated_commit": "nan",
    "label_commit": "fix typo in data migration",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": 0.06947685033082962,
    "answer_correctness": 0.18191476610484472
  },
  {
    "diff": "diff --git a/docker/conf/config.py b/docker/conf/config.py\nindex 2ca8788d6..bcde59953 100644\n--- a/docker/conf/config.py\n+++ b/docker/conf/config.py\n@@ -17,6 +17,10 @@\n     'unauth_ssh.AlertUnauthSSH': {'schedule': crontab(minute='*/1')},\n }\n \n+ALERT_PLUGINS = [\n+    # 'relative pythonfile name (exclude the .py) - EX: sso_dashboard',\n+]\n+\n RABBITMQ = {\n     'mqserver': 'localhost',\n     'mquser': 'guest',\n",
    "generated_commit": "Fixing small typo in config file (#2732)",
    "label_commit": "Add alert plugins whitelist to docker alert config",
    "bleu": 0.028634401465295497,
    "rouge_l": 0.13333333333333333,
    "meteor": 0.06329113924050632,
    "bert_score_f1": 0.1293349713087082,
    "answer_correctness": 0.17840257890310746
  },
  {
    "diff": "diff --git a/apps/search/tests/test_views.py b/apps/search/tests/test_views.py\nindex 5451fca6cdc..1d0a3ffd673 100644\n--- a/apps/search/tests/test_views.py\n+++ b/apps/search/tests/test_views.py\n@@ -977,6 +977,28 @@ def test_results_other_applications(self):\n         eq_(self.get_results(r), [sm_collection.id])\n \n \n+    def test_session_version_sidebar(self):\n+        request = RequestFactory()\n+        request.session = {}\n+        request.APP = amo.FIREFOX\n+\n+        request.get(reverse('search.search'))\n+        facets = {\n+            u'platforms': [{u'count': 58, u'term': 1}],\n+            u'appversions': [{u'count': 58, u'term': 5000000200100}],\n+            u'categories': [{u'count': 55, u'term': 1}],\n+            u'tags': [],\n+        }\n+        versions = version_sidebar(request, {}, facets)\n+        assert not versions[1].selected\n+\n+        versions = version_sidebar(request, {'appver': '5.0'}, facets)\n+        assert versions[1].selected\n+\n+        versions = version_sidebar(request, {}, facets)\n+        assert versions[1].selected\n+\n+\n def test_search_redirects():\n     changes = (\n         ('q=yeah&sort=newest', 'q=yeah&sort=updated'),\ndiff --git a/apps/search/views.py b/apps/search/views.py\nindex 8f659b53156..2f214f626ba 100644\n--- a/apps/search/views.py\n+++ b/apps/search/views.py\n@@ -533,7 +533,9 @@ def category_sidebar(request, query, facets):\n \n \n def version_sidebar(request, query, facets):\n-    appver = query.get('appver')\n+    appver = query.get('appver') or request.session.get('search.appver')\n+    if query.get('appver'):\n+        request.session['search.appver'] = appver\n     app = unicode(request.APP.pretty)\n     exclude_versions = getattr(request.APP, 'exclude_versions', [])\n     # L10n: {0} is an application, such as Firefox. This means \"any version of\n",
    "generated_commit": "fix: add the correct repo version check in the API (#314)",
    "label_commit": "memorize last choice in addon firefox version (bug 729414)",
    "bleu": 0.022416933501922302,
    "rouge_l": 0.09999999999999999,
    "meteor": 0.10869565217391303,
    "bert_score_f1": 0.11983385682106018,
    "answer_correctness": 0.18379382762052643
  },
  {
    "diff": "diff --git a/api/views/__init__.py b/api/views/__init__.py\nindex a48bea98a7..5ac09eabcc 100644\n--- a/api/views/__init__.py\n+++ b/api/views/__init__.py\n@@ -24,7 +24,7 @@\n from allauth.socialaccount.providers.fxa.provider import FirefoxAccountsProvider  # type: ignore\n from allauth.socialaccount.providers.fxa.views import FirefoxAccountsOAuth2Adapter\n from django_filters import rest_framework as filters\n-from waffle import get_waffle_flag_model\n+from waffle import flag_is_active, get_waffle_flag_model\n from waffle.models import Switch, Sample\n from rest_framework import (\n     decorators,\n@@ -38,6 +38,7 @@\n \n from privaterelay.utils import (\n     get_countries_info_from_request_and_mapping,\n+    get_premium_country_language_mapping,\n )\n \n from emails.models import (\n@@ -242,6 +243,8 @@ def runtime_data(request):\n     switch_values = [(s.name, s.is_active()) for s in switches]\n     samples = Sample.get_all()\n     sample_values = [(s.name, s.is_active()) for s in samples]\n+    eu_country_expansion = flag_is_active(request, \"eu_country_expansion\")\n+    premium_mapping = get_premium_country_language_mapping(eu_country_expansion)\n     return response.Response(\n         {\n             \"FXA_ORIGIN\": settings.FXA_BASE_ORIGIN,\n@@ -250,7 +253,7 @@ def runtime_data(request):\n             \"BUNDLE_PRODUCT_ID\": settings.BUNDLE_PROD_ID,\n             \"PHONE_PRODUCT_ID\": settings.PHONE_PROD_ID,\n             \"PERIODICAL_PREMIUM_PLANS\": get_countries_info_from_request_and_mapping(\n-                request, settings.PERIODICAL_PREMIUM_PLAN_COUNTRY_LANG_MAPPING\n+                request, premium_mapping\n             ),\n             \"PHONE_PLANS\": get_countries_info_from_request_and_mapping(\n                 request, settings.PHONE_PLAN_COUNTRY_LANG_MAPPING\ndiff --git a/emails/models.py b/emails/models.py\nindex 0193af790e..a9bfca9240 100644\n--- a/emails/models.py\n+++ b/emails/models.py\n@@ -22,7 +22,7 @@\n from rest_framework.authtoken.models import Token\n \n from api.exceptions import ErrorContextType, RelayAPIException\n-from privaterelay.utils import flag_is_active_in_task\n+from privaterelay.utils import flag_is_active_in_task, get_premium_countries\n \n \n emails_config = apps.get_app_config(\"emails\")\n@@ -172,11 +172,14 @@ def language(self):\n     def fxa_locale_in_premium_country(self):\n         if self.fxa and self.fxa.extra_data.get(\"locale\"):\n             accept_langs = parse_accept_lang_header(self.fxa.extra_data.get(\"locale\"))\n+            eu_country_expansion = flag_is_active_in_task(\n+                \"eu_country_expansion\", self.user\n+            )\n+            premium_countries = get_premium_countries(eu_country_expansion)\n             if (\n                 len(accept_langs) >= 1\n                 and len(accept_langs[0][0].split(\"-\")) >= 2\n-                and accept_langs[0][0].split(\"-\")[1]\n-                in settings.PERIODICAL_PREMIUM_PLAN_COUNTRY_LANG_MAPPING.keys()\n+                and accept_langs[0][0].split(\"-\")[1] in premium_countries\n             ):\n                 return True\n             # If a language but no country is known, check if there's a country\n@@ -186,8 +189,7 @@ def fxa_locale_in_premium_country(self):\n             if (\n                 len(accept_langs) >= 1\n                 and len(accept_langs[0][0].split(\"-\")) == 1\n-                and accept_langs[0][0].split(\"-\")[0]\n-                in settings.PERIODICAL_PREMIUM_PLAN_COUNTRY_LANG_MAPPING.keys()\n+                and accept_langs[0][0].split(\"-\")[0] in premium_countries\n             ):\n                 return True\n         return False\ndiff --git a/emails/tests/models_tests.py b/emails/tests/models_tests.py\nindex 46bf730c47..3f93568792 100644\n--- a/emails/tests/models_tests.py\n+++ b/emails/tests/models_tests.py\n@@ -13,6 +13,7 @@\n )\n \n from allauth.socialaccount.models import SocialAccount\n+from waffle.testutils import override_flag\n import pytest\n \n from model_bakery import baker\n@@ -830,6 +831,26 @@ def test_locale_in_premium_country_returns_False_if_premium_not_available_in_cou\n     def test_locale_in_premium_country_returns_False_if_no_fxa_account(self):\n         assert self.profile.fxa_locale_in_premium_country is False\n \n+    @override_flag(\"eu_country_expansion\", active=True)\n+    def test_locale_in_premium_country_with_eu_expansion_flag(self):\n+        baker.make(\n+            SocialAccount,\n+            user=self.profile.user,\n+            provider=\"fxa\",\n+            extra_data={\"locale\": \"et-ee,et;q=0.8\"},\n+        )\n+        assert self.profile.fxa_locale_in_premium_country is True\n+\n+    @override_flag(\"eu_country_expansion\", active=False)\n+    def test_locale_in_premium_country_without_eu_expansion_flag(self):\n+        baker.make(\n+            SocialAccount,\n+            user=self.profile.user,\n+            provider=\"fxa\",\n+            extra_data={\"locale\": \"et-ee,et;q=0.8\"},\n+        )\n+        assert self.profile.fxa_locale_in_premium_country is False\n+\n     def test_user_joined_before_premium_release_returns_True(self):\n         user = baker.make(\n             User, date_joined=datetime.fromisoformat(\"2021-10-18 17:00:00+00:00\")\ndiff --git a/privaterelay/settings.py b/privaterelay/settings.py\nindex c216fdb0c2..bcf7e5a718 100644\n--- a/privaterelay/settings.py\n+++ b/privaterelay/settings.py\n@@ -507,6 +507,129 @@ def _get_initial_middleware() -> list[str]:\n                 \"currency\": \"EUR\",\n             },\n         },\n+        \"si\": {\n+            \"monthly\": {\n+                \"id\": \"price_1NHALmJNcmPzuWtR2nIoAzEt\",\n+                \"price\": 1.99,\n+                \"currency\": \"EUR\",\n+            },\n+            \"yearly\": {\n+                \"id\": \"price_1NHAL9JNcmPzuWtRSZ3BWQs0\",\n+                \"price\": 0.99,\n+                \"currency\": \"EUR\",\n+            },\n+        },\n+        \"sk\": {\n+            \"monthly\": {\n+                \"id\": \"price_1NHAJsJNcmPzuWtR71WX0Pz9\",\n+                \"price\": 1.99,\n+                \"currency\": \"EUR\",\n+            },\n+            \"yearly\": {\n+                \"id\": \"price_1NHAKYJNcmPzuWtRtETl30gb\",\n+                \"price\": 0.99,\n+                \"currency\": \"EUR\",\n+            },\n+        },\n+        \"pt\": {\n+            \"monthly\": {\n+                \"id\": \"price_1NHAI1JNcmPzuWtRx8jXjkrQ\",\n+                \"price\": 1.99,\n+                \"currency\": \"EUR\",\n+            },\n+            \"yearly\": {\n+                \"id\": \"price_1NHAHWJNcmPzuWtRCRMnWyvK\",\n+                \"price\": 0.99,\n+                \"currency\": \"EUR\",\n+            },\n+        },\n+        \"lu\": {\n+            \"monthly\": {\n+                \"id\": \"price_1NHAFZJNcmPzuWtRm5A7w5qJ\",\n+                \"price\": 1.99,\n+                \"currency\": \"EUR\",\n+            },\n+            \"yearly\": {\n+                \"id\": \"price_1NHAF8JNcmPzuWtRG1FiPK0N\",\n+                \"price\": 0.99,\n+                \"currency\": \"EUR\",\n+            },\n+        },\n+        \"lt\": {\n+            \"monthly\": {\n+                \"id\": \"price_1NHACcJNcmPzuWtR5ZJeVtJA\",\n+                \"price\": 1.99,\n+                \"currency\": \"EUR\",\n+            },\n+            \"yearly\": {\n+                \"id\": \"price_1NHADOJNcmPzuWtR2PSMBMLr\",\n+                \"price\": 0.99,\n+                \"currency\": \"EUR\",\n+            },\n+        },\n+        \"lv\": {\n+            \"monthly\": {\n+                \"id\": \"price_1NHAASJNcmPzuWtRpcliwx0R\",\n+                \"price\": 1.99,\n+                \"currency\": \"EUR\",\n+            },\n+            \"yearly\": {\n+                \"id\": \"price_1NHA9lJNcmPzuWtRLf7DV6GA\",\n+                \"price\": 0.99,\n+                \"currency\": \"EUR\",\n+            },\n+        },\n+        \"gr\": {\n+            \"monthly\": {\n+                \"id\": \"price_1NHA5CJNcmPzuWtR1JSmxqFA\",\n+                \"price\": 1.99,\n+                \"currency\": \"EUR\",\n+            },\n+            \"yearly\": {\n+                \"id\": \"price_1NHA4lJNcmPzuWtRniS23IuE\",\n+                \"price\": 0.99,\n+                \"currency\": \"EUR\",\n+            },\n+        },\n+        \"ee\": {\n+            \"monthly\": {\n+                \"id\": \"price_1NHA1tJNcmPzuWtRvSeyiVYH\",\n+                \"price\": 1.99,\n+                \"currency\": \"EUR\",\n+            },\n+            \"yearly\": {\n+                \"id\": \"price_1NHA2TJNcmPzuWtR10yknZHf\",\n+                \"price\": 0.99,\n+                \"currency\": \"EUR\",\n+            },\n+        },\n+        \"mt\": {\n+            \"monthly\": {\n+                \"id\": \"price_1NH9yxJNcmPzuWtRChanpIQU\",\n+                \"price\": 1.99,\n+                \"currency\": \"EUR\",\n+            },\n+            \"yearly\": {\n+                \"id\": \"price_1NH9y3JNcmPzuWtRIJkQos9q\",\n+                \"price\": 0.99,\n+                \"currency\": \"EUR\",\n+            },\n+        },\n+        # TODO: clarify this entry\n+        # \"cy\" the language code means Welsh\n+        # \"cy\" means \"Cyprus\" in our usage, which is probably Greek or Turkish\n+        \"cy\": {\n+            \"monthly\": {\n+                \"id\": \"price_1NH9saJNcmPzuWtRpffF5I59\",\n+                \"price\": 1.99,\n+                \"currency\": \"EUR\",\n+            },\n+            \"yearly\": {\n+                \"id\": \"price_1NH9rKJNcmPzuWtRzDiXCeEG\",\n+                \"price\": 0.99,\n+                \"currency\": \"EUR\",\n+            },\n+        },\n     },\n     \"usd\": {\n         \"en\": {\n@@ -535,6 +658,7 @@ def _get_initial_middleware() -> list[str]:\n         },\n     },\n }\n+\n PERIODICAL_PREMIUM_PLAN_COUNTRY_LANG_MAPPING = {\n     # Austria\n     \"at\": {\n@@ -584,25 +708,74 @@ def _get_initial_middleware() -> list[str]:\n     \"fi\": {\n         \"fi\": PERIODICAL_PREMIUM_PLAN_ID_MATRIX[\"euro\"][\"fi\"],\n     },\n+    # United States\n     \"us\": {\n         \"en\": PERIODICAL_PREMIUM_PLAN_ID_MATRIX[\"usd\"][\"en\"],\n     },\n+    # United Kingdom\n     \"gb\": {\n         \"en\": PERIODICAL_PREMIUM_PLAN_ID_MATRIX[\"usd\"][\"gb\"],\n     },\n+    # Canada\n     \"ca\": {\n         \"en\": PERIODICAL_PREMIUM_PLAN_ID_MATRIX[\"usd\"][\"en\"],\n     },\n+    # New Zealand\n     \"nz\": {\n         \"en\": PERIODICAL_PREMIUM_PLAN_ID_MATRIX[\"usd\"][\"gb\"],\n     },\n+    # Malaysia\n     \"my\": {\n         \"en\": PERIODICAL_PREMIUM_PLAN_ID_MATRIX[\"usd\"][\"gb\"],\n     },\n+    # Singapore\n     \"sg\": {\n         \"en\": PERIODICAL_PREMIUM_PLAN_ID_MATRIX[\"usd\"][\"gb\"],\n     },\n }\n+EU_EXPANSION_PREMIUM_PLAN_COUNTRY_LANG_MAPPING = {\n+    # Cyprus\n+    \"cy\": {\n+        # TODO: Welsh (cy) seems wrong. Maybe el (greek) and tr (turkish)?\n+        \"en\": PERIODICAL_PREMIUM_PLAN_ID_MATRIX[\"euro\"][\"cy\"],\n+    },\n+    # Estonia\n+    \"ee\": {\n+        \"et\": PERIODICAL_PREMIUM_PLAN_ID_MATRIX[\"euro\"][\"ee\"],\n+    },\n+    # Greece\n+    \"gr\": {\n+        \"el\": PERIODICAL_PREMIUM_PLAN_ID_MATRIX[\"euro\"][\"gr\"],\n+    },\n+    # Latvia\n+    \"lv\": {\n+        \"lv\": PERIODICAL_PREMIUM_PLAN_ID_MATRIX[\"euro\"][\"lv\"],\n+    },\n+    # Lithuania\n+    \"lt\": {\n+        \"lt\": PERIODICAL_PREMIUM_PLAN_ID_MATRIX[\"euro\"][\"lt\"],\n+    },\n+    # Luxembourg\n+    \"lu\": {\n+        \"en\": PERIODICAL_PREMIUM_PLAN_ID_MATRIX[\"euro\"][\"lu\"],\n+    },\n+    # Malta\n+    \"mt\": {\n+        \"en\": PERIODICAL_PREMIUM_PLAN_ID_MATRIX[\"euro\"][\"mt\"],\n+    },\n+    # Portugal\n+    \"pt\": {\n+        \"pt\": PERIODICAL_PREMIUM_PLAN_ID_MATRIX[\"euro\"][\"pt\"],\n+    },\n+    # Slovakia\n+    \"sk\": {\n+        \"sk\": PERIODICAL_PREMIUM_PLAN_ID_MATRIX[\"euro\"][\"sk\"],\n+    },\n+    # Slovenia\n+    \"si\": {\n+        \"sl\": PERIODICAL_PREMIUM_PLAN_ID_MATRIX[\"euro\"][\"si\"],\n+    },\n+}\n \n PHONE_PLAN_ID_MATRIX = {\n     \"usd\": {\ndiff --git a/privaterelay/tests/utils_tests.py b/privaterelay/tests/utils_tests.py\nindex 073224f001..c0d28c52f0 100644\n--- a/privaterelay/tests/utils_tests.py\n+++ b/privaterelay/tests/utils_tests.py\n@@ -17,27 +17,60 @@\n from ..utils import (\n     flag_is_active_in_task,\n     get_premium_country_lang,\n+    get_premium_country_language_mapping,\n )\n \n \n class GetPremiumCountryLangTest(TestCase):\n+    def setUp(self):\n+        self.mapping = get_premium_country_language_mapping(None)\n+\n     def test_get_premium_country_lang(self):\n-        cc, lang = get_premium_country_lang(\"en-au,\")\n+        cc, lang = get_premium_country_lang(\"en-au,\", self.mapping)\n         assert cc == \"au\"\n         assert lang == \"en\"\n \n-        cc, lang = get_premium_country_lang(\"en-us,\")\n+        cc, lang = get_premium_country_lang(\"en-us,\", self.mapping)\n         assert cc == \"us\"\n         assert lang == \"en\"\n \n-        cc, lang = get_premium_country_lang(\"de-be,\")\n+        cc, lang = get_premium_country_lang(\"de-be,\", self.mapping)\n         assert cc == \"be\"\n         assert lang == \"de\"\n \n-        cc, lang = get_premium_country_lang(\"de-be,\", \"at\")\n+        cc, lang = get_premium_country_lang(\"de-be,\", self.mapping, \"at\")\n         assert cc == \"at\"\n         assert lang == \"de\"\n \n+    def test_en_fallback(self) -> None:\n+        cc, lang = get_premium_country_lang(\"en,\", self.mapping)\n+        assert cc == \"us\"\n+        assert lang == \"en\"\n+\n+    def test_first_lang_fallback_two_parts(self) -> None:\n+        accept_lang = \"sgn-us,\"  # American Sign Language\n+        cc, lang = get_premium_country_lang(accept_lang, self.mapping)\n+        assert cc == \"us\"\n+        assert lang == \"en\"\n+\n+    def test_first_lang_fallback_three_parts(self) -> None:\n+        accept_lang = \"sgn-ch-de,\"  # Swiss German Sign Language\n+        cc, lang = get_premium_country_lang(accept_lang, self.mapping)\n+        assert cc == \"ch\"\n+        assert lang == \"fr\"\n+\n+    def test_eu_country_expansion_active(self) -> None:\n+        mapping = get_premium_country_language_mapping(eu_country_expansion=True)\n+        cc, lang = get_premium_country_lang(\"et-ee\", mapping)\n+        assert cc == \"ee\"\n+        assert lang == \"et\"\n+\n+    def test_eu_country_expansion_inactive(self) -> None:\n+        mapping = get_premium_country_language_mapping(eu_country_expansion=False)\n+        cc, lang = get_premium_country_lang(\"et-ee\", mapping)\n+        assert cc == \"ee\"\n+        assert lang == \"en\"\n+\n \n #\n # flag_is_active_in_task tests\ndiff --git a/privaterelay/utils.py b/privaterelay/utils.py\nindex 0032b35afc..1f3cf42453 100644\n--- a/privaterelay/utils.py\n+++ b/privaterelay/utils.py\n@@ -16,7 +16,7 @@\n \n \n def get_countries_info_from_request_and_mapping(request, mapping):\n-    country_code = _get_cc_from_request(request)\n+    country_code = _get_cc_from_request(request, mapping)\n     countries = mapping.keys()\n     available_in_country = country_code in countries\n     return {\n@@ -27,35 +27,48 @@ def get_countries_info_from_request_and_mapping(request, mapping):\n     }\n \n \n-def _get_cc_from_request(request):\n+def _get_cc_from_request(request, premium_mapping):\n     if \"X-Client-Region\" in request.headers:\n         return request.headers[\"X-Client-Region\"].lower()\n     if \"Accept-Language\" in request.headers:\n-        return get_premium_country_lang(request.headers[\"Accept-Language\"])[0]\n+        return get_premium_country_lang(\n+            request.headers[\"Accept-Language\"], premium_mapping\n+        )[0]\n     if settings.DEBUG:\n         return \"us\"\n     return \"us\"\n \n \n-def get_premium_country_lang(accept_lang, cc=None):\n+def get_premium_country_lang(accept_lang, mapping, cc=None):\n     lang = accept_lang.split(\",\")[0]\n     lang_parts = lang.split(\"-\") if lang and \"-\" in lang else [lang]\n     lang = lang_parts[0].lower()\n     if cc is None:\n-        cc = lang_parts[1] if len(lang_parts) == 2 else lang_parts[0]\n+        cc = lang_parts[1] if len(lang_parts) >= 2 else lang_parts[0]\n         cc = cc.lower()\n         # if the language was just \"en\", default to US\n         if cc == \"en\":\n             cc = \"us\"\n \n-    if cc in settings.PERIODICAL_PREMIUM_PLAN_COUNTRY_LANG_MAPPING.keys():\n-        languages = settings.PERIODICAL_PREMIUM_PLAN_COUNTRY_LANG_MAPPING[cc]\n+    if languages := mapping.get(cc):\n         if lang in languages.keys():\n             return cc, lang\n         return cc, list(languages.keys())[0]\n     return cc, \"en\"\n \n \n+def get_premium_country_language_mapping(eu_country_expansion):\n+    mapping = settings.PERIODICAL_PREMIUM_PLAN_COUNTRY_LANG_MAPPING.copy()\n+    if eu_country_expansion:\n+        mapping.update(settings.EU_EXPANSION_PREMIUM_PLAN_COUNTRY_LANG_MAPPING)\n+    return mapping\n+\n+\n+def get_premium_countries(eu_country_expansion):\n+    mapping = get_premium_country_language_mapping(eu_country_expansion)\n+    return set(mapping.keys())\n+\n+\n def enable_or_404(\n     check_function: Callable[[], bool],\n     message: str = \"This conditional view is disabled.\",\n",
    "generated_commit": "Fix bug in vio code snip point of incorrect type",
    "label_commit": "MPP-2085 - Change premium plan map ID config to include new countries.",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": 0.014470105059444904,
    "answer_correctness": 0.17559394166627817
  },
  {
    "diff": "diff --git a/pontoon/base/views.py b/pontoon/base/views.py\nindex f5a87ffd39..ae79c36438 100755\n--- a/pontoon/base/views.py\n+++ b/pontoon/base/views.py\n@@ -19,7 +19,7 @@\n from django.contrib.auth.models import User\n from django.core.mail import send_mail\n from django.core.urlresolvers import reverse\n-from django.db.models import Count\n+from django.db.models import Count, F\n \n from django.http import (\n     Http404,\n@@ -313,7 +313,12 @@ def contributor(request, email, template='user.html'):\n     except User.DoesNotExist:\n         raise Http404\n \n-    translations = Translation.objects.filter(user=user)\n+    translations = (\n+        Translation.objects.filter(user=user)\n+        .exclude(string=F('entity__string'))\n+        .exclude(string=F('entity__string_plural'))\n+    )\n+\n     current = translations.exclude(entity__obsolete=True) \\\n         .extra({'day': \"date(date)\"})\n \n@@ -357,7 +362,11 @@ def contributors(request, template='users.html'):\n         .order_by('-translation_count')[:100]\n \n     for user in users:\n-        user.translations = Translation.objects.filter(user=user)\n+        user.translations = (\n+            Translation.objects.filter(user=user)\n+            .exclude(string=F('entity__string'))\n+            .exclude(string=F('entity__string_plural'))\n+        )\n         user.gravatar_url = get_gravatar_url(user.email, 44)\n \n     data = {\n",
    "generated_commit": "Fixing issue #233 by adding non-ascii characters to the list of licenses",
    "label_commit": "Exclude unchanged translations from stats",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": 0.014102110639214516,
    "answer_correctness": 0.18072328463287612
  },
  {
    "diff": "diff --git a/airflow/providers/google/cloud/example_dags/example_cloud_memorystore.py b/airflow/providers/google/cloud/example_dags/example_cloud_memorystore.py\nindex 4c50e482955e3..55768ca9f9722 100644\n--- a/airflow/providers/google/cloud/example_dags/example_cloud_memorystore.py\n+++ b/airflow/providers/google/cloud/example_dags/example_cloud_memorystore.py\n@@ -88,7 +88,11 @@\n \n     # [START howto_operator_get_instance]\n     get_instance = CloudMemorystoreGetInstanceOperator(\n-        task_id=\"get-instance\", location=\"europe-north1\", instance=INSTANCE_NAME, project_id=GCP_PROJECT_ID\n+        task_id=\"get-instance\",\n+        location=\"europe-north1\",\n+        instance=INSTANCE_NAME,\n+        project_id=GCP_PROJECT_ID,\n+        do_xcom_push=True,\n     )\n     # [END howto_operator_get_instance]\n \n@@ -212,6 +216,7 @@\n     create_instance_2 >> import_instance\n     create_instance >> list_instances >> list_instances_result\n     list_instances >> delete_instance\n+    export_instance >> update_instance\n     update_instance >> delete_instance\n     get_instance >> set_acl_permission >> export_instance\n     export_instance >> import_instance\n",
    "generated_commit": "[AIRFLOW-XXX] Fix flaky Google Cloud StorageDownload (#4275)",
    "label_commit": "Improve example DAGs for Cloud Memorystore (#7855)",
    "bleu": 0.033031643180138064,
    "rouge_l": 0.13333333333333333,
    "meteor": 0.07142857142857142,
    "bert_score_f1": 0.20733432471752167,
    "answer_correctness": 0.19748400955081477
  },
  {
    "diff": "diff --git a/airflow/operators/local_to_gcs.py b/airflow/operators/local_to_gcs.py\nindex 417a4beb22d4a..6170e104c51bb 100644\n--- a/airflow/operators/local_to_gcs.py\n+++ b/airflow/operators/local_to_gcs.py\n@@ -33,7 +33,8 @@ class FileToGoogleCloudStorageOperator(BaseOperator):\n \n     :param src: Path to the local file. (templated)\n     :type src: str\n-    :param dst: Destination path within the specified bucket. (templated)\n+    :param dst: Destination path within the specified bucket, it must be the full file path\n+        to destination object on GCS, including GCS object (ex. `path/to/file.txt`) (templated)\n     :type dst: str\n     :param bucket: The bucket to upload to. (templated)\n     :type bucket: str\n",
    "generated_commit": "[AIRFLOW-XXX] Fix docstring in gcs operator documentation (#5048)",
    "label_commit": "[AIRFLOW-5384] Improve dst param info in FileToGCSOperator (#5985)\n\nThis commit add more info about dst parameter to indicate that the path\nmust include file name.",
    "bleu": 0.0033173928594616674,
    "rouge_l": 0.1142857142857143,
    "meteor": 0.02145922746781116,
    "bert_score_f1": 0.3391551971435547,
    "answer_correctness": 0.3964888432373284
  },
  {
    "diff": "diff --git a/airflow/models.py b/airflow/models.py\nindex 11260a791d252..98b10d1f7fab4 100644\n--- a/airflow/models.py\n+++ b/airflow/models.py\n@@ -813,7 +813,13 @@ class derived from this one results in the creation of a task object,\n     :param depends_on_past: when set to true, task instances will run\n         sequentially while relying on the previous task's schedule to\n         succeed. The task instance for the start_date is allowed to run.\n-    :type depends_on_past: boolean\n+    :type depends_on_past: bool\n+    :param wait_for_downstream: when set to true, the task instances\n+        of task X will wait for the dependencies of the previous instance\n+        of task X to finish before it moves on the to next schedule.\n+        This is useful if the different instances of a task X alter\n+        the same asset, and this asset is used by the dependencies of task X.\n+    :type wait_for_downstream: bool\n     :param dag: a reference to the dag the task is attached to (if any)\n     :type dag: DAG\n     \"\"\"\n",
    "generated_commit": "[AIRFLOW-XXX] Add wait_dagrun param to BaseOperator docstring (#3927)",
    "label_commit": "Documented BaseOperator's wait_for_downstream attribute",
    "bleu": 0,
    "rouge_l": 0.11764705882352941,
    "meteor": 0.0,
    "bert_score_f1": 0.1768338531255722,
    "answer_correctness": 0.1981882460676715
  },
  {
    "diff": "diff --git a/airflow/contrib/hooks/gcp_kms_hook.py b/airflow/contrib/hooks/gcp_kms_hook.py\nnew file mode 100644\nindex 0000000000000..6f2b3aedffdbf\n--- /dev/null\n+++ b/airflow/contrib/hooks/gcp_kms_hook.py\n@@ -0,0 +1,108 @@\n+# -*- coding: utf-8 -*-\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+#\n+\n+import base64\n+\n+from airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook\n+\n+from apiclient.discovery import build\n+\n+\n+def _b64encode(s):\n+    \"\"\" Base 64 encodes a bytes object to a string \"\"\"\n+    return base64.b64encode(s).decode('ascii')\n+\n+\n+def _b64decode(s):\n+    \"\"\" Base 64 decodes a string to bytes. \"\"\"\n+    return base64.b64decode(s.encode('utf-8'))\n+\n+\n+class GoogleCloudKMSHook(GoogleCloudBaseHook):\n+    \"\"\"\n+    Interact with Google Cloud KMS. This hook uses the Google Cloud Platform\n+    connection.\n+    \"\"\"\n+\n+    def __init__(self, gcp_conn_id='google_cloud_default', delegate_to=None):\n+        super(GoogleCloudKMSHook, self).__init__(gcp_conn_id, delegate_to=delegate_to)\n+\n+    def get_conn(self):\n+        \"\"\"\n+        Returns a KMS service object.\n+\n+        :rtype: apiclient.discovery.Resource\n+        \"\"\"\n+        http_authorized = self._authorize()\n+        return build(\n+            'cloudkms', 'v1', http=http_authorized, cache_discovery=False)\n+\n+    def encrypt(self, key_name, plaintext, authenticated_data=None):\n+        \"\"\"\n+        Encrypts a plaintext message using Google Cloud KMS.\n+\n+        :param key_name: The Resource Name for the key (or key version)\n+                         to be used for encyption. Of the form\n+                         ``projects/*/locations/*/keyRings/*/cryptoKeys/**``\n+        :type key_name: str\n+        :param plaintext: The message to be encrypted.\n+        :type plaintext: bytes\n+        :param authenticated_data: Optional additional authenticated data that\n+                                   must also be provided to decrypt the message.\n+        :type authenticated_data: bytes\n+        :return: The base 64 encoded ciphertext of the original message.\n+        :rtype: str\n+        \"\"\"\n+        keys = self.get_conn().projects().locations().keyRings().cryptoKeys()\n+        body = {'plaintext': _b64encode(plaintext)}\n+        if authenticated_data:\n+            body['additionalAuthenticatedData'] = _b64encode(authenticated_data)\n+\n+        request = keys.encrypt(name=key_name, body=body)\n+        response = request.execute()\n+\n+        ciphertext = response['ciphertext']\n+        return ciphertext\n+\n+    def decrypt(self, key_name, ciphertext, authenticated_data=None):\n+        \"\"\"\n+        Decrypts a ciphertext message using Google Cloud KMS.\n+\n+        :param key_name: The Resource Name for the key to be used for decyption.\n+                         Of the form ``projects/*/locations/*/keyRings/*/cryptoKeys/**``\n+        :type key_name: str\n+        :param ciphertext: The message to be decrypted.\n+        :type ciphertext: str\n+        :param authenticated_data: Any additional authenticated data that was\n+                                   provided when encrypting the message.\n+        :type authenticated_data: bytes\n+        :return: The original message.\n+        :rtype: bytes\n+        \"\"\"\n+        keys = self.get_conn().projects().locations().keyRings().cryptoKeys()\n+        body = {'ciphertext': ciphertext}\n+        if authenticated_data:\n+            body['additionalAuthenticatedData'] = _b64encode(authenticated_data)\n+\n+        request = keys.decrypt(name=key_name, body=body)\n+        response = request.execute()\n+\n+        plaintext = _b64decode(response['plaintext'])\n+        return plaintext\ndiff --git a/tests/contrib/hooks/test_gcp_kms_hook.py b/tests/contrib/hooks/test_gcp_kms_hook.py\nnew file mode 100644\nindex 0000000000000..eabf20e5642b2\n--- /dev/null\n+++ b/tests/contrib/hooks/test_gcp_kms_hook.py\n@@ -0,0 +1,160 @@\n+# -*- coding: utf-8 -*-\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+#\n+\n+from __future__ import unicode_literals\n+\n+import unittest\n+from base64 import b64encode\n+\n+from airflow.contrib.hooks.gcp_kms_hook import GoogleCloudKMSHook\n+\n+try:\n+    from unittest import mock\n+except ImportError:\n+    try:\n+        import mock\n+    except ImportError:\n+        mock = None\n+\n+BASE_STRING = 'airflow.contrib.hooks.gcp_api_base_hook.{}'\n+KMS_STRING = 'airflow.contrib.hooks.gcp_kms_hook.{}'\n+\n+\n+TEST_PROJECT = 'test-project'\n+TEST_LOCATION = 'global'\n+TEST_KEY_RING = 'test-key-ring'\n+TEST_KEY = 'test-key'\n+TEST_KEY_ID = 'projects/{}/locations/{}/keyRings/{}/cryptoKeys/{}'.format(\n+    TEST_PROJECT, TEST_LOCATION, TEST_KEY_RING, TEST_KEY)\n+\n+\n+def mock_init(self, gcp_conn_id, delegate_to=None):\n+    pass\n+\n+\n+class GoogleCloudKMSHookTest(unittest.TestCase):\n+    def setUp(self):\n+        with mock.patch(BASE_STRING.format('GoogleCloudBaseHook.__init__'),\n+                        new=mock_init):\n+            self.kms_hook = GoogleCloudKMSHook(gcp_conn_id='test')\n+\n+    @mock.patch(KMS_STRING.format('GoogleCloudKMSHook.get_conn'))\n+    def test_encrypt(self, mock_service):\n+        plaintext = b'Test plaintext'\n+        ciphertext = 'Test ciphertext'\n+        plaintext_b64 = b64encode(plaintext).decode('ascii')\n+        body = {'plaintext': plaintext_b64}\n+        response = {'ciphertext': ciphertext}\n+\n+        encrypt_method = (mock_service.return_value\n+                          .projects.return_value\n+                          .locations.return_value\n+                          .keyRings.return_value\n+                          .cryptoKeys.return_value\n+                          .encrypt)\n+        execute_method = encrypt_method.return_value.execute\n+        execute_method.return_value = response\n+\n+        ret_val = self.kms_hook.encrypt(TEST_KEY_ID, plaintext)\n+        encrypt_method.assert_called_with(name=TEST_KEY_ID,\n+                                          body=body)\n+        execute_method.assert_called_with()\n+        self.assertEqual(ciphertext, ret_val)\n+\n+    @mock.patch(KMS_STRING.format('GoogleCloudKMSHook.get_conn'))\n+    def test_encrypt_authdata(self, mock_service):\n+        plaintext = b'Test plaintext'\n+        auth_data = b'Test authdata'\n+        ciphertext = 'Test ciphertext'\n+        plaintext_b64 = b64encode(plaintext).decode('ascii')\n+        auth_data_b64 = b64encode(auth_data).decode('ascii')\n+        body = {\n+            'plaintext': plaintext_b64,\n+            'additionalAuthenticatedData': auth_data_b64\n+        }\n+        response = {'ciphertext': ciphertext}\n+\n+        encrypt_method = (mock_service.return_value\n+                          .projects.return_value\n+                          .locations.return_value\n+                          .keyRings.return_value\n+                          .cryptoKeys.return_value\n+                          .encrypt)\n+        execute_method = encrypt_method.return_value.execute\n+        execute_method.return_value = response\n+\n+        ret_val = self.kms_hook.encrypt(TEST_KEY_ID, plaintext,\n+                                        authenticated_data=auth_data)\n+        encrypt_method.assert_called_with(name=TEST_KEY_ID,\n+                                          body=body)\n+        execute_method.assert_called_with()\n+        self.assertEqual(ciphertext, ret_val)\n+\n+    @mock.patch(KMS_STRING.format('GoogleCloudKMSHook.get_conn'))\n+    def test_decrypt(self, mock_service):\n+        plaintext = b'Test plaintext'\n+        ciphertext = 'Test ciphertext'\n+        plaintext_b64 = b64encode(plaintext).decode('ascii')\n+        body = {'ciphertext': ciphertext}\n+        response = {'plaintext': plaintext_b64}\n+\n+        decrypt_method = (mock_service.return_value\n+                          .projects.return_value\n+                          .locations.return_value\n+                          .keyRings.return_value\n+                          .cryptoKeys.return_value\n+                          .decrypt)\n+        execute_method = decrypt_method.return_value.execute\n+        execute_method.return_value = response\n+\n+        ret_val = self.kms_hook.decrypt(TEST_KEY_ID, ciphertext)\n+        decrypt_method.assert_called_with(name=TEST_KEY_ID,\n+                                          body=body)\n+        execute_method.assert_called_with()\n+        self.assertEqual(plaintext, ret_val)\n+\n+    @mock.patch(KMS_STRING.format('GoogleCloudKMSHook.get_conn'))\n+    def test_decrypt_authdata(self, mock_service):\n+        plaintext = b'Test plaintext'\n+        auth_data = b'Test authdata'\n+        ciphertext = 'Test ciphertext'\n+        plaintext_b64 = b64encode(plaintext).decode('ascii')\n+        auth_data_b64 = b64encode(auth_data).decode('ascii')\n+        body = {\n+            'ciphertext': ciphertext,\n+            'additionalAuthenticatedData': auth_data_b64\n+        }\n+        response = {'plaintext': plaintext_b64}\n+\n+        decrypt_method = (mock_service.return_value\n+                          .projects.return_value\n+                          .locations.return_value\n+                          .keyRings.return_value\n+                          .cryptoKeys.return_value\n+                          .decrypt)\n+        execute_method = decrypt_method.return_value.execute\n+        execute_method.return_value = response\n+\n+        ret_val = self.kms_hook.decrypt(TEST_KEY_ID, ciphertext,\n+                                        authenticated_data=auth_data)\n+        decrypt_method.assert_called_with(name=TEST_KEY_ID,\n+                                          body=body)\n+        execute_method.assert_called_with()\n+        self.assertEqual(plaintext, ret_val)\n",
    "generated_commit": "Avoid triggering of incorrect credit in get_conn",
    "label_commit": "[AIRFLOW-2826] Add GoogleCloudKMSHook (#3677)\n\nAdds a hook enabling encryption and decryption through Google Cloud KMS.\nThis should also contribute to AIRFLOW-2062.",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": -0.14105747640132904,
    "answer_correctness": 0.16309982006670962
  },
  {
    "diff": "diff --git a/emails/management/commands/send_welcome_emails.py b/emails/management/commands/send_welcome_emails.py\nindex 1227a410cc..c82f6a49f9 100644\n--- a/emails/management/commands/send_welcome_emails.py\n+++ b/emails/management/commands/send_welcome_emails.py\n@@ -7,6 +7,7 @@\n from django.core.management.base import BaseCommand\n \n from allauth.socialaccount.models import SocialAccount\n+import django_ftl\n \n from emails.apps import EmailsConfig\n from emails.models import Profile\n@@ -43,6 +44,8 @@ def send_welcome_email(profile: Profile, **kwargs):\n     ses_client = app_config.ses_client\n     assert ses_client\n     assert settings.RELAY_FROM_ADDRESS\n+    with django_ftl.override(profile.language):\n+        translated_subject = ftl_bundle.format(\"first-time-user-email-welcome\")\n     try:\n         ses_client.send_email(\n             Destination={\n@@ -50,9 +53,7 @@ def send_welcome_email(profile: Profile, **kwargs):\n             },\n             Source=settings.RELAY_FROM_ADDRESS,\n             Message={\n-                \"Subject\": _ses_message_props(\n-                    ftl_bundle.format(\"first-time-user-email-welcome\")\n-                ),\n+                \"Subject\": _ses_message_props(translated_subject),\n                 \"Body\": {\n                     \"Html\": _ses_message_props(get_welcome_email(user, \"html\")),\n                     \"Text\": _ses_message_props(get_welcome_email(user, \"txt\")),\ndiff --git a/emails/tests/mgmt_send_welcome_emails.py b/emails/tests/mgmt_send_welcome_emails.py\nindex 8b2caad6b5..01af0beb99 100644\n--- a/emails/tests/mgmt_send_welcome_emails.py\n+++ b/emails/tests/mgmt_send_welcome_emails.py\n@@ -7,7 +7,7 @@\n from django.core.management import call_command\n \n from allauth.socialaccount.models import SocialAccount\n-from django_ftl import activate\n+import django_ftl\n \n from emails.models import Profile\n from emails.tests.models_tests import make_free_test_user\n@@ -38,14 +38,18 @@ def test_no_profiles_need_welcome_email(caplog: pytest.LogCaptureFixture):\n def test_no_locale_defaults_to_en(\n     mock_ses_client: MagicMock, caplog: pytest.LogCaptureFixture\n ):\n+    ftl_bundle.reload()\n     user = _make_user_who_needs_welcome_email_with_locale(\"\")\n \n     call_command(COMMAND_NAME)\n     _assert_caplog_for_1_email_to_user(user, caplog)\n \n-    to_addresses, source, body_html = _get_send_email_args(mock_ses_client)\n+    to_addresses, source, subject, body_html = _get_send_email_args(mock_ses_client)\n     assert to_addresses == [user.email]\n     assert source == settings.RELAY_FROM_ADDRESS\n+    with django_ftl.override(\"en\"):\n+        expected_subject = ftl_bundle.format(\"first-time-user-email-welcome\")\n+    assert subject == expected_subject\n     assert 'lang=\"en\"' in body_html\n \n \n@@ -56,15 +60,18 @@ def test_send_welcome_emails(\n ):\n     ftl_bundle.reload()\n     user = _make_user_who_needs_welcome_email_with_locale(locale)\n-    activate(user.profile.language)\n \n     call_command(COMMAND_NAME)\n     _assert_caplog_for_1_email_to_user(user, caplog)\n \n-    to_addresses, source, body_html = _get_send_email_args(mock_ses_client)\n+    to_addresses, source, subject, body_html = _get_send_email_args(mock_ses_client)\n     assert to_addresses == [user.email]\n     assert source == settings.RELAY_FROM_ADDRESS\n-    assert ftl_bundle.format(\"first-time-user-email-hero-cta\") in body_html\n+    with django_ftl.override(user.profile.language):\n+        expected_subject = ftl_bundle.format(\"first-time-user-email-welcome\")\n+        expected_cta = ftl_bundle.format(\"first-time-user-email-hero-cta\")\n+    assert subject == expected_subject\n+    assert expected_cta in body_html\n \n \n def _make_user_who_needs_welcome_email_with_locale(locale: str = \"\") -> User:\n@@ -96,5 +103,6 @@ def _get_send_email_args(mock_ses_client: MagicMock) -> Tuple:\n     call_args = mock_ses_client.send_email.call_args[1]\n     to_addresses = call_args[\"Destination\"][\"ToAddresses\"]\n     source = call_args[\"Source\"]\n+    subject = call_args[\"Message\"][\"Subject\"][\"Data\"]\n     body_html = call_args[\"Message\"][\"Body\"][\"Html\"][\"Data\"]\n-    return to_addresses, source, body_html\n+    return to_addresses, source, subject, body_html\n",
    "generated_commit": "Fix code scanning alert",
    "label_commit": "MPP-3257: activate the user's language to translate subject",
    "bleu": 0,
    "rouge_l": 0.0,
    "meteor": 0.0,
    "bert_score_f1": 0.03670023754239082,
    "answer_correctness": 0.1843893934710515
  },
  {
    "diff": "diff --git a/airflow/providers/amazon/aws/hooks/glue.py b/airflow/providers/amazon/aws/hooks/glue.py\nindex 1d4c87d6767b1..baf6780e07802 100644\n--- a/airflow/providers/amazon/aws/hooks/glue.py\n+++ b/airflow/providers/amazon/aws/hooks/glue.py\n@@ -44,7 +44,8 @@ class GlueJobHook(AwsBaseHook):\n     :param retry_limit: Maximum number of times to retry this job if it fails\n     :param num_of_dpus: Number of AWS Glue DPUs to allocate to this Job\n     :param region_name: aws region name (example: us-east-1)\n-    :param iam_role_name: AWS IAM Role for Glue Job Execution\n+    :param iam_role_name: AWS IAM Role for Glue Job Execution. If set `iam_role_arn` must equal None.\n+    :param iam_role_arn: AWS IAM Role ARN for Glue Job Execution, If set `iam_role_name` must equal None.\n     :param create_job_kwargs: Extra arguments for Glue Job Creation\n     :param update_config: Update job configuration on Glue (default: False)\n \n@@ -72,6 +73,7 @@ def __init__(\n         retry_limit: int = 0,\n         num_of_dpus: int | float | None = None,\n         iam_role_name: str | None = None,\n+        iam_role_arn: str | None = None,\n         create_job_kwargs: dict | None = None,\n         update_config: bool = False,\n         job_poll_interval: int | float = 6,\n@@ -85,6 +87,7 @@ def __init__(\n         self.retry_limit = retry_limit\n         self.s3_bucket = s3_bucket\n         self.role_name = iam_role_name\n+        self.role_arn = iam_role_arn\n         self.s3_glue_logs = \"logs/glue-logs/\"\n         self.create_job_kwargs = create_job_kwargs or {}\n         self.update_config = update_config\n@@ -93,6 +96,8 @@ def __init__(\n         worker_type_exists = \"WorkerType\" in self.create_job_kwargs\n         num_workers_exists = \"NumberOfWorkers\" in self.create_job_kwargs\n \n+        if self.role_arn and self.role_name:\n+            raise ValueError(\"Cannot set iam_role_arn and iam_role_name simultaneously\")\n         if worker_type_exists and num_workers_exists:\n             if num_of_dpus is not None:\n                 raise ValueError(\"Cannot specify num_of_dpus with custom WorkerType\")\n@@ -114,12 +119,16 @@ def create_glue_job_config(self) -> dict:\n             \"ScriptLocation\": self.script_location,\n         }\n         command = self.create_job_kwargs.pop(\"Command\", default_command)\n-        execution_role = self.get_iam_execution_role()\n+        if not self.role_arn:\n+            execution_role = self.get_iam_execution_role()\n+            role_arn = execution_role[\"Role\"][\"Arn\"]\n+        else:\n+            role_arn = self.role_arn\n \n         config = {\n             \"Name\": self.job_name,\n             \"Description\": self.desc,\n-            \"Role\": execution_role[\"Role\"][\"Arn\"],\n+            \"Role\": role_arn,\n             \"ExecutionProperty\": {\"MaxConcurrentRuns\": self.concurrent_run_limit},\n             \"Command\": command,\n             \"MaxRetries\": self.retry_limit,\n@@ -144,7 +153,6 @@ def list_jobs(self) -> list:\n         return self.conn.get_jobs()\n \n     def get_iam_execution_role(self) -> dict:\n-        \"\"\"Get IAM Role for job execution.\"\"\"\n         try:\n             iam_client = self.get_session(region_name=self.region_name).client(\n                 \"iam\", endpoint_url=self.conn_config.endpoint_url, config=self.config, verify=self.verify\ndiff --git a/airflow/providers/amazon/aws/operators/glue.py b/airflow/providers/amazon/aws/operators/glue.py\nindex 265d057de51ae..d47d1c25de5ec 100644\n--- a/airflow/providers/amazon/aws/operators/glue.py\n+++ b/airflow/providers/amazon/aws/operators/glue.py\n@@ -53,7 +53,8 @@ class GlueJobOperator(BaseOperator):\n     :param num_of_dpus: Number of AWS Glue DPUs to allocate to this Job.\n     :param region_name: aws region name (example: us-east-1)\n     :param s3_bucket: S3 bucket where logs and local etl script will be uploaded\n-    :param iam_role_name: AWS IAM Role for Glue Job Execution\n+    :param iam_role_name: AWS IAM Role for Glue Job Execution. If set `iam_role_arn` must equal None.\n+    :param iam_role_arn: AWS IAM ARN for Glue Job Execution. If set `iam_role_name` must equal None.\n     :param create_job_kwargs: Extra arguments for Glue Job Creation\n     :param run_job_kwargs: Extra arguments for Glue Job Run\n     :param wait_for_completion: Whether to wait for job run completion. (default: True)\n@@ -72,6 +73,7 @@ class GlueJobOperator(BaseOperator):\n         \"create_job_kwargs\",\n         \"s3_bucket\",\n         \"iam_role_name\",\n+        \"iam_role_arn\",\n     )\n     template_ext: Sequence[str] = ()\n     template_fields_renderers = {\n@@ -96,6 +98,7 @@ def __init__(\n         region_name: str | None = None,\n         s3_bucket: str | None = None,\n         iam_role_name: str | None = None,\n+        iam_role_arn: str | None = None,\n         create_job_kwargs: dict | None = None,\n         run_job_kwargs: dict | None = None,\n         wait_for_completion: bool = True,\n@@ -118,6 +121,7 @@ def __init__(\n         self.region_name = region_name\n         self.s3_bucket = s3_bucket\n         self.iam_role_name = iam_role_name\n+        self.iam_role_arn = iam_role_arn\n         self.s3_protocol = \"s3://\"\n         self.s3_artifacts_prefix = \"artifacts/glue-scripts/\"\n         self.create_job_kwargs = create_job_kwargs\n@@ -154,6 +158,7 @@ def glue_job_hook(self) -> GlueJobHook:\n             region_name=self.region_name,\n             s3_bucket=self.s3_bucket,\n             iam_role_name=self.iam_role_name,\n+            iam_role_arn=self.iam_role_arn,\n             create_job_kwargs=self.create_job_kwargs,\n             update_config=self.update_config,\n             job_poll_interval=self.job_poll_interval,\ndiff --git a/tests/providers/amazon/aws/hooks/test_glue.py b/tests/providers/amazon/aws/hooks/test_glue.py\nindex c41598f3d9aa6..1fae16e339558 100644\n--- a/tests/providers/amazon/aws/hooks/test_glue.py\n+++ b/tests/providers/amazon/aws/hooks/test_glue.py\n@@ -66,6 +66,22 @@ def test_get_iam_execution_role(self, role_path):\n         assert \"Arn\" in iam_role[\"Role\"]\n         assert iam_role[\"Role\"][\"Arn\"] == f\"arn:aws:iam::123456789012:role{role_path}{expected_role}\"\n \n+    @mock.patch.object(GlueJobHook, \"get_iam_execution_role\")\n+    @mock.patch.object(GlueJobHook, \"conn\")\n+    def test_init_iam_role_value_error(self, mock_conn, mock_get_iam_execution_role):\n+        mock_get_iam_execution_role.return_value = mock.MagicMock(\n+            Role={\"RoleName\": \"my_test_role_name\", \"RoleArn\": \"my_test_role\"}\n+        )\n+\n+        with pytest.raises(ValueError, match=\"Cannot set iam_role_arn and iam_role_name simultaneously\"):\n+            GlueJobHook(\n+                job_name=\"aws_test_glue_job\",\n+                desc=\"This is test case job from Airflow\",\n+                s3_bucket=\"some-bucket\",\n+                iam_role_name=\"my_test_role_name\",\n+                iam_role_arn=\"my_test_role\",\n+            )\n+\n     @mock.patch.object(AwsBaseHook, \"conn\")\n     def test_has_job_exists(self, mock_conn):\n         job_name = \"aws_test_glue_job\"\n@@ -90,6 +106,56 @@ class JobNotFoundException(Exception):\n         assert result is False\n         mock_conn.get_job.assert_called_once_with(JobName=job_name)\n \n+    @mock.patch.object(GlueJobHook, \"get_iam_execution_role\")\n+    @mock.patch.object(AwsBaseHook, \"conn\")\n+    def test_role_arn_has_job_exists(self, mock_conn, mock_get_iam_execution_role):\n+        \"\"\"\n+        Calls 'create_or_update_glue_job' with no existing job.\n+        Should create a new job.\n+        \"\"\"\n+\n+        class JobNotFoundException(Exception):\n+            pass\n+\n+        expected_job_name = \"aws_test_glue_job\"\n+        job_description = \"This is test case job from Airflow\"\n+        role_name = \"my_test_role\"\n+        role_name_arn = \"test_role\"\n+        some_s3_bucket = \"bucket\"\n+\n+        mock_conn.exceptions.EntityNotFoundException = JobNotFoundException\n+        mock_conn.get_job.side_effect = JobNotFoundException()\n+        mock_get_iam_execution_role.return_value = {\"Role\": {\"RoleName\": role_name, \"Arn\": role_name_arn}}\n+\n+        hook = GlueJobHook(\n+            s3_bucket=some_s3_bucket,\n+            job_name=expected_job_name,\n+            desc=job_description,\n+            concurrent_run_limit=2,\n+            retry_limit=3,\n+            num_of_dpus=5,\n+            iam_role_arn=role_name_arn,\n+            create_job_kwargs={\"Command\": {}},\n+            region_name=self.some_aws_region,\n+            update_config=True,\n+        )\n+\n+        result = hook.create_or_update_glue_job()\n+\n+        mock_conn.get_job.assert_called_once_with(JobName=expected_job_name)\n+        mock_conn.create_job.assert_called_once_with(\n+            Command={},\n+            Description=job_description,\n+            ExecutionProperty={\"MaxConcurrentRuns\": 2},\n+            LogUri=f\"s3://{some_s3_bucket}/logs/glue-logs/{expected_job_name}\",\n+            MaxCapacity=5,\n+            MaxRetries=3,\n+            Name=expected_job_name,\n+            Role=role_name_arn,\n+        )\n+        mock_conn.update_job.assert_not_called()\n+        assert result == expected_job_name\n+\n     @mock.patch.object(GlueJobHook, \"get_iam_execution_role\")\n     @mock.patch.object(GlueJobHook, \"conn\")\n     def test_create_or_update_glue_job_create_new_job(self, mock_conn, mock_get_iam_execution_role):\ndiff --git a/tests/providers/amazon/aws/operators/test_glue.py b/tests/providers/amazon/aws/operators/test_glue.py\nindex 9eed48e47adc9..dc298563ae65c 100644\n--- a/tests/providers/amazon/aws/operators/test_glue.py\n+++ b/tests/providers/amazon/aws/operators/test_glue.py\n@@ -48,6 +48,7 @@ def test_render_template(self, create_task_instance_of_operator):\n             script_args=\"{{ dag.dag_id }}\",\n             create_job_kwargs=\"{{ dag.dag_id }}\",\n             iam_role_name=\"{{ dag.dag_id }}\",\n+            iam_role_arn=\"{{ dag.dag_id }}\",\n             s3_bucket=\"{{ dag.dag_id }}\",\n             job_name=\"{{ dag.dag_id }}\",\n         )\n@@ -57,6 +58,7 @@ def test_render_template(self, create_task_instance_of_operator):\n         assert DAG_ID == rendered_template.script_args\n         assert DAG_ID == rendered_template.create_job_kwargs\n         assert DAG_ID == rendered_template.iam_role_name\n+        assert DAG_ID == rendered_template.iam_role_arn\n         assert DAG_ID == rendered_template.s3_bucket\n         assert DAG_ID == rendered_template.job_name\n \n@@ -99,6 +101,27 @@ def test_execute_without_failure(\n         mock_print_job_logs.assert_not_called()\n         assert glue.job_name == JOB_NAME\n \n+    @mock.patch.object(GlueJobHook, \"initialize_job\")\n+    @mock.patch.object(GlueJobHook, \"get_conn\")\n+    def test_role_arn_execute_deferrable(self, _, mock_initialize_job):\n+        glue = GlueJobOperator(\n+            task_id=TASK_ID,\n+            job_name=JOB_NAME,\n+            script_location=\"s3://folder/file\",\n+            aws_conn_id=\"aws_default\",\n+            region_name=\"us-west-2\",\n+            s3_bucket=\"some_bucket\",\n+            iam_role_arn=\"test_role\",\n+            deferrable=True,\n+        )\n+        mock_initialize_job.return_value = {\"JobRunState\": \"RUNNING\", \"JobRunId\": JOB_RUN_ID}\n+\n+        with pytest.raises(TaskDeferred) as defer:\n+            glue.execute(mock.MagicMock())\n+\n+        assert defer.value.trigger.job_name == JOB_NAME\n+        assert defer.value.trigger.run_id == JOB_RUN_ID\n+\n     @mock.patch.object(GlueJobHook, \"initialize_job\")\n     @mock.patch.object(GlueJobHook, \"get_conn\")\n     def test_execute_deferrable(self, _, mock_initialize_job):\n",
    "generated_commit": "Add support for AWS API to get the latest PR (#3271)",
    "label_commit": "Add parameter to pass role ARN to `GlueJobOperator ` (#33408)",
    "bleu": 0.022416933501922302,
    "rouge_l": 0.19999999999999998,
    "meteor": 0.09900990099009903,
    "bert_score_f1": 0.10385257005691528,
    "answer_correctness": 0.19806943536585525
  },
  {
    "diff": "diff --git a/src/collectors/memcached/memcached.py b/src/collectors/memcached/memcached.py\nindex 9199f3a17..f2f7e8fe0 100644\n--- a/src/collectors/memcached/memcached.py\n+++ b/src/collectors/memcached/memcached.py\n@@ -17,7 +17,7 @@\n \n     [[[hosts]]]\n \n-    [[[[app-1]]]\n+    [[[[app-1]]]]\n     host = localhost\n     port = 11211\n ```\n",
    "generated_commit": "Fix bug in the documentation for `env: 127` (#22)",
    "label_commit": "Fix typo in docstring",
    "bleu": 0.028517539529041507,
    "rouge_l": 0.30769230769230765,
    "meteor": 0.22222222222222224,
    "bert_score_f1": 0.2538627088069916,
    "answer_correctness": 0.20188882986954565
  },
  {
    "diff": "diff --git a/dev/mypy/plugin/decorators.py b/dev/mypy/plugin/decorators.py\nindex c05fd4560881b..1957b59996576 100644\n--- a/dev/mypy/plugin/decorators.py\n+++ b/dev/mypy/plugin/decorators.py\n@@ -25,7 +25,6 @@\n from mypy.types import CallableType, NoneType, UnionType\n \n TYPED_DECORATORS = {\n-    \"airflow.utils.session.provide_session\": [],\n     \"airflow.providers.google.cloud.hooks.dataflow._fallback_to_project_id_from_variables\": [\"project_id\"],\n     \"fallback_to_default_project_id of GoogleBaseHook\": [\"project_id\"],\n     \"provide_gcp_credential_file of GoogleBaseHook\": [],\n",
    "generated_commit": "Remove unused `os.environ` (#3536)",
    "label_commit": "Remove provide_session from Mypy plugin usage (#31498)",
    "bleu": 0.037951271263104894,
    "rouge_l": 0.15384615384615385,
    "meteor": 0.07462686567164178,
    "bert_score_f1": 0.29466986656188965,
    "answer_correctness": 0.18880726780843732
  }
]