---- SAMPLE #1 ----
Generated commit : Remove unused variable
Ground-truth     : Modify logger level in backup snapshot cron
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / 0.0439 / 0.1741
Diff:
diff --git a/cron/backupSnapshot.py b/cron/backupSnapshot.py
index 732363a2e..8405a2b12 100755
--- a/cron/backupSnapshot.py
+++ b/cron/backupSnapshot.py
@@ -36,7 +36,6 @@
 from os.path import expanduser
 
 logger = logging.getLogger(sys.argv[0])
-logger.level=logging.DEBUG
 formatter = logging.Formatter('%(asctime)s %(name)s %(levelname)s %(message)s')
 
 


---- SAMPLE #2 ----
Generated commit : [AIRFLOW-XXX] Fix MySQL to work with empty value (#5033)
Ground-truth     : [AIRFLOW-6246] Add support None in mysql_to_gcs (#6808)
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.3000 / 0.0000 / 0.3780 / 0.4210
Diff:
diff --git a/airflow/operators/mysql_to_gcs.py b/airflow/operators/mysql_to_gcs.py
index b2282ab14d7cb..4ce135ee87789 100644
--- a/airflow/operators/mysql_to_gcs.py
+++ b/airflow/operators/mysql_to_gcs.py
@@ -113,6 +113,8 @@ def convert_type(self, value, schema_type):
         :param schema_type: BigQuery data type
         :type schema_type: str
         """
+        if value is None:
+            return value
         if isinstance(value, (datetime, date)):
             return calendar.timegm(value.timetuple())
         if isinstance(value, timedelta):
diff --git a/tests/operators/test_mysql_to_gcs.py b/tests/operators/test_mysql_to_gcs.py
index dd4d394d4e40c..a9a259045b5b4 100644
--- a/tests/operators/test_mysql_to_gcs.py
+++ b/tests/operators/test_mysql_to_gcs.py
@@ -88,6 +88,7 @@ def test_init(self):
         (datetime.datetime(1970, 1, 1, 1, 0), None, 3600),
         (decimal.Decimal(5), None, 5),
         (b"bytes", "BYTES", "Ynl0ZXM="),
+        (None, "BYTES", None)
     ])
     def test_convert_type(self, value, schema_type, expected):
         op = MySqlToGoogleCloudStorageOperator(


---- SAMPLE #3 ----
Generated commit : Add back data of the geo to the Data paragraph
Ground-truth     : forgot premium_type in feed app serializer (bug 1055742)
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / -0.1050 / 0.1712
Diff:
diff --git a/mkt/webapps/serializers.py b/mkt/webapps/serializers.py
index 643be563f7e..f3abbe19e0a 100644
--- a/mkt/webapps/serializers.py
+++ b/mkt/webapps/serializers.py
@@ -547,8 +547,8 @@ class ESAppFeedSerializer(BaseESAppFeedSerializer):
     class Meta(ESAppSerializer.Meta):
         fields = [
             'author', 'device_types', 'icons', 'id', 'is_packaged',
-            'manifest_url', 'name', 'payment_required', 'price',
-            'price_locale', 'ratings', 'slug'
+            'manifest_url', 'name', 'payment_required', 'premium_type',
+            'price', 'price_locale', 'ratings', 'slug'
         ]
 
 


---- SAMPLE #4 ----
Generated commit : Add tests for Data Profiling
Ground-truth     : Make region/carrier choices in rocketfuel API serializable, to answer OPTIONS (bug 984899)
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / -0.0283 / 0.1807
Diff:
diff --git a/mkt/api/fields.py b/mkt/api/fields.py
index fb11f307114..83b3d90fc7e 100644
--- a/mkt/api/fields.py
+++ b/mkt/api/fields.py
@@ -318,6 +318,17 @@ def __init__(self, *args, **kwargs):
         kwargs['choices'] = slugs_choices + ids_choices
         return super(SlugChoiceField, self).__init__(*args, **kwargs)
 
+    def metadata(self):
+        """Return metadata about the choices. It's customized to return the
+        name of each choice, because in that class, choices values are objects,
+        not strings directly. This makes it possible to serialize the metadata
+        without errors, which is necessary to answer OPTIONS (bug 984899)"""
+        data = super(SlugChoiceField, self).metadata()
+        data['choices'] = [{'value': v,
+                            'display_name': unicode(getattr(n, 'name', n))}
+                            for v, n in self.choices]
+        return data
+
     def to_native(self, value):
         if value:
             choice = self.ids_choices_dict.get(value, None)
diff --git a/mkt/api/tests/test_oauth.py b/mkt/api/tests/test_oauth.py
index 1aba1729ea1..2c540d25bed 100644
--- a/mkt/api/tests/test_oauth.py
+++ b/mkt/api/tests/test_oauth.py
@@ -132,6 +132,10 @@ def patch(self, url, data='', **kw):
         response = self.request(**kw)
         return response
 
+    def options(self, url):
+        url, headers, body = self.sign('OPTIONS', self.get_absolute_url(url))
+        return wrap(super(OAuthClient, self).options(url, **self.kw(headers)))
+
 
 class BaseOAuth(BaseAPI):
     fixtures = fixture('user_2519', 'group_admin', 'group_editor',
diff --git a/mkt/collections/tests/test_serializers.py b/mkt/collections/tests/test_serializers.py
index fcd6cff47ed..7088c34af21 100644
--- a/mkt/collections/tests/test_serializers.py
+++ b/mkt/collections/tests/test_serializers.py
@@ -1,4 +1,5 @@
 # -*- coding: utf-8 -*-
+import json
 from mock import patch
 from nose.tools import eq_, ok_
 from rest_framework import serializers
@@ -227,6 +228,9 @@ def setUp(self):
         self.serializer = CollectionSerializer(self.collection,
                                                context=minimal_context)
 
+    def test_metadata_is_serialized_to_json(self):
+        ok_(json.dumps(self.serializer.metadata()))
+
     def test_to_native(self, apps=None):
         if apps:
             for app in apps:
diff --git a/mkt/collections/tests/test_views.py b/mkt/collections/tests/test_views.py
index 36ff102dd75..a8f6318cc04 100644
--- a/mkt/collections/tests/test_views.py
+++ b/mkt/collections/tests/test_views.py
@@ -153,7 +153,7 @@ def create_additional_data(self):
 
 class TestCollectionViewSetListing(BaseCollectionViewSetTest):
     """
-    Tests the handling of GET requests to the list endpoint of
+    Tests the handling of GET/OPTIONS requests to the list endpoint of
     CollectionViewSet.
     """
     def listing(self, client):
@@ -183,6 +183,11 @@ def test_listing_has_perms(self):
         self.make_publisher()
         self.listing(self.client)
 
+    def test_options_has_perms(self):
+        self.make_publisher()
+        res = self.client.options(self.list_url)
+        eq_(res.status_code, 200)
+
     def test_listing_curator(self):
         self.make_curator()
         self.listing(self.client)


---- SAMPLE #5 ----
Generated commit : Remove the usage of built-in API calls
Ground-truth     : create thread/notes on resubmission from devhub (bug 908652)
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / 0.0307 / 0.1800
Diff:
diff --git a/apps/comm/utils.py b/apps/comm/utils.py
index e8ba5b0a508..9fd98636bee 100644
--- a/apps/comm/utils.py
+++ b/apps/comm/utils.py
@@ -3,16 +3,27 @@
 
 import commonware.log
 from email_reply_parser import EmailReplyParser
+import waffle
 
 from access import acl
 from access.models import Group
 from comm.models import (CommunicationNote, CommunicationNoteRead,
-                         CommunicationThreadCC, CommunicationThreadToken)
+                         CommunicationThreadCC, CommunicationThread,
+                         CommunicationThreadToken)
 from mkt.constants import comm
 from users.models import UserProfile
 
 
 log = commonware.log.getLogger('comm')
+action_note_types = {
+    'approve': comm.APPROVAL,
+    'disable': comm.DISABLED,
+    'escalate': comm.ESCALATION,
+    'info': comm.MORE_INFO_REQUIRED,
+    'comment': comm.REVIEWER_COMMENT,
+    'reject': comm.REJECTION,
+    'resubmit': comm.RESUBMISSION
+}
 
 
 class ThreadObjectPermission(object):
@@ -215,3 +226,40 @@ def get_recipients(note, fresh_thread=False):
         new_recipients_list.append((user_email, tok.uuid))
 
     return new_recipients_list
+
+
+def create_comm_thread(**kwargs):
+    if not waffle.switch_is_active('comm-dashboard'):
+        return
+
+    addon = kwargs['addon']
+    version = kwargs['version']
+    thread = CommunicationThread.objects.filter(addon=addon, version=version)
+
+    perms = {}
+    for key in kwargs['perms']:
+        perms['read_permission_%s' % key] = True
+
+    if thread.exists():
+        thread = thread[0]
+    else:
+        thread = CommunicationThread.objects.create(addon=addon,
+            version=version, **perms)
+
+    note = CommunicationNote.objects.create(
+        note_type=action_note_types[kwargs['action']],
+        body=kwargs['comments'], author=kwargs['profile'],
+        thread=thread, **perms)
+
+    moz_emails = addon.get_mozilla_contacts()
+
+    # CC mozilla contact.
+    for email in moz_emails:
+        try:
+            moz_contact = UserProfile.objects.get(email=email)
+        except UserProfile.DoesNotExist:
+            pass
+        else:
+            CommunicationThreadCC.objects.get_or_create(
+                thread=thread, user=moz_contact)
+    return thread, note
diff --git a/mkt/developers/tests/test_views_versions.py b/mkt/developers/tests/test_views_versions.py
index aeca75b40c8..84c4da96626 100644
--- a/mkt/developers/tests/test_views_versions.py
+++ b/mkt/developers/tests/test_views_versions.py
@@ -116,6 +116,19 @@ def test_rejected(self):
             addon=webapp, activity_log__action=action.id).exists(), (
                 "Didn't find `%s` action in logs." % action.short)
 
+    def test_comm_thread_after_resubmission(self):
+        self.create_switch('comm-dashboard')
+        self.webapp.update(status=amo.STATUS_REJECTED)
+        amo.set_user(UserProfile.objects.get(username='admin'))
+        (self.webapp.versions.latest()
+                             .all_files[0].update(status=amo.STATUS_DISABLED))
+        my_reply = 'no give up'
+        self.client.post(self.url, {'notes': my_reply,
+                                    'resubmit-app': ''})
+        notes = CommunicationNote.objects.all()
+        eq_(notes.count(), 1)
+        eq_(notes[0].body, my_reply)
+
     def test_rejected_packaged(self):
         self.webapp.update(is_packaged=True)
         comments = "oh no you di'nt!!"
diff --git a/mkt/developers/views.py b/mkt/developers/views.py
index b8882b69108..fa1531cd310 100644
--- a/mkt/developers/views.py
+++ b/mkt/developers/views.py
@@ -32,6 +32,7 @@
                             post_required)
 from amo.urlresolvers import reverse
 from amo.utils import escape_all
+from comm.utils import create_comm_thread
 from devhub.models import AppLog
 from files.models import File, FileUpload
 from files.utils import parse_addon
@@ -54,7 +55,6 @@
                                   trap_duplicate)
 from mkt.developers.utils import check_upload
 from mkt.developers.tasks import run_validator
-from mkt.reviewers.utils import create_comm_thread
 from mkt.submit.forms import AppFeaturesForm, NewWebappVersionForm
 from mkt.webapps.tasks import _update_manifest, update_manifests
 from mkt.webapps.models import Webapp
@@ -209,6 +209,11 @@ def status(request, addon_id, addon, webapp=False):
     if request.method == 'POST':
         if 'resubmit-app' in request.POST and form.is_valid():
             form.save()
+            perms = ('reviewer', 'senior_reviewer', 'staff')
+            create_comm_thread(action='resubmit', addon=addon,
+                comments=form.data['notes'], profile=request.amo_user,
+                version=addon.current_version, perms=perms)
+
             messages.success(request, _('App successfully resubmitted.'))
             return redirect(addon.get_dev_url('versions'))
 
diff --git a/mkt/reviewers/utils.py b/mkt/reviewers/utils.py
index 5d478e74f68..b2225d190c6 100644
--- a/mkt/reviewers/utils.py
+++ b/mkt/reviewers/utils.py
@@ -16,9 +16,7 @@
 from amo.helpers import absolutify
 from amo.urlresolvers import reverse
 from amo.utils import JSONEncoder, send_mail_jinja, to_language
-from comm.models import (CommunicationNote, CommunicationThread,
-                         CommunicationThreadCC)
-from comm.utils import get_recipients
+from comm.utils import create_comm_thread, get_recipients
 from editors.models import EscalationQueue, RereviewQueue, ReviewerScore
 from files.models import File
 
@@ -26,18 +24,9 @@
 from mkt.constants.features import FeatureProfile
 from mkt.site.helpers import product_as_dict
 from mkt.webapps.models import Webapp
-from users.models import UserProfile
 
 
 log = commonware.log.getLogger('z.mailer')
-action_note_types = {
-    'approve': comm.APPROVAL,
-    'disable': comm.DISABLED,
-    'escalate': comm.ESCALATION,
-    'info': comm.MORE_INFO_REQUIRED,
-    'comment': comm.REVIEWER_COMMENT,
-    'reject': comm.REJECTION
-}
 
 
 def send_mail(subject, template, context, emails, perm_setting=None, cc=None,
@@ -74,43 +63,6 @@ def send_note_emails(note):
                   [email], perm_setting='app_reviewed', reply_to=reply_to)
 
 
-def create_comm_thread(**kwargs):
-    if not waffle.switch_is_active('comm-dashboard'):
-        return
-
-    addon = kwargs['addon']
-    version = kwargs['version']
-    thread = CommunicationThread.objects.filter(addon=addon, version=version)
-
-    perms = {}
-    for key in kwargs['perms']:
-        perms['read_permission_%s' % key] = True
-
-    if thread.exists():
-        thread = thread[0]
-    else:
-        thread = CommunicationThread.objects.create(addon=addon,
-            version=version, **perms)
-
-    note = CommunicationNote.objects.create(
-        note_type=action_note_types[kwargs['action']],
-        body=kwargs['comments'], author=kwargs['profile'],
-        thread=thread, **perms)
-
-    moz_emails = addon.get_mozilla_contacts()
-
-    # CC mozilla contact.
-    for email in moz_emails:
-        try:
-            moz_contact = UserProfile.objects.get(email=email)
-        except UserProfile.DoesNotExist:
-            pass
-        else:
-            CommunicationThreadCC.objects.get_or_create(
-                thread=thread, user=moz_contact)
-    return thread, note
-
-
 class ReviewBase(object):
 
     def __init__(self, request, addon, version, review_type,


---- SAMPLE #6 ----
Generated commit : [AIRFLOW-XXX] Fix flaky tests in unittest (#4259)
Ground-truth     : tests: always cleanup registered test listeners (#27896)
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.1333 / 0.0714 / 0.1252 / 0.1944
Diff:
diff --git a/tests/plugins/test_plugins_manager.py b/tests/plugins/test_plugins_manager.py
index 9ed00cae05a01..9ae6f55b6be2e 100644
--- a/tests/plugins/test_plugins_manager.py
+++ b/tests/plugins/test_plugins_manager.py
@@ -65,6 +65,7 @@ def on_load(self, *args, **kwargs):
 
 @pytest.fixture(autouse=True, scope="module")
 def clean_plugins():
+    get_listener_manager().clear()
     yield
     get_listener_manager().clear()
 
diff --git a/tests/task/task_runner/test_standard_task_runner.py b/tests/task/task_runner/test_standard_task_runner.py
index c54a27ae8997b..797462136a47c 100644
--- a/tests/task/task_runner/test_standard_task_runner.py
+++ b/tests/task/task_runner/test_standard_task_runner.py
@@ -72,6 +72,7 @@ def logging_and_db(self):
         (as the test environment does not have enough context for the normal
         way to run) and ensures they reset back to normal on the way out.
         """
+        get_listener_manager().clear()
         clear_db_runs()
         dictConfig(LOGGING_CONFIG)
         yield
@@ -79,6 +80,7 @@ def logging_and_db(self):
         airflow_logger.handlers = []
         clear_db_runs()
         dictConfig(DEFAULT_LOGGING_CONFIG)
+        get_listener_manager().clear()
 
     def test_start_and_terminate(self):
         local_task_job = mock.Mock()


---- SAMPLE #7 ----
Generated commit : fixing issues with missing urls (#218)
Ground-truth     : [lambda][output][cli] credential storage on s3 will be prefixed with the service
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0177 / 0.1053 / 0.0476 / 0.0697 / 0.1734
Diff:
diff --git a/stream_alert/alert_processor/output_base.py b/stream_alert/alert_processor/output_base.py
index 23b9ea739..c8b8790fd 100644
--- a/stream_alert/alert_processor/output_base.py
+++ b/stream_alert/alert_processor/output_base.py
@@ -139,6 +139,9 @@ def _get_creds_from_s3(self, cred_location, descriptor):
             [boolean] True if download of creds from s3 was a success
         """
         try:
+            if not os.path.exists(os.path.dirname(cred_location)):
+                os.makedirs(os.path.dirname(cred_location))
+
             client = boto3.client('s3', region_name=self.region)
             with open(cred_location, 'wb') as cred_output:
                 client.download_fileobj(self.secrets_bucket,
@@ -253,7 +256,7 @@ def output_cred_name(self, descriptor):
 
         # should descriptor be enforced in all rules?
         if descriptor:
-            cred_name = '{}_{}'.format(cred_name, descriptor)
+            cred_name = '{}/{}'.format(cred_name, descriptor)
 
         return cred_name
 
diff --git a/stream_alert_cli/outputs.py b/stream_alert_cli/outputs.py
index 104127da7..407a350e5 100644
--- a/stream_alert_cli/outputs.py
+++ b/stream_alert_cli/outputs.py
@@ -125,7 +125,8 @@ def send_creds_to_s3(region, bucket, key, blob_data):
         client.put_object(
             Body=blob_data,
             Bucket=bucket,
-            Key=key
+            Key=key,
+            ServerSideEncryption='AES256'
         )
     except ClientError as err:
         LOGGER_CLI.exception('an error occurred while sending credentials for key [%s] to S3: %s',


---- SAMPLE #8 ----
Generated commit : Task decorator should be called with the original stack trace
Ground-truth     : Catch exceptions occuring in Mediator+Pool callbacks. Thanks to Christopher Hesse.
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / 0.2087 / 0.1874
Diff:
diff --git a/celery/concurrency/processes/__init__.py b/celery/concurrency/processes/__init__.py
index 4d8c74d5e11..abc1694a315 100644
--- a/celery/concurrency/processes/__init__.py
+++ b/celery/concurrency/processes/__init__.py
@@ -3,6 +3,8 @@
 Process Pools.
 
 """
+import traceback
+
 from time import sleep, time
 
 from celery import log
@@ -134,9 +136,20 @@ def on_ready(self, callbacks, errbacks, ret_value):
             if isinstance(ret_value.exception, (
                     SystemExit, KeyboardInterrupt)):
                 raise ret_value.exception
-            [errback(ret_value) for errback in errbacks]
+            [self.safe_apply_callback(errback, ret_value)
+                    for errback in errbacks]
         else:
-            [callback(ret_value) for callback in callbacks]
+            [self.safe_apply_callback(callback, ret_value)
+                    for callback in callbacks]
+
+    def safe_apply_callback(self, fun, *args):
+        try:
+            fun(*args)
+        except:
+            self.logger.error("Pool callback raised exception: %s" % (
+                traceback.format_exc(), ))
+
+
 
     @property
     def info(self):
diff --git a/celery/worker/controllers.py b/celery/worker/controllers.py
index 20c54af1418..9ec1156d0a2 100644
--- a/celery/worker/controllers.py
+++ b/celery/worker/controllers.py
@@ -4,6 +4,7 @@
 
 """
 import threading
+import traceback
 from Queue import Empty as QueueEmpty
 
 from celery import log
@@ -31,6 +32,7 @@ def __init__(self, ready_queue, callback, logger=None):
         self._shutdown = threading.Event()
         self._stopped = threading.Event()
         self.setDaemon(True)
+        self.setName(self.__class__.__name__)
 
     def move(self):
         try:
@@ -45,7 +47,14 @@ def move(self):
         self.logger.debug(
             "Mediator: Running callback for task: %s[%s]" % (
                 task.task_name, task.task_id))
-        self.callback(task)                 # execute
+
+        try:
+            self.callback(task)
+        except Exception, exc:
+            self.logger.error("Mediator callback raised exception %r\n%s" % (
+                exc, traceback.format_exc()))
+
+
 
     def run(self):
         while not self._shutdown.isSet():


---- SAMPLE #9 ----
Generated commit : fixing issue with missing keys (#2186)
Ground-truth     : add full list of events to aggregated alert, closes #229
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / 0.2388 / 0.1927
Diff:
diff --git a/alerts/lib/alerttask.py b/alerts/lib/alerttask.py
index 533dc746e..e2539eb72 100644
--- a/alerts/lib/alerttask.py
+++ b/alerts/lib/alerttask.py
@@ -267,13 +267,17 @@ def searchEventsAggreg(self, aggregField, samplesLimit=5):
                 idict = {
                     'value': i[0],
                     'count': i[1],
-                    'events': []
+                    'events': [],
+                    'allevents': []
                 }
                 for r in results:
                     if r['_source']['details'][aggregField].encode('ascii', 'ignore') == i[0]:
                         # copy events detail into this aggregation up to our samples limit
                         if len(idict['events']) < samplesLimit:
                             idict['events'].append(r)
+                        # also copy all events to a non-sampled list
+                        # so we mark all events as alerted and don't re-alert
+                        idict['allevents'].append(r)
                 aggregList.append(idict)
 
             self.aggregations = aggregList
@@ -306,7 +310,7 @@ def walkAggregations(self, threshold):
                     self.log.debug(alert)
                     if alert:
                         alertResultES = self.alertToES(alert)
-                        self.tagEventsAlert(aggreg['events'], alertResultES)
+                        self.tagEventsAlert(aggreg['allevents'], alertResultES)
                         self.alertToMessageQueue(alert)
 
     def createAlertDict(self, summary, category, tags, events, severity='NOTICE'):


---- SAMPLE #10 ----
Generated commit : Fixing typo in OPA API docs (#36)
Ground-truth     : rename details.service to details.finding

As per: https://github.com/mozilla/guardDuty2MozDef/pull/1/files
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / -0.1421 / 0.1919
Diff:
diff --git a/mq/plugins/guardDuty.py b/mq/plugins/guardDuty.py
index d15175f1a..4a00c9198 100644
--- a/mq/plugins/guardDuty.py
+++ b/mq/plugins/guardDuty.py
@@ -21,8 +21,8 @@ def __init__(self):
         # AWS guard duty sends dates as iso_8601 which ES doesn't appreciate
         # here's a list of date fields we'll convert to isoformat
         self.date_keys = [
-            'details.service.eventLastSeen',
-            'details.service.eventFirstSeen',
+            'details.finding.eventLastSeen',
+            'details.finding.eventFirstSeen',
             'details.resource.instanceDetails.launchTime',
             'details.createdAt',
             'details.updatedAt'
@@ -32,8 +32,8 @@ def __init__(self):
         # Lets pick out some likely targets and format them
         # so other mozdef plugins can rely on their location
         self.ipaddress_keys =[
-            'details.service.action.networkConnectionAction.remoteIpDetails.ipAddressV4',
-            'details.service.action.awsApiCallAction.remoteIpDetails.ipAdrressV4'
+            'details.finding.action.networkConnectionAction.remoteIpDetails.ipAddressV4',
+            'details.finding.action.awsApiCallAction.remoteIpDetails.ipAdrressV4'
         ]
 
     def convert_key_date_format(self, needle, haystack):


---- SAMPLE #11 ----
Generated commit : [AIRFLOW-3254] Fix scheduling failure in scheduler job creation (#4201)
Ground-truth     : Only send an SlaCallbackRequest if the DAG is scheduled (#26089)
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.1000 / 0.0505 / 0.1050 / 0.1983
Diff:
diff --git a/airflow/dag_processing/processor.py b/airflow/dag_processing/processor.py
index 1a7fa6f10565a..b7dac828e2894 100644
--- a/airflow/dag_processing/processor.py
+++ b/airflow/dag_processing/processor.py
@@ -417,26 +417,23 @@ def manage_slas(self, dag: DAG, session: Session = None) -> None:
 
             sla_misses = []
             next_info = dag.next_dagrun_info(dag.get_run_data_interval(ti.dag_run), restricted=False)
-            if next_info is None:
-                self.log.info("Skipping SLA check for %s because task does not have scheduled date", ti)
-            else:
-                while next_info.logical_date < ts:
-                    next_info = dag.next_dagrun_info(next_info.data_interval, restricted=False)
-
-                    if next_info is None:
-                        break
-                    if (ti.dag_id, ti.task_id, next_info.logical_date) in recorded_slas_query:
-                        break
-                    if next_info.logical_date + task.sla < ts:
-
-                        sla_miss = SlaMiss(
-                            task_id=ti.task_id,
-                            dag_id=ti.dag_id,
-                            execution_date=next_info.logical_date,
-                            timestamp=ts,
-                        )
-                        sla_misses.append(sla_miss)
-                        Stats.incr('sla_missed')
+            while next_info and next_info.logical_date < ts:
+                next_info = dag.next_dagrun_info(next_info.data_interval, restricted=False)
+
+                if next_info is None:
+                    break
+                if (ti.dag_id, ti.task_id, next_info.logical_date) in recorded_slas_query:
+                    break
+                if next_info.logical_date + task.sla < ts:
+
+                    sla_miss = SlaMiss(
+                        task_id=ti.task_id,
+                        dag_id=ti.dag_id,
+                        execution_date=next_info.logical_date,
+                        timestamp=ts,
+                    )
+                    sla_misses.append(sla_miss)
+                    Stats.incr('sla_missed')
             if sla_misses:
                 session.add_all(sla_misses)
         session.commit()
diff --git a/airflow/jobs/scheduler_job.py b/airflow/jobs/scheduler_job.py
index 62907e7d7dd36..40411f270e172 100644
--- a/airflow/jobs/scheduler_job.py
+++ b/airflow/jobs/scheduler_job.py
@@ -1318,6 +1318,10 @@ def _send_sla_callbacks_to_processor(self, dag: DAG) -> None:
             self.log.debug("Skipping SLA check for %s because no tasks in DAG have SLAs", dag)
             return
 
+        if not dag.timetable.periodic:
+            self.log.debug("Skipping SLA check for %s because DAG is not scheduled", dag)
+            return
+
         request = SlaCallbackRequest(full_filepath=dag.fileloc, dag_id=dag.dag_id)
         self.executor.send_callback(request)
 
diff --git a/tests/jobs/test_scheduler_job.py b/tests/jobs/test_scheduler_job.py
index 584059d414ad2..0466a66455704 100644
--- a/tests/jobs/test_scheduler_job.py
+++ b/tests/jobs/test_scheduler_job.py
@@ -2985,10 +2985,18 @@ def test_send_sla_callbacks_to_processor_sla_no_task_slas(self, dag_maker):
             self.scheduler_job._send_sla_callbacks_to_processor(dag)
             self.scheduler_job.executor.callback_sink.send.assert_not_called()
 
-    def test_send_sla_callbacks_to_processor_sla_with_task_slas(self, dag_maker):
+    @pytest.mark.parametrize(
+        "schedule",
+        [
+            "@daily",
+            "0 10 * * *",
+            timedelta(hours=2),
+        ],
+    )
+    def test_send_sla_callbacks_to_processor_sla_with_task_slas(self, schedule, dag_maker):
         """Test SLA Callbacks are sent to the DAG Processor when SLAs are defined on tasks"""
         dag_id = 'test_send_sla_callbacks_to_processor_sla_with_task_slas'
-        with dag_maker(dag_id=dag_id, schedule='@daily') as dag:
+        with dag_maker(dag_id=dag_id, schedule=schedule) as dag:
             EmptyOperator(task_id='task1', sla=timedelta(seconds=60))
 
         with patch.object(settings, "CHECK_SLAS", True):
@@ -3000,6 +3008,26 @@ def test_send_sla_callbacks_to_processor_sla_with_task_slas(self, dag_maker):
             expected_callback = SlaCallbackRequest(full_filepath=dag.fileloc, dag_id=dag.dag_id)
             self.scheduler_job.executor.callback_sink.send.assert_called_once_with(expected_callback)
 
+    @pytest.mark.parametrize(
+        "schedule",
+        [
+            None,
+            [Dataset("foo")],
+        ],
+    )
+    def test_send_sla_callbacks_to_processor_sla_dag_not_scheduled(self, schedule, dag_maker):
+        """Test SLA Callbacks are not sent when DAG isn't scheduled"""
+        dag_id = 'test_send_sla_callbacks_to_processor_sla_no_task_slas'
+        with dag_maker(dag_id=dag_id, schedule=schedule) as dag:
+            EmptyOperator(task_id='task1', sla=timedelta(seconds=5))
+
+        with patch.object(settings, "CHECK_SLAS", True):
+            self.scheduler_job = SchedulerJob(subdir=os.devnull)
+            self.scheduler_job.executor = MockExecutor()
+
+            self.scheduler_job._send_sla_callbacks_to_processor(dag)
+            self.scheduler_job.executor.callback_sink.send.assert_not_called()
+
     def test_create_dag_runs(self, dag_maker):
         """
         Test various invariants of _create_dag_runs.


---- SAMPLE #12 ----
Generated commit : [sentinel] now raises KeyError when setting base_url is not set (#5535)
Ground-truth     : Fix handling of non-string keys in filter_hidden_settings
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0909 / 0.0676 / 0.1667 / 0.1798
Diff:
diff --git a/celery/app/utils.py b/celery/app/utils.py
index ba5e1bb8b14..a409d8fac12 100644
--- a/celery/app/utils.py
+++ b/celery/app/utils.py
@@ -175,11 +175,12 @@ def filter_hidden_settings(conf):
     def maybe_censor(key, value, mask='*' * 8):
         if isinstance(value, Mapping):
             return filter_hidden_settings(value)
-        if isinstance(value, string_t) and HIDDEN_SETTINGS.search(key):
-            return mask
-        if isinstance(key, string_t) and 'BROKER_URL' in key.upper():
-            from kombu import Connection
-            return Connection(value).as_uri(mask=mask)
+        if isinstance(key, string_t):
+            if HIDDEN_SETTINGS.search(key):
+                return mask
+            if 'BROKER_URL' in key.upper():
+                from kombu import Connection
+                return Connection(value).as_uri(mask=mask)
         return value
 
     return {k: maybe_censor(k, v) for k, v in items(conf)}
diff --git a/celery/tests/app/test_utils.py b/celery/tests/app/test_utils.py
index dc7e3811042..b0ff108e834 100644
--- a/celery/tests/app/test_utils.py
+++ b/celery/tests/app/test_utils.py
@@ -2,7 +2,7 @@
 
 from collections import Mapping, MutableMapping
 
-from celery.app.utils import Settings, bugreport
+from celery.app.utils import Settings, filter_hidden_settings, bugreport
 
 from celery.tests.case import AppCase, Mock
 
@@ -20,6 +20,22 @@ def test_is_mutable_mapping(self):
         self.assertTrue(issubclass(Settings, MutableMapping))
 
 
+class test_filter_hidden_settings(AppCase):
+
+    def test_handles_non_string_keys(self):
+        """filter_hidden_settings shouldn't raise an exception when handling
+        mappings with non-string keys"""
+        conf = {
+            'STRING_KEY': 'VALUE1',
+            ('NON', 'STRING', 'KEY'): 'VALUE2',
+            'STRING_KEY2': {
+                'STRING_KEY3': 1,
+                ('NON', 'STRING', 'KEY', '2'): 2
+            },
+        }
+        filter_hidden_settings(conf)
+
+
 class test_bugreport(AppCase):
 
     def test_no_conn_driver_info(self):


---- SAMPLE #13 ----
Generated commit : fix doc string
Ground-truth     : rename postgre collector IdleInTransactions metric
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / 0.1413 / 0.1773
Diff:
diff --git a/src/collectors/postgres/postgres.py b/src/collectors/postgres/postgres.py
index 7af28e5a3..0b329adbc 100644
--- a/src/collectors/postgres/postgres.py
+++ b/src/collectors/postgres/postgres.py
@@ -373,7 +373,7 @@ class TransactionCount(QueryStats):
 
 
 class IdleInTransactions(QueryStats):
-    path = "%(datname)s.longest_running.%(metric)s"
+    path = "%(datname)s.idle_in_tranactions.%(metric)s"
     multi_db = True
     query = """
         SELECT 'idle_in_transaction',


---- SAMPLE #14 ----
Generated commit : [AIRFLOW-XXX] Add missing parenthesis for running the task (#3974)
Ground-truth     : [AIRFLOW-908] Print hostname at the start of cli run

Closes #2329 from AllisonWang/master
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0154 / 0.1600 / 0.0794 / 0.2578 / 0.1916
Diff:
diff --git a/airflow/bin/cli.py b/airflow/bin/cli.py
index 6aa71196973ff..41f979fa51edb 100755
--- a/airflow/bin/cli.py
+++ b/airflow/bin/cli.py
@@ -19,6 +19,7 @@
 import reprlib
 
 import os
+import socket
 import subprocess
 import textwrap
 import warnings
@@ -384,6 +385,9 @@ def run(args, dag=None):
             level=settings.LOGGING_LEVEL,
             format=settings.LOG_FORMAT)
 
+    hostname = socket.getfqdn()
+    logging.info("Running on host {}".format(hostname))
+
     if not args.pickle and not dag:
         dag = get_dag(args)
     elif not dag:


---- SAMPLE #15 ----
Generated commit : Ignore errors when processing task queries
Ground-truth     : Modify workers to stop bulk queue on errors
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0293 / 0.1429 / 0.0641 / 0.2582 / 0.1930
Diff:
diff --git a/mq/esworker_cloudtrail.py b/mq/esworker_cloudtrail.py
index e7c34b31f..48f47efac 100755
--- a/mq/esworker_cloudtrail.py
+++ b/mq/esworker_cloudtrail.py
@@ -505,4 +505,10 @@ def initConfig():
     es = esConnect()
 
     pluginList = registerPlugins()
-    main()
+
+    try:
+        main()
+    except Exception as e:
+        if options.esbulksize != 0:
+            es.finish_bulk()
+        raise
diff --git a/mq/esworker_eventtask.py b/mq/esworker_eventtask.py
index 3a7a27923..bc1e1408b 100755
--- a/mq/esworker_eventtask.py
+++ b/mq/esworker_eventtask.py
@@ -362,4 +362,9 @@ def initConfig():
 
     pluginList = registerPlugins()
 
-    main()
+    try:
+        main()
+    except Exception as e:
+        if options.esbulksize != 0:
+            es.finish_bulk()
+        raise
diff --git a/mq/esworker_papertrail.py b/mq/esworker_papertrail.py
index e96858dc5..ef402f28b 100755
--- a/mq/esworker_papertrail.py
+++ b/mq/esworker_papertrail.py
@@ -267,11 +267,8 @@ def run(self):
 
                 time.sleep(options.ptinterval)
 
-            except KeyboardInterrupt:
-                sys.exit(1)
             except ValueError as e:
                 logger.exception('Exception while handling message: %r' % e)
-                sys.exit(1)
 
     def on_message(self, body, message):
         # print("RECEIVED MESSAGE: %r" % (body, ))
@@ -417,4 +414,9 @@ def initConfig():
 
     pluginList = registerPlugins()
 
-    main()
+    try:
+        main()
+    except Exception as e:
+        if options.esbulksize != 0:
+            es.finish_bulk()
+        raise
diff --git a/mq/esworker_sns_sqs.py b/mq/esworker_sns_sqs.py
index 63e702bcf..10e816de5 100755
--- a/mq/esworker_sns_sqs.py
+++ b/mq/esworker_sns_sqs.py
@@ -61,24 +61,20 @@ def run(self):
         self.taskQueue.set_message_class(RawMessage)
 
         while True:
-            try:
-                records = self.taskQueue.get_messages(self.options.prefetch)
-                for msg in records:
-                    msg_body = msg.get_body()
-                    try:
-                        # get_body() should be json
-                        message_json = json.loads(msg_body)
-                        self.on_message(message_json)
-                        # delete message from queue
-                        self.taskQueue.delete_message(msg)
-                    except ValueError:
-                        logger.error('Invalid message, not JSON <dropping message and continuing>: %r' % msg_body)
-                        self.taskQueue.delete_message(msg)
-                        continue
-                time.sleep(.1)
-            except Exception as e:
-                logger.exception(e)
-                sys.exit(1)
+            records = self.taskQueue.get_messages(self.options.prefetch)
+            for msg in records:
+                msg_body = msg.get_body()
+                try:
+                    # get_body() should be json
+                    message_json = json.loads(msg_body)
+                    self.on_message(message_json)
+                    # delete message from queue
+                    self.taskQueue.delete_message(msg)
+                except ValueError:
+                    logger.error('Invalid message, not JSON <dropping message and continuing>: %r' % msg_body)
+                    self.taskQueue.delete_message(msg)
+                    continue
+            time.sleep(.1)
 
     def on_message(self, message):
         try:
@@ -241,4 +237,10 @@ def initConfig():
 
     # open ES connection globally so we don't waste time opening it per message
     es = esConnect()
-    main()
+
+    try:
+        main()
+    except Exception as e:
+        if options.esbulksize != 0:
+            es.finish_bulk()
+        raise
diff --git a/mq/esworker_sqs.py b/mq/esworker_sqs.py
index 602f422e5..a12814eb2 100755
--- a/mq/esworker_sqs.py
+++ b/mq/esworker_sqs.py
@@ -218,11 +218,9 @@ def run(self):
                     self.taskQueue.delete_message(msg)
                 time.sleep(.1)
 
-            except KeyboardInterrupt:
-                sys.exit(1)
             except ValueError as e:
                 logger.exception('Exception while handling message: %r' % e)
-                sys.exit(1)
+                self.taskQueue.delete_message(msg)
 
     def on_message(self, body, message):
         # print("RECEIVED MESSAGE: %r" % (body, ))
@@ -399,4 +397,9 @@ def initConfig():
 
     pluginList = registerPlugins()
 
-    main()
+    try:
+        main()
+    except Exception as e:
+        if options.esbulksize != 0:
+            es.finish_bulk()
+        raise


---- SAMPLE #16 ----
Generated commit : Fix bug with missing args (#866)
Ground-truth     : updated autosuggest API to replace 'icon' with 'icons' dict (bug 948367)
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0177 / 0.1176 / 0.0476 / 0.1876 / 0.1881
Diff:
diff --git a/apps/addons/models.py b/apps/addons/models.py
index c9b41438fdd..d4c6331f93b 100644
--- a/apps/addons/models.py
+++ b/apps/addons/models.py
@@ -906,8 +906,8 @@ def get_icon_url(self, size, use_default=True):
             icon_type_split = self.icon_type.split('/')
 
         # Get the closest allowed size without going over
-        if (size not in amo.ADDON_ICON_SIZES and
-            size >= amo.ADDON_ICON_SIZES[0]):
+        if (size not in amo.ADDON_ICON_SIZES
+                and size >= amo.ADDON_ICON_SIZES[0]):
             size = [s for s in amo.ADDON_ICON_SIZES if s < size][-1]
         elif size < amo.ADDON_ICON_SIZES[0]:
             size = amo.ADDON_ICON_SIZES[0]
diff --git a/apps/search/tests/test_views.py b/apps/search/tests/test_views.py
index 0d23cd2828b..aa04e20b2d1 100644
--- a/apps/search/tests/test_views.py
+++ b/apps/search/tests/test_views.py
@@ -1061,7 +1061,8 @@ def search_addons(self, url, params, addons=[], types=amo.ADDON_TYPES,
             if src:
                 expected_url += '?src=ss'
             eq_(got['url'], expected_url)
-            eq_(got['icon'], expected.icon_url)
+            eq_(got['icons'], {'32': expected.get_icon_url(32),
+                               '64': expected.get_icon_url(64)})
 
             assert expected.status in amo.REVIEWED_STATUSES, (
                 'Unreviewed add-ons should not appear in search results.')
diff --git a/apps/search/views.py b/apps/search/views.py
index ac0db4e8b97..f40c6c97b36 100644
--- a/apps/search/views.py
+++ b/apps/search/views.py
@@ -120,7 +120,10 @@ class BaseAjaxSearch(object):
             "id": 1865,
             "name": "Adblock Plus",
             "url": "http://path/to/details/page",
-            "icon": "http://path/to/icon",
+            "icons": {
+                "32": "http://path/to/icon-32",
+                "64": "http://path/to/icon-64"
+            }
         },
         ...
     ]
@@ -141,7 +144,10 @@ def __init__(self, request, excluded_ids=(), ratings=False):
             'id': 'id',
             'name': 'name',
             'url': 'get_url_path',
-            'icon': 'icon_url'
+            'icons': {
+                '32': ('get_icon_url', 32),
+                '64': ('get_icon_url', 64)
+            }
         }
         self.fields = getattr(self, 'fields', default_fields)
         if self.ratings:
@@ -171,18 +177,29 @@ def queryset(self):
                                     status__in=amo.REVIEWED_STATUSES)
         return results
 
+    def _build_fields(self, item, fields):
+        data = {}
+        for key, prop in fields.iteritems():
+            if isinstance(prop, dict):
+                data[key] = self._build_fields(item, prop)
+            else:
+                # prop is a tuple like: ('method', 'arg1, 'argN').
+                if isinstance(prop, tuple):
+                    val = getattr(item, prop[0])(*prop[1:])
+                else:
+                    val = getattr(item, prop, '')
+                    if callable(val):
+                        val = val()
+                data[key] = unicode(val)
+        return data
+
     def build_list(self):
         """Populate a list of dictionaries based on label => property."""
         results = []
         for item in self.queryset()[:self.limit]:
             if item.id in self.excluded_ids:
                 continue
-            d = {}
-            for key, prop in self.fields.iteritems():
-                val = getattr(item, prop, '')
-                if callable(val):
-                    val = val()
-                d[key] = unicode(val)
+            d = self._build_fields(item, self.fields)
             if self.src and 'url' in d:
                 d['url'] = urlparams(d['url'], src=self.src)
             results.append(d)


---- SAMPLE #17 ----
Generated commit : Remove unused code related to Python 3 compatibility (#214)
Ground-truth     : Use `isnot(None)` instead of `!= None` in SQL queries
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / 0.0084 / 0.1826
Diff:
diff --git a/ichnaea/service/search/views.py b/ichnaea/service/search/views.py
index fe69e567c..c7fbe5fad 100644
--- a/ichnaea/service/search/views.py
+++ b/ichnaea/service/search/views.py
@@ -33,7 +33,6 @@ def configure_search(config):
 
 
 def search_cell(session, data):
-    sql_null = None  # avoid pep8 warning
     radio = RADIO_TYPE.get(data['radio'], -1)
     cells = []
     for cell in data['cell']:
@@ -51,8 +50,8 @@ def search_cell(session, data):
             Cell.mnc == cell['mnc']).filter(
             Cell.lac == cell['lac']).filter(
             Cell.cid == cell['cid']).filter(
-            Cell.lat != sql_null).filter(
-            Cell.lon != sql_null
+            Cell.lat.isnot(None)).filter(
+            Cell.lon.isnot(None)
         )
         result = query.first()
         if result is not None:
@@ -72,7 +71,6 @@ def search_cell(session, data):
 
 
 def search_cell_lac(session, data):
-    sql_null = None  # avoid pep8 warning
     radio = RADIO_TYPE.get(data['radio'], -1)
     lacs = []
     for cell in data['cell']:
@@ -90,8 +88,8 @@ def search_cell_lac(session, data):
             Cell.mnc == cell['mnc']).filter(
             Cell.lac == cell['lac']).filter(
             Cell.cid == CELLID_LAC).filter(
-            Cell.lat != sql_null).filter(
-            Cell.lon != sql_null
+            Cell.lat.isnot(None)).filter(
+            Cell.lon.isnot(None)
         )
         result = query.first()
         if result is not None:
@@ -132,11 +130,10 @@ def signal_strength(w):
     if len(wifi_keys) < 3:
         # we didn't even get three keys, bail out
         return None
-    sql_null = None  # avoid pep8 warning
     query = session.query(Wifi.key, Wifi.lat, Wifi.lon).filter(
         Wifi.key.in_(wifi_keys)).filter(
-        Wifi.lat != sql_null).filter(
-        Wifi.lon != sql_null)
+        Wifi.lat.isnot(None)).filter(
+        Wifi.lon.isnot(None))
     wifis = query.all()
     if len(wifis) < 3:
         # we got fewer than three actual matches
diff --git a/ichnaea/service/submit/tasks.py b/ichnaea/service/submit/tasks.py
index bcb3c4a7b..bda48ae72 100644
--- a/ichnaea/service/submit/tasks.py
+++ b/ichnaea/service/submit/tasks.py
@@ -36,9 +36,6 @@
 from ichnaea.worker import celery
 
 
-sql_null = None  # avoid pep8 warning
-
-
 def process_mapstat_keyed(factor, stat_key, positions, session):
     tiles = defaultdict(int)
     # aggregate to tiles, according to factor


---- SAMPLE #18 ----
Generated commit : Fix bug in parse tests
Ground-truth     : Fix mkt.api.tests.test_base.TestSubRouterWithFormat tests

DRF apparently made the format slightly more permissive, update
our expected pattern.
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0086 / 0.1600 / 0.0714 / 0.0434 / 0.1984
Diff:
diff --git a/mkt/api/tests/test_base.py b/mkt/api/tests/test_base.py
index 2f27fa2597b..dcd592ef37b 100644
--- a/mkt/api/tests/test_base.py
+++ b/mkt/api/tests/test_base.py
@@ -67,7 +67,7 @@ def test_cors(self):
         def foo(request):
             return Response()
         request = RequestFactory().get('/')
-        r = foo(request)
+        foo(request)
         eq_(request.CORS, ['GET', 'PATCH'])
 
 
@@ -84,9 +84,9 @@ def test_format_is_included(self):
             {'name': 'bar-list', 'pattern': '^(?P<pk>[^/]+)/foo/$' },
             {'name': 'bar-detail', 'pattern': '^(?P<pk>[^/]+)/foo/$' },
             {'name': 'bar-list',
-             'pattern': '^(?P<pk>[^/.]+)/foo\\.(?P<format>[a-z]+)$' },
+             'pattern': '^(?P<pk>[^/.]+)/foo\\.(?P<format>[a-z0-9]+)$' },
             {'name': 'bar-detail',
-             'pattern': '^(?P<pk>[^/.]+)/foo\\.(?P<format>[a-z]+)$'},
+             'pattern': '^(?P<pk>[^/.]+)/foo\\.(?P<format>[a-z0-9]+)$'},
         ]
         actual = [{
             'name': url.name, 'pattern': url.regex.pattern
@@ -101,9 +101,9 @@ def test_format_is_included_no_trailing_slashes(self):
             {'name': 'bar-list', 'pattern': '^(?P<pk>[^/.]+)/foo$' },
             {'name': 'bar-detail', 'pattern': '^(?P<pk>[^/.]+)/foo$' },
             {'name': 'bar-list',
-             'pattern': '^(?P<pk>[^/.]+)/foo\\.(?P<format>[a-z]+)$' },
+             'pattern': '^(?P<pk>[^/.]+)/foo\\.(?P<format>[a-z0-9]+)$' },
             {'name': 'bar-detail',
-             'pattern': '^(?P<pk>[^/.]+)/foo\\.(?P<format>[a-z]+)$'},
+             'pattern': '^(?P<pk>[^/.]+)/foo\\.(?P<format>[a-z0-9]+)$'},
         ]
         actual = [{
             'name': url.name, 'pattern': url.regex.pattern


---- SAMPLE #19 ----
Generated commit : Adding the ability to control the service on the geographic location
Ground-truth     : Enable support for Relay
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / 0.2297 / 0.1970
Diff:
diff --git a/kitsune/customercare/forms.py b/kitsune/customercare/forms.py
index 2a7a0e1cacd..02225b4a432 100644
--- a/kitsune/customercare/forms.py
+++ b/kitsune/customercare/forms.py
@@ -3,6 +3,8 @@
 
 from kitsune.customercare.zendesk import CATEGORY_CHOICES, OS_CHOICES, ZendeskClient
 
+PRODUCTS_WITH_OS = ["firefox-private-network-vpn"]
+
 
 class ZendeskForm(forms.Form):
     """Form for submitting a ticket to Zendesk."""
@@ -23,6 +25,8 @@ class ZendeskForm(forms.Form):
     def __init__(self, *args, product, **kwargs):
         kwargs.update({"initial": {"product": product.slug}})
         super().__init__(*args, **kwargs)
+        if product.slug not in PRODUCTS_WITH_OS:
+            del self.fields["os"]
 
     def send(self, user):
         client = ZendeskClient()
diff --git a/kitsune/questions/config.py b/kitsune/questions/config.py
index 3a392358a32..068011a4736 100644
--- a/kitsune/questions/config.py
+++ b/kitsune/questions/config.py
@@ -773,6 +773,17 @@
                 ),
             },
         ),
+        (
+            "relay",
+            {
+                "name": _lazy("Firefox Relay"),
+                "subtitle": _lazy("Service that lets you create aliases to hide your real email"),
+                "extra_fields": [],
+                "tags": ["relay"],
+                "product": "relay",
+                "categories": OrderedDict([]),
+            },
+        ),
     ]
 )
 


---- SAMPLE #20 ----
Generated commit : Fix the order of consecutive space in the docs
Ground-truth     : Reorder position sources, put geoip as a last resort.
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.1111 / 0.0975 / 0.1880
Diff:
diff --git a/ichnaea/api/locate/searcher.py b/ichnaea/api/locate/searcher.py
index 108c7a524..646ea61a8 100644
--- a/ichnaea/api/locate/searcher.py
+++ b/ichnaea/api/locate/searcher.py
@@ -128,10 +128,10 @@ class PositionSearcher(Searcher):
 
     result_type = Position
     source_classes = (
-        ('geoip', GeoIPPositionSource),
         ('internal', InternalPositionSource),
         ('ocid', OCIDPositionSource),
         ('fallback', FallbackPositionSource),
+        ('geoip', GeoIPPositionSource),
     )
 
     def format_result(self, result):


---- SAMPLE #21 ----
Generated commit : [AIRFLOW-XXX] Fix ubuntu error when querying the master branch (#4275)
Ground-truth     : Fix argument error in AzureContainerInstancesOperator (#19668)
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0251 / 0.2353 / 0.1562 / 0.0991 / 0.1918
Diff:
diff --git a/airflow/providers/microsoft/azure/operators/container_instances.py b/airflow/providers/microsoft/azure/operators/container_instances.py
index da297c0a65462..2b317d81d7d71 100644
--- a/airflow/providers/microsoft/azure/operators/container_instances.py
+++ b/airflow/providers/microsoft/azure/operators/container_instances.py
@@ -199,7 +199,7 @@ def execute(self, context: dict) -> int:
         # Check name again in case it was templated.
         self._check_name(self.name)
 
-        self._ci_hook = AzureContainerInstanceHook(self.ci_conn_id)
+        self._ci_hook = AzureContainerInstanceHook(conn_id=self.ci_conn_id)
 
         if self.fail_if_exists:
             self.log.info("Testing if container group already exists")


---- SAMPLE #22 ----
Generated commit : A few changes around issues causing multiple bugs
Ground-truth     : Bug 1452212 - Don't use len() to check if sequences are empty

Fixes pylint `len-as-condition`.
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0800 / 0.0350 / -0.0396 / 0.1745
Diff:
diff --git a/tests/autoclassify/test_classify_failures.py b/tests/autoclassify/test_classify_failures.py
index 22ddedd67c7..b13f32f599e 100644
--- a/tests/autoclassify/test_classify_failures.py
+++ b/tests/autoclassify/test_classify_failures.py
@@ -168,8 +168,8 @@ def test_autoclassified_no_update_after_manual_classification_1(test_job_2,
         error_line.refresh_from_db()
         failure_line.refresh_from_db()
 
-    assert len(test_error_lines[0].matches.all()) == 0
-    assert len(test_failure_lines[0].matches.all()) == 0
+    assert not test_error_lines[0].matches.all().exists()
+    assert not test_failure_lines[0].matches.all().exists()
 
 
 def test_autoclassified_no_update_after_manual_classification_2(test_user, test_job_2,
@@ -189,7 +189,7 @@ def test_autoclassified_no_update_after_manual_classification_2(test_user, test_
     for item in test_failure_lines:
         item.refresh_from_db()
 
-    assert len(test_failure_lines[0].matches.all()) == 0
+    assert not test_failure_lines[0].matches.all().exists()
 
 
 def test_classify_skip_ignore(test_job_2,
diff --git a/tests/e2e/test_client_job_ingestion.py b/tests/e2e/test_client_job_ingestion.py
index f9cdb0c8e30..e1d1dd460c7 100644
--- a/tests/e2e/test_client_job_ingestion.py
+++ b/tests/e2e/test_client_job_ingestion.py
@@ -117,7 +117,7 @@ def test_post_job_pending_to_completed_with_unparsed_log(test_repository,
     post_collection(test_repository.name, tjc)
     # should have no text log errors or bug suggestions
     assert TextLogError.objects.count() == 0
-    assert len(get_error_summary(Job.objects.get(guid=job_guid))) == 0
+    assert get_error_summary(Job.objects.get(guid=job_guid)) == []
 
     # the second time, post a log that will get parsed
     log_url = add_log_response("mozilla-central-macosx64-debug-bm65-build1-build15.txt.gz")
diff --git a/tests/etl/test_job_ingestion.py b/tests/etl/test_job_ingestion.py
index 8e5a944c796..fa6c062652c 100644
--- a/tests/etl/test_job_ingestion.py
+++ b/tests/etl/test_job_ingestion.py
@@ -228,7 +228,7 @@ def test_remove_existing_jobs_single_existing(test_repository, failure_classific
     assert Job.objects.count() == 1
 
     data = _remove_existing_jobs(job_data)
-    assert len(data) == 0
+    assert data == []
 
 
 def test_remove_existing_jobs_one_existing_one_new(test_repository, failure_classifications,
diff --git a/tests/model/test_bugscache.py b/tests/model/test_bugscache.py
index e88bcb56a8b..d2e5fdd3307 100644
--- a/tests/model/test_bugscache.py
+++ b/tests/model/test_bugscache.py
@@ -88,7 +88,7 @@ def test_get_open_recent_bugs(transactional_db, sample_bugs, search_term, exp_bu
     suggestions = Bugscache.search(search_term)
     open_recent_bugs = [b['id'] for b in suggestions['open_recent']]
     assert open_recent_bugs == exp_bugs
-    assert len(suggestions['all_others']) == 0
+    assert suggestions['all_others'] == []
 
 
 @pytest.mark.parametrize(("search_term", "exp_bugs"), BUG_SEARCHES)
@@ -103,7 +103,7 @@ def test_get_all_other_bugs(transactional_db, sample_bugs, search_term, exp_bugs
     _update_bugscache(bug_list)
 
     suggestions = Bugscache.search(search_term)
-    assert len(suggestions['open_recent']) == 0
+    assert suggestions['open_recent'] == []
     all_others_bugs = [b['id'] for b in suggestions['all_others']]
     assert all_others_bugs == exp_bugs
 
@@ -122,8 +122,7 @@ def test_get_recent_resolved_bugs(transactional_db, sample_bugs):
     _update_bugscache(bug_list)
 
     suggestions = Bugscache.search(search_term)
-    print(suggestions)
-    assert len(suggestions['open_recent']) == 0
+    assert suggestions['open_recent'] == []
     all_others_bugs = [b['id'] for b in suggestions['all_others']]
     assert all_others_bugs == exp_bugs
 
diff --git a/tests/model/test_classified_failure.py b/tests/model/test_classified_failure.py
index 8b8ac3cc0d3..38c0565459a 100644
--- a/tests/model/test_classified_failure.py
+++ b/tests/model/test_classified_failure.py
@@ -43,7 +43,7 @@ def test_set_bug_duplicate(failure_lines, classified_failures, test_matcher):
     # Check we picked the better of the two scores for the new match.
     assert matches[0].score == Decimal("0.8")
     # Ensure we deleted the ClassifiedFailure on which we tried to set the bug
-    assert len(ClassifiedFailure.objects.filter(id=classified_failures[1].id)) == 0
+    assert not ClassifiedFailure.objects.filter(id=classified_failures[1].id).exists()
 
 
 def test_update_autoclassification_bug(test_job, test_job_2,
diff --git a/tests/webapp/api/test_job_details_api.py b/tests/webapp/api/test_job_details_api.py
index c1a2fb93f51..253c30a4b14 100644
--- a/tests/webapp/api/test_job_details_api.py
+++ b/tests/webapp/api/test_job_details_api.py
@@ -103,7 +103,7 @@ def test_job_details(test_repository, failure_classifications,
                       '?repository={}&job_id__in=3'.format(
                           test_repository.name))
     assert resp.status_int == 200
-    assert len(resp.json['results']) == 0
+    assert resp.json['results'] == []
 
     # add an extra one, but filter to just get those with a specific title.
     # we should only have one
diff --git a/tests/webapp/api/test_performance_alertsummary_api.py b/tests/webapp/api/test_performance_alertsummary_api.py
index f130a555686..414fec45e42 100644
--- a/tests/webapp/api/test_performance_alertsummary_api.py
+++ b/tests/webapp/api/test_performance_alertsummary_api.py
@@ -97,7 +97,7 @@ def test_alert_summaries_get(webapp, test_perf_alert_summary,
         'related_summary_id',
         'classifier'
     ])
-    assert len(resp.json['results'][0]['related_alerts']) == 0
+    assert resp.json['results'][0]['related_alerts'] == []
 
 
 def test_alert_summaries_get_onhold(webapp, test_perf_alert_summary,
@@ -141,7 +141,7 @@ def test_alert_summaries_get_onhold(webapp, test_perf_alert_summary,
         'related_summary_id',
         'classifier'
     ])
-    assert len(resp.json['results'][0]['related_alerts']) == 0
+    assert resp.json['results'][0]['related_alerts'] == []
 
 
 def test_alert_summaries_put(webapp, test_repository, test_perf_signature,
diff --git a/treeherder/etl/jobs.py b/treeherder/etl/jobs.py
index d49dfb946f7..55e6049931e 100644
--- a/treeherder/etl/jobs.py
+++ b/treeherder/etl/jobs.py
@@ -123,7 +123,7 @@ def _load_job(repository, job_datum, push_id, lower_tier_signatures):
         symbol=job_datum.get('group_symbol') or 'unknown')
 
     product_name = job_datum.get('product_name', 'unknown')
-    if len(product_name.strip()) == 0:
+    if not product_name.strip():
         product_name = 'unknown'
     product, _ = Product.objects.get_or_create(name=product_name)
 
diff --git a/treeherder/perf/management/commands/test_analyze_perf.py b/treeherder/perf/management/commands/test_analyze_perf.py
index caaf7d040f4..1523855d23a 100644
--- a/treeherder/perf/management/commands/test_analyze_perf.py
+++ b/treeherder/perf/management/commands/test_analyze_perf.py
@@ -105,10 +105,7 @@ def handle(self, *args, **options):
                 for r in detect_changes(data):
                     if r.state == 'regression':
                         pushes = pc.get_pushes(project, id=r.testrun_id)
-                        if len(pushes):
-                            revision = pushes[0]['revision']
-                        else:
-                            revision = ''
+                        revision = pushes[0]['revision'] if pushes else ''
                         initial_value = r.historical_stats['avg']
                         new_value = r.forward_stats['avg']
                         if initial_value != 0:
diff --git a/treeherder/perf/models.py b/treeherder/perf/models.py
index e9fd7757c68..d402709dd58 100644
--- a/treeherder/perf/models.py
+++ b/treeherder/perf/models.py
@@ -249,16 +249,15 @@ def autodetermine_status(self):
                   PerformanceAlert.objects.filter(related_summary=self))
 
         # if no alerts yet, we'll say untriaged
-        if len(alerts) == 0:
+        if not alerts:
             return PerformanceAlertSummary.UNTRIAGED
 
         # if any untriaged, then set to untriaged
-        if len([a for a in alerts if a.status == PerformanceAlert.UNTRIAGED]):
+        if any(alert.status == PerformanceAlert.UNTRIAGED for alert in alerts):
             return PerformanceAlertSummary.UNTRIAGED
 
         # if all invalid, then set to invalid
-        if all([alert.status == PerformanceAlert.INVALID for alert in
-                alerts]):
+        if all(alert.status == PerformanceAlert.INVALID for alert in alerts):
             return PerformanceAlertSummary.INVALID
 
         # otherwise filter out invalid alerts
@@ -267,9 +266,9 @@ def autodetermine_status(self):
         # if there are any "acknowledged" alerts, then set to investigating
         # if not one of the resolved statuses and there are regressions,
         # otherwise we'll say it's an improvement
-        if len([a for a in alerts if a.status == PerformanceAlert.ACKNOWLEDGED]):
-            if all([not alert.is_regression for alert in
-                    alerts if alert.status == PerformanceAlert.ACKNOWLEDGED]):
+        if any(alert.status == PerformanceAlert.ACKNOWLEDGED for alert in alerts):
+            if all(not alert.is_regression for alert in
+                   alerts if alert.status == PerformanceAlert.ACKNOWLEDGED):
                 return PerformanceAlertSummary.IMPROVEMENT
             elif self.status not in (PerformanceAlertSummary.IMPROVEMENT,
                                      PerformanceAlertSummary.INVESTIGATING,
@@ -284,7 +283,7 @@ def autodetermine_status(self):
         # alerts of its own: all alerts should be either reassigned,
         # downstream, or invalid (but not all invalid, that case is covered
         # above)
-        if len([a for a in alerts if a.status == PerformanceAlert.REASSIGNED]):
+        if any(alert.status == PerformanceAlert.REASSIGNED for alert in alerts):
             return PerformanceAlertSummary.REASSIGNED
 
         return PerformanceAlertSummary.DOWNSTREAM
diff --git a/treeherder/perfalert/perfalert/__init__.py b/treeherder/perfalert/perfalert/__init__.py
index 16ae974ac56..b74a52606e9 100644
--- a/treeherder/perfalert/perfalert/__init__.py
+++ b/treeherder/perfalert/perfalert/__init__.py
@@ -60,7 +60,7 @@ def calc_t(w1, w2, weight_fn=None):
 
     See the analyze() function for a description of the `weight_fn` argument.
     """
-    if len(w1) == 0 or len(w2) == 0:
+    if not w1 or not w2:
         return 0
 
     s1 = analyze(w1, weight_fn)
diff --git a/treeherder/seta/high_value_jobs.py b/treeherder/seta/high_value_jobs.py
index e40633141b2..09a84d66a81 100644
--- a/treeherder/seta/high_value_jobs.py
+++ b/treeherder/seta/high_value_jobs.py
@@ -27,7 +27,7 @@ def check_removal(failures, removals):
             if not found:
                 results[failure].append(failure_job)
 
-        if len(results[failure]) == 0:
+        if not results[failure]:
             del results[failure]
 
     return results
diff --git a/treeherder/seta/preseed.py b/treeherder/seta/preseed.py
index f5e22bccb63..89472261de3 100644
--- a/treeherder/seta/preseed.py
+++ b/treeherder/seta/preseed.py
@@ -32,7 +32,7 @@ def load_preseed():
                 queryset = queryset.filter(**{field: job[field]})
 
         # Deal with the case where we have a new entry in preseed
-        if len(queryset) == 0:
+        if not queryset:
             create_new_entry(job)
         else:
             # We can have wildcards, so loop on all returned values in data


---- SAMPLE #23 ----
Generated commit : Fix bug in filtering file limit (#781)
Ground-truth     : Don't fail on max or min when the file sizes list is empty
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0140 / 0.0952 / 0.0403 / 0.1599 / 0.1891
Diff:
diff --git a/bugbug/repository.py b/bugbug/repository.py
index e21a60a362..dda194771d 100644
--- a/bugbug/repository.py
+++ b/bugbug/repository.py
@@ -229,8 +229,8 @@ def _transform(commit):
     obj["average_file_size"] = (
         obj["total_file_size"] / len(sizes) if len(sizes) > 0 else 0
     )
-    obj["maximum_file_size"] = max(sizes)
-    obj["minimum_file_size"] = min(sizes)
+    obj["maximum_file_size"] = max(sizes) if len(sizes) > 0 else 0
+    obj["minimum_file_size"] = min(sizes) if len(sizes) > 0 else 0
 
     obj["files_modified_num"] = len(patch_data)
 


---- SAMPLE #24 ----
Generated commit : celeryctl shell now works with celery.Task.TaskSet
Ground-truth     : celeryctl shell: chord, TaskSet/group and BaseTask is now in locals
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0249 / 0.3158 / 0.1042 / 0.4016 / 0.4639
Diff:
diff --git a/celery/bin/celeryctl.py b/celery/bin/celeryctl.py
index 3ced1c14324..dbd39e29048 100644
--- a/celery/bin/celeryctl.py
+++ b/celery/bin/celeryctl.py
@@ -381,8 +381,13 @@ def run(self, force_ipython=False, force_bpython=False,
             import_module("celery.concurrency.eventlet")
         if gevent:
             import_module("celery.concurrency.gevent")
+        from .. import task
         self.app.loader.import_default_modules()
-        self.locals = {"celery": self.app}
+        self.locals = {"celery": self.app,
+                       "BaseTask": task.BaseTask,
+                       "TaskSet": task.TaskSet,
+                       "chord": task.chord,
+                       "group": task.group}
 
         if not without_tasks:
             self.locals.update(dict((task.__name__, task)


---- SAMPLE #25 ----
Generated commit : [AIRFLOW-3624] Fix get_task params in trigger_dag (#3708)
Ground-truth     : Improve importing the module in Airflow API package (#33797)
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0248 / 0.1053 / 0.0568 / 0.1569 / 0.1975
Diff:
diff --git a/airflow/api/auth/backend/basic_auth.py b/airflow/api/auth/backend/basic_auth.py
index dbfeaa5f806d1..6d4b507a0345f 100644
--- a/airflow/api/auth/backend/basic_auth.py
+++ b/airflow/api/auth/backend/basic_auth.py
@@ -18,15 +18,17 @@
 from __future__ import annotations
 
 from functools import wraps
-from typing import Any, Callable, TypeVar, cast
+from typing import TYPE_CHECKING, Any, Callable, TypeVar, cast
 
 from flask import Response, request
 from flask_appbuilder.const import AUTH_LDAP
 from flask_login import login_user
 
-from airflow.auth.managers.fab.models import User
 from airflow.utils.airflow_flask_app import get_airflow_app
 
+if TYPE_CHECKING:
+    from airflow.auth.managers.fab.models import User
+
 CLIENT_AUTH: tuple[str, str] | Any | None = None
 
 
diff --git a/airflow/api/client/__init__.py b/airflow/api/client/__init__.py
index 35608032abcb6..26b0fa31f288e 100644
--- a/airflow/api/client/__init__.py
+++ b/airflow/api/client/__init__.py
@@ -19,11 +19,14 @@
 from __future__ import annotations
 
 from importlib import import_module
+from typing import TYPE_CHECKING
 
 from airflow import api
-from airflow.api.client.api_client import Client
 from airflow.configuration import conf
 
+if TYPE_CHECKING:
+    from airflow.api.client.api_client import Client
+
 
 def get_current_api_client() -> Client:
     """Return current API Client based on current Airflow configuration."""
diff --git a/airflow/api/common/delete_dag.py b/airflow/api/common/delete_dag.py
index 1d879a667addc..c94b3c39dfeca 100644
--- a/airflow/api/common/delete_dag.py
+++ b/airflow/api/common/delete_dag.py
@@ -19,9 +19,9 @@
 from __future__ import annotations
 
 import logging
+from typing import TYPE_CHECKING
 
 from sqlalchemy import and_, delete, or_, select
-from sqlalchemy.orm import Session
 
 from airflow import models
 from airflow.exceptions import AirflowException, DagNotFound
@@ -31,6 +31,9 @@
 from airflow.utils.session import NEW_SESSION, provide_session
 from airflow.utils.state import TaskInstanceState
 
+if TYPE_CHECKING:
+    from sqlalchemy.orm import Session
+
 log = logging.getLogger(__name__)
 
 
diff --git a/airflow/api/common/experimental/__init__.py b/airflow/api/common/experimental/__init__.py
index 35a4da3f1735a..e8f5548c4e629 100644
--- a/airflow/api/common/experimental/__init__.py
+++ b/airflow/api/common/experimental/__init__.py
@@ -18,10 +18,15 @@
 """Experimental APIs."""
 from __future__ import annotations
 
-from datetime import datetime
+from typing import TYPE_CHECKING
 
 from airflow.exceptions import DagNotFound, DagRunNotFound, TaskNotFound
-from airflow.models import DagBag, DagModel, DagRun
+from airflow.models import DagBag, DagModel
+
+if TYPE_CHECKING:
+    from datetime import datetime
+
+    from airflow.models import DagRun
 
 
 def check_and_get_dag(dag_id: str, task_id: str | None = None) -> DagModel:
diff --git a/airflow/api/common/experimental/get_dag_run_state.py b/airflow/api/common/experimental/get_dag_run_state.py
index cdf044a1569b1..fe246e8f77c21 100644
--- a/airflow/api/common/experimental/get_dag_run_state.py
+++ b/airflow/api/common/experimental/get_dag_run_state.py
@@ -18,12 +18,15 @@
 """DAG run APIs."""
 from __future__ import annotations
 
-from datetime import datetime
+from typing import TYPE_CHECKING
 
 from deprecated import deprecated
 
 from airflow.api.common.experimental import check_and_get_dag, check_and_get_dagrun
 
+if TYPE_CHECKING:
+    from datetime import datetime
+
 
 @deprecated(reason="Use DagRun().get_state() instead", version="2.2.4")
 def get_dag_run_state(dag_id: str, execution_date: datetime) -> dict[str, str]:
diff --git a/airflow/api/common/experimental/get_lineage.py b/airflow/api/common/experimental/get_lineage.py
index 73bc9dd862ef3..45bf2249c005b 100644
--- a/airflow/api/common/experimental/get_lineage.py
+++ b/airflow/api/common/experimental/get_lineage.py
@@ -19,16 +19,18 @@
 from __future__ import annotations
 
 import collections
-import datetime
-from typing import Any
-
-from sqlalchemy.orm import Session
+from typing import TYPE_CHECKING, Any
 
 from airflow.api.common.experimental import check_and_get_dag, check_and_get_dagrun
 from airflow.lineage import PIPELINE_INLETS, PIPELINE_OUTLETS
 from airflow.models.xcom import XCom
 from airflow.utils.session import NEW_SESSION, provide_session
 
+if TYPE_CHECKING:
+    import datetime
+
+    from sqlalchemy.orm import Session
+
 
 @provide_session
 def get_lineage(
diff --git a/airflow/api/common/experimental/get_task.py b/airflow/api/common/experimental/get_task.py
index 34e0fac37983f..8e1c4f641db9e 100644
--- a/airflow/api/common/experimental/get_task.py
+++ b/airflow/api/common/experimental/get_task.py
@@ -18,10 +18,14 @@
 """Task APIs."""
 from __future__ import annotations
 
+from typing import TYPE_CHECKING
+
 from deprecated import deprecated
 
 from airflow.api.common.experimental import check_and_get_dag
-from airflow.models import TaskInstance
+
+if TYPE_CHECKING:
+    from airflow.models import TaskInstance
 
 
 @deprecated(reason="Use DAG().get_task", version="2.2.4")
diff --git a/airflow/api/common/experimental/get_task_instance.py b/airflow/api/common/experimental/get_task_instance.py
index cc8c734338284..abc178516c3ef 100644
--- a/airflow/api/common/experimental/get_task_instance.py
+++ b/airflow/api/common/experimental/get_task_instance.py
@@ -18,13 +18,17 @@
 """Task instance APIs."""
 from __future__ import annotations
 
-from datetime import datetime
+from typing import TYPE_CHECKING
 
 from deprecated import deprecated
 
 from airflow.api.common.experimental import check_and_get_dag, check_and_get_dagrun
 from airflow.exceptions import TaskInstanceNotFound
-from airflow.models import TaskInstance
+
+if TYPE_CHECKING:
+    from datetime import datetime
+
+    from airflow.models import TaskInstance
 
 
 @deprecated(version="2.2.4", reason="Use DagRun.get_task_instance instead")
diff --git a/airflow/api/common/experimental/pool.py b/airflow/api/common/experimental/pool.py
index 1134bda989014..87adacdd76fb7 100644
--- a/airflow/api/common/experimental/pool.py
+++ b/airflow/api/common/experimental/pool.py
@@ -18,14 +18,18 @@
 """Pool APIs."""
 from __future__ import annotations
 
+from typing import TYPE_CHECKING
+
 from deprecated import deprecated
 from sqlalchemy import select
-from sqlalchemy.orm import Session
 
 from airflow.exceptions import AirflowBadRequest, PoolNotFound
 from airflow.models import Pool
 from airflow.utils.session import NEW_SESSION, provide_session
 
+if TYPE_CHECKING:
+    from sqlalchemy.orm import Session
+
 
 @deprecated(reason="Use Pool.get_pool() instead", version="2.2.4")
 @provide_session
diff --git a/airflow/api/common/mark_tasks.py b/airflow/api/common/mark_tasks.py
index 4d2df78e82db9..d71957f86b81d 100644
--- a/airflow/api/common/mark_tasks.py
+++ b/airflow/api/common/mark_tasks.py
@@ -18,15 +18,12 @@
 """Marks tasks APIs."""
 from __future__ import annotations
 
-from datetime import datetime
 from typing import TYPE_CHECKING, Collection, Iterable, Iterator, NamedTuple
 
 from sqlalchemy import or_, select
-from sqlalchemy.orm import Session as SASession, lazyload
+from sqlalchemy.orm import lazyload
 
-from airflow.models.dag import DAG
 from airflow.models.dagrun import DagRun
-from airflow.models.operator import Operator
 from airflow.models.taskinstance import TaskInstance
 from airflow.operators.subdag import SubDagOperator
 from airflow.utils import timezone
@@ -35,6 +32,14 @@
 from airflow.utils.state import DagRunState, State, TaskInstanceState
 from airflow.utils.types import DagRunType
 
+if TYPE_CHECKING:
+    from datetime import datetime
+
+    from sqlalchemy.orm import Session as SASession
+
+    from airflow.models.dag import DAG
+    from airflow.models.operator import Operator
+
 
 class _DagRunInfo(NamedTuple):
     logical_date: datetime
diff --git a/airflow/api/common/trigger_dag.py b/airflow/api/common/trigger_dag.py
index 01da7745c7bf3..a522b938df449 100644
--- a/airflow/api/common/trigger_dag.py
+++ b/airflow/api/common/trigger_dag.py
@@ -19,7 +19,7 @@
 from __future__ import annotations
 
 import json
-from datetime import datetime
+from typing import TYPE_CHECKING
 
 from airflow.exceptions import DagNotFound, DagRunAlreadyExists
 from airflow.models import DagBag, DagModel, DagRun
@@ -27,6 +27,9 @@
 from airflow.utils.state import DagRunState
 from airflow.utils.types import DagRunType
 
+if TYPE_CHECKING:
+    from datetime import datetime
+
 
 def _trigger_dag(
     dag_id: str,


---- SAMPLE #26 ----
Generated commit : [AIRFLOW-XXX] Fix invalid SQL error in the UI (#3736)
Ground-truth     : Use requires_access to check read permission on dag instead of checking it explicitly (#34940)
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / 0.1201 / 0.1876
Diff:
diff --git a/airflow/api_connexion/endpoints/dag_warning_endpoint.py b/airflow/api_connexion/endpoints/dag_warning_endpoint.py
index 367b0ae104571..c9d8207b0f65f 100644
--- a/airflow/api_connexion/endpoints/dag_warning_endpoint.py
+++ b/airflow/api_connexion/endpoints/dag_warning_endpoint.py
@@ -22,7 +22,6 @@
 from sqlalchemy import select
 
 from airflow.api_connexion import security
-from airflow.api_connexion.exceptions import PermissionDenied
 from airflow.api_connexion.parameters import apply_sorting, check_limit, format_parameters
 from airflow.api_connexion.schemas.dag_warning_schema import (
     DagWarningCollection,
@@ -40,7 +39,12 @@
     from airflow.api_connexion.types import APIResponse
 
 
-@security.requires_access([(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING)])
+@security.requires_access(
+    [
+        (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),
+        (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING),
+    ]
+)
 @format_parameters({"limit": check_limit})
 @provide_session
 def get_dag_warnings(
@@ -60,8 +64,6 @@ def get_dag_warnings(
     allowed_filter_attrs = ["dag_id", "warning_type", "message", "timestamp"]
     query = select(DagWarningModel)
     if dag_id:
-        if not get_airflow_app().appbuilder.sm.can_read_dag(dag_id, g.user):
-            raise PermissionDenied(detail=f"User not allowed to access this DAG: {dag_id}")
         query = query.where(DagWarningModel.dag_id == dag_id)
     else:
         readable_dags = get_airflow_app().appbuilder.sm.get_accessible_dag_ids(g.user)


---- SAMPLE #27 ----
Generated commit : rewrite the OUUS bottleneck into account for efficiency (#241)
Ground-truth     : Switch between Mozci prod/testing bindings with an env variable (#7415)

Co-authored-by: Eva Bardou <ebardou@teklia.com>
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / -0.0800 / 0.1781
Diff:
diff --git a/treeherder/services/pulse/consumers.py b/treeherder/services/pulse/consumers.py
index 91aecb8a44a..55862e8f786 100644
--- a/treeherder/services/pulse/consumers.py
+++ b/treeherder/services/pulse/consumers.py
@@ -42,9 +42,12 @@
     "exchange/hgpushes/v1.#",
 ]
 
-MOZCI_CLASSIFICATION_BINDINGS = [
+MOZCI_CLASSIFICATION_PRODUCTION_BINDINGS = [
     "exchange/taskcluster-queue/v1/task-completed.route.index.project.mozci.classification.#",
 ]
+MOZCI_CLASSIFICATION_TESTING_BINDINGS = [
+    "exchange/taskcluster-queue/v1/task-completed.route.index.project.mozci.testing.classification.#",
+]
 
 
 class PulseConsumer(ConsumerMixin):
@@ -171,7 +174,15 @@ class MozciClassificationConsumer(PulseConsumer):
     queue_suffix = env("PULSE_MOZCI_CLASSIFICATION_QUEUE_NAME", default="tasksclassification")
 
     def bindings(self):
-        return MOZCI_CLASSIFICATION_BINDINGS
+        mozci_env = env('PULSE_MOZCI_ENVIRONMENT', default='production')
+        if mozci_env == 'testing':
+            return MOZCI_CLASSIFICATION_TESTING_BINDINGS
+
+        if mozci_env != 'production':
+            logger.warning(
+                f'PULSE_MOZCI_ENVIRONMENT should be testing or production not {mozci_env}, defaulting to production'
+            )
+        return MOZCI_CLASSIFICATION_PRODUCTION_BINDINGS
 
     @newrelic.agent.background_task(
         name='pulse-listener-tasks-classification.on_message', group='Pulse Listener'
@@ -228,7 +239,16 @@ def bindings(self):
         if self.source.get('tasks'):
             rv += TASKCLUSTER_TASK_BINDINGS
         if self.source.get('mozci-classification'):
-            rv += MOZCI_CLASSIFICATION_BINDINGS
+            mozci_env = env('PULSE_MOZCI_ENVIRONMENT', default='production')
+            if mozci_env == 'testing':
+                rv += MOZCI_CLASSIFICATION_TESTING_BINDINGS
+            else:
+                if mozci_env != 'production':
+                    logger.warning(
+                        f'PULSE_MOZCI_ENVIRONMENT should be testing or production not {mozci_env}, defaulting to production'
+                    )
+                rv += MOZCI_CLASSIFICATION_PRODUCTION_BINDINGS
+
         return rv
 
     @newrelic.agent.background_task(name='pulse-joint-listener.on_message', group='Pulse Listener')


---- SAMPLE #28 ----
Generated commit : Fixing issue with filtering API (#707)
Ground-truth     : Fix bug 1408637: EntityQuerySet.prefetch_resources_translations() is deprecated (#734)
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.1250 / 0.0725 / 0.1463 / 0.1922
Diff:
diff --git a/pontoon/base/views.py b/pontoon/base/views.py
index b9e6faedd1..6f5cba5760 100755
--- a/pontoon/base/views.py
+++ b/pontoon/base/views.py
@@ -173,7 +173,8 @@ def _get_entities_list(locale, project, form):
     """
     entities = (
         Entity.objects.filter(pk__in=form.cleaned_data['entity_ids'])
-        .prefetch_resources_translations(locale)
+        .prefetch_related('resource')
+        .prefetch_translations(locale)
         .distinct()
         .order_by('order')
     )
@@ -302,7 +303,8 @@ def batch_edit_translations(request):
 
     entities = (
         Entity.objects.filter(pk__in=entity_pks)
-        .prefetch_resources_translations(locale)
+        .prefetch_related('resource')
+        .prefetch_translations(locale)
     )
 
     if not entities.exists():


---- SAMPLE #29 ----
Generated commit : Remove unused imports
Ground-truth     : Adjust the sequence of import statements based on alphabetical order
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.1538 / 0.0538 / 0.0648 / 0.2036
Diff:
diff --git a/knowledge_repo/utils/encoding.py b/knowledge_repo/utils/encoding.py
index b0188ae9d..9d56f2547 100644
--- a/knowledge_repo/utils/encoding.py
+++ b/knowledge_repo/utils/encoding.py
@@ -1,6 +1,6 @@
+import logging
 import os
 import sys
-import logging
 
 
 __all__ = ['encode', 'decode']
diff --git a/knowledge_repo/utils/exec_code.py b/knowledge_repo/utils/exec_code.py
index b0e12f0dd..bba3bc3d6 100644
--- a/knowledge_repo/utils/exec_code.py
+++ b/knowledge_repo/utils/exec_code.py
@@ -1,5 +1,5 @@
-import sys
 import importlib
+import sys
 import time
 
 
diff --git a/knowledge_repo/utils/git.py b/knowledge_repo/utils/git.py
index f55ef2aa7..5087a3486 100644
--- a/knowledge_repo/utils/git.py
+++ b/knowledge_repo/utils/git.py
@@ -1,4 +1,3 @@
-import errno
 import os
 import shutil
 import tempfile
diff --git a/knowledge_repo/utils/import_submodules.py b/knowledge_repo/utils/import_submodules.py
index c70acf652..738d4b518 100644
--- a/knowledge_repo/utils/import_submodules.py
+++ b/knowledge_repo/utils/import_submodules.py
@@ -1,6 +1,6 @@
-import sys
 import importlib
 import pkgutil
+import sys
 
 
 def import_submodules(package_name):
diff --git a/knowledge_repo/utils/types.py b/knowledge_repo/utils/types.py
index 98098caa8..3232bdcb8 100644
--- a/knowledge_repo/utils/types.py
+++ b/knowledge_repo/utils/types.py
@@ -1,5 +1,3 @@
-import sys
-
 from sqlalchemy import Text
 from sqlalchemy.dialects.mysql import MEDIUMTEXT
 


---- SAMPLE #30 ----
Generated commit : Detach celery settings fixes
Ground-truth     : Allow for --logfile to work even if not detached, also set all default log files to None (stderr)
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.1818 / 0.0301 / -0.0838 / 0.1755
Diff:
diff --git a/celery/bin/celerybeat.py b/celery/bin/celerybeat.py
index 71b10156582..c277388b004 100644
--- a/celery/bin/celerybeat.py
+++ b/celery/bin/celerybeat.py
@@ -72,7 +72,7 @@
             help="Path to the schedule database. The extension \
                     '.db' will be appended to the filename. Default: %s" % (
                     conf.CELERYBEAT_SCHEDULE_FILENAME)),
-    optparse.make_option('-f', '--logfile', default=conf.CELERYBEAT_LOG_FILE,
+    optparse.make_option('-f', '--logfile', default=EAT_LOG_FILE,
             action="store", dest="logfile",
             help="Path to log file."),
     optparse.make_option('-l', '--loglevel',
@@ -115,8 +115,6 @@ def run_clockservice(detach=False, loglevel=conf.CELERYBEAT_LOG_LEVEL,
     # Setup logging
     if not isinstance(loglevel, int):
         loglevel = conf.LOG_LEVELS[loglevel.upper()]
-    if not detach:
-        logfile = None # log to stderr when not running in the background.
 
     # Run the worker init handler.
     # (Usually imports task modules and such.)
diff --git a/celery/bin/celeryd.py b/celery/bin/celeryd.py
index 047244c7112..85132e53940 100644
--- a/celery/bin/celeryd.py
+++ b/celery/bin/celeryd.py
@@ -165,8 +165,6 @@ def run_worker(concurrency=conf.CELERYD_CONCURRENCY, detach=False,
     # Setup logging
     if not isinstance(loglevel, int):
         loglevel = conf.LOG_LEVELS[loglevel.upper()]
-    if not detach:
-        logfile = None # log to stderr when not running in the background.
 
     if discard:
         discarded_count = discard_all()
diff --git a/celery/conf.py b/celery/conf.py
index 6ade4a8cf7a..babb49007d6 100644
--- a/celery/conf.py
+++ b/celery/conf.py
@@ -30,15 +30,15 @@
     "CELERYD_CONCURRENCY": 0, # defaults to cpu count
     "CELERYD_LOG_FORMAT": DEFAULT_P_LOG_FMT,
     "CELERYD_LOG_LEVEL": "WARN",
-    "CELERYD_LOG_FILE": "celeryd.log",
+    "CELERYD_LOG_FILE": None, # stderr
     "CELERYD_PID_FILE": "celeryd.pid",
     "CELERYBEAT_SCHEDULE_FILENAME": "celerybeat-schedule",
     "CELERYBEAT_MAX_LOOP_INTERVAL": 5 * 60, # five minutes.
     "CELERYBEAT_LOG_LEVEL": "INFO",
-    "CELERYBEAT_LOG_FILE": "celerybeat.log",
+    "CELERYBEAT_LOG_FILE": None, # stderr
     "CELERYBEAT_PID_FILE": "celerybeat.pid",
     "CELERYMON_LOG_LEVEL": "INFO",
-    "CELERYMON_LOG_FILE": "celerymon.log",
+    "CELERYMON_LOG_FILE": None, # stderr
     "CELERYMON_PID_FILE": "celerymon.pid",
     "CELERYMON_LOG_FORMAT": DEFAULT_LOG_FMT,
     "CELERY_BROADCAST_QUEUE": "celeryctl",


---- SAMPLE #31 ----
Generated commit : Add code to test the number of events that were accidentally consumed
Ground-truth     : Cosmetic: Add in_review.py script
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0170 / 0.1111 / 0.1042 / 0.2472 / 0.1763
Diff:
diff --git a/scripts/in_review.py b/scripts/in_review.py
new file mode 100755
index 00000000000..9154d0b8565
--- /dev/null
+++ b/scripts/in_review.py
@@ -0,0 +1,351 @@
+#!/usr/bin/env python
+import datetime
+import subprocess
+import sys
+import textwrap
+import urllib
+
+import requests
+
+
+BUGZILLA_API_URL = 'https://bugzilla.mozilla.org/rest/'
+BUGZILLA_PRODUCT = 'support.mozilla.org'
+
+QUARTERS = {
+    1: [(1, 1), (3, 31)],
+    2: [(4, 1), (6, 30)],
+    3: [(7, 1), (9, 30)],
+    4: [(10, 1), (12, 31)]
+}
+
+USAGE = 'Usage: in_review.py <YEAR> [<QUARTER>]'
+HEADER = 'in_review.py: find out what happened year or quarter!'
+
+all_people = set()
+
+
+def wrap(text, indent='    ', subsequent='    '):
+    text = text.split('\n\n')
+    text = [textwrap.fill(part, expand_tabs=True, initial_indent=indent,
+                          subsequent_indent=subsequent)
+            for part in text]
+    return '\n\n'.join(text)
+
+
+def fetch(url):
+    resp = requests.get(url)
+    if resp.status_code != 200:
+        raise Exception(resp.text)
+
+    json_data = resp.json()
+
+    if json_data.get('error'):
+        raise Exception('ERROR ' + repr(json_data))
+
+    return json_data
+
+
+def fetch_bugs(params):
+    url = BUGZILLA_API_URL + '/bug' + '?' + urllib.urlencode(params)
+    return fetch(url)
+
+
+def fetch_bug_history(bugid):
+    url = BUGZILLA_API_URL + ('/bug/%d/history' % bugid)
+    return fetch(url)
+
+
+def fetch_bug_comments(bugid):
+    url = BUGZILLA_API_URL + ('/bug/%d/comment' % bugid)
+    return fetch(url)
+
+
+def print_bugzilla_stats(from_date, to_date):
+    # ------------------------------------------------
+    # Bug creation stats
+    # ------------------------------------------------
+    params = {
+        'product': BUGZILLA_PRODUCT,
+        'f1': 'creation_ts',
+        'o1': 'greaterthaneq',
+        'v1': from_date.strftime('%Y-%m-%d'),
+        'f2': 'creation_ts',
+        'o2': 'lessthan',
+        'v2': to_date.strftime('%Y-%m-%d')
+    }
+
+    json_data = fetch_bugs(params)
+
+    creation_count = len(json_data['bugs'])
+
+    creators = {}
+    for bug in json_data['bugs']:
+        creator = bug['creator_detail']['real_name']
+        if not creator:
+            creator = bug['creator'].split('@')[0]
+        creators[creator] = creators.get(creator, 0) + 1
+        all_people.add(creator)
+
+    print 'Bugs created: %s' % creation_count
+    print 'Creators: %s' % len(creators)
+    print ''
+    creators = sorted(creators.items(), reverse=True, key=lambda item: item[1])
+    for person, count in creators:
+        print ' %34s : %s' % (person[:30], count)
+    print ''
+
+    # ------------------------------------------------
+    # Bug resolution stats
+    # ------------------------------------------------
+    params = {
+        'product': BUGZILLA_PRODUCT,
+        'f1': 'cf_last_resolved',
+        'o1': 'greaterthaneq',
+        'v1': from_date.strftime('%Y-%m-%d'),
+        'f2': 'cf_last_resolved',
+        'o2': 'lessthan',
+        'v2': to_date.strftime('%Y-%m-%d')
+    }
+
+    json_data = fetch_bugs(params)
+
+    resolved_count = len(json_data['bugs'])
+    resolved_map = {}
+    resolvers = {}
+    traceback_bugs = []
+    research_bugs = []
+    tracker_bugs = []
+    commenters = {}
+
+    for bug in json_data['bugs']:
+        summary = bug['summary'].lower()
+        if summary.startswith('[traceback]'):
+            traceback_bugs.append(bug)
+        elif summary.startswith('[research]'):
+            research_bugs.append(bug)
+        elif summary.startswith('[tracker]'):
+            tracker_bugs.append(bug)
+
+        history = fetch_bug_history(bug['id'])
+
+        resolution = bug['resolution']
+        resolved_map[resolution] = resolved_map.get(resolution, 0) + 1
+        assigned = bug['assigned_to_detail']['real_name']
+        if not assigned:
+            assigned = bug['assigned_to'].split('@')[0]
+
+        if 'nobody' in assigned.lower():
+            # If no one was assigned, we give "credit" to whoever
+            # triaged the bug. We go through the history in reverse
+            # order because the "resolver" is the last person to
+            # resolve the bug.
+            for item in reversed(history['bugs'][0]['history']):
+                # See if this item in the history is the resolving event.
+                # If it is, then we know who resolved the bug and we
+                # can stop looking at history.
+                changes = [change for change in item['changes']
+                           if change['field_name'] == 'status' and
+                           change['added'] == 'RESOLVED']
+
+                if not changes:
+                    continue
+
+                assigned = item['who']
+                break
+
+        if assigned:
+            if '@' in assigned:
+                assigned = assigned.split('@')[0]
+
+            resolvers[assigned] = resolvers.get(assigned, 0) + 1
+            all_people.add(assigned)
+
+        # Now get all the commenters
+        comments = fetch_bug_comments(bug['id'])
+        # The Bugzilla REST api has some interesting things about it.
+        for comment in comments['bugs'][str(bug['id'])]['comments']:
+            commenter = comment['author']
+            if '@' in commenter:
+                commenter = commenter.split('@')[0]
+
+            commenters[commenter] = commenters.get(commenter, 0) + 1
+            all_people.add(commenter)
+
+    print 'Bugs resolved: %s' % resolved_count
+    print ''
+    for resolution, count in resolved_map.items():
+        print ' %34s : %s' % (resolution, count)
+
+    print ''
+    for title, count in [('Tracebacks', len(traceback_bugs)),
+                         ('Research', len(research_bugs)),
+                         ('Tracker', len(tracker_bugs))]:
+        print ' %34s : %s' % (title, count)
+
+    print ''
+    print 'Research bugs: %s' % len(research_bugs)
+    print ''
+    for bug in research_bugs:
+        print wrap('%s: %s' % (bug['id'], bug['summary']),
+                   subsequent='        ')
+
+    print ''
+    print 'Tracker bugs: %s' % len(tracker_bugs)
+    print ''
+    for bug in tracker_bugs:
+        print wrap('%s: %s' % (bug['id'], bug['summary']),
+                   subsequent='        ')
+
+    print ''
+    print 'Resolvers: %s' % len(resolvers)
+    print ''
+    resolvers = sorted(resolvers.items(), reverse=True,
+                       key=lambda item: item[1])
+    for person, count in resolvers:
+        print ' %34s : %s' % (person[:30], count)
+
+    print ''
+    print 'Commenters: %s' % len(commenters)
+    print ''
+    commenters = sorted(commenters.items(), reverse=True,
+                        key=lambda item: item[1])
+    for person, count in commenters:
+        print ' %34s : %s' % (person[:30], count)
+
+
+def git(*args):
+    return subprocess.check_output(args)
+
+
+def print_git_stats(from_date, to_date):
+    all_commits = subprocess.check_output([
+        'git', 'log',
+        '--after=' + from_date.strftime('%Y-%m-%d'),
+        '--before=' + to_date.strftime('%Y-%m-%d'),
+        '--format=%H'
+    ])
+
+    all_commits = all_commits.splitlines()
+
+    # Person -> # commits
+    committers = {}
+
+    # Person -> (# files changed, # inserted, # deleted)
+    changes = {}
+
+    for commit in all_commits:
+        author = git('git', 'log', '--format=%an',
+                     '{0}~..{1}'.format(commit, commit))
+
+        author = author.strip()
+        # FIXME - this is lame. what's going on is that there are
+        # merge commits which have multiple authors, so we just grab
+        # the second one.
+        if '\n' in author:
+            author = author.splitlines()[1]
+
+        committers[author] = committers.get(author, 0) + 1
+        all_people.add(author)
+
+        diff_data = git('git', 'diff', '--numstat', '--find-copies-harder',
+                        '{0}~..{1}'.format(commit, commit))
+        total_added = 0
+        total_deleted = 0
+        total_files = 0
+
+        for line in diff_data.splitlines():
+            added, deleted, fn = line.split('\t')
+            if fn.startswith('vendor/'):
+                continue
+            if added != '-':
+                total_added += int(added)
+            if deleted != '-':
+                total_deleted += int(deleted)
+            total_files += 1
+
+        old_changes = changes.get(author, (0, 0, 0))
+        changes[author] = (
+            old_changes[0] + total_added,
+            old_changes[1] + total_deleted,
+            old_changes[2] + total_files
+        )
+
+    print 'Total commits:', len(all_commits)
+    print ''
+
+    committers = sorted(
+        committers.items(), key=lambda item: item[1], reverse=True)
+    for person, count in committers:
+        print '  %20s : %5s  (+%s, -%s, files %s)' % (
+            person, count,
+            changes[person][0], changes[person][1], changes[person][2])
+        all_people.add(person)
+
+    # This is goofy summing, but whatevs.
+    print ''
+    print 'Total lines added:', sum([item[0] for item in changes.values()])
+    print 'Total lines deleted:', sum([item[1] for item in changes.values()])
+    print 'Total files changed:', sum([item[2] for item in changes.values()])
+
+
+def print_all_people():
+    # We do this sorting thing to make it a little easier to suss out
+    # duplicates since we're pulling names from three different forms
+    # between Bugzilla and git. You're still going to have to go
+    # through it by hand to remove duplicates.
+    people = sorted(all_people, key=lambda a: a.decode('utf-8').lower())
+
+    for person in people:
+        print '    ' + person
+
+
+def print_header(text):
+    print ''
+    print text
+    print '=' * len(text)
+    print ''
+
+
+def main(argv):
+    # XXX: This helps debug bugzilla xmlrpc bits.
+    # logging.basicConfig(level=logging.DEBUG)
+
+    if len(argv) < 1:
+        print USAGE
+        print 'Error: Must specify year or year and quarter. Examples:'
+        print 'in_review.py 2014'
+        print 'in_review.py 2014 1'
+        return 1
+
+    print HEADER
+
+    year = int(argv[0])
+    if len(argv) == 1:
+        from_date = datetime.date(year, 1, 1)
+        to_date = datetime.date(year, 12, 31)
+        print_header('Year %s (%s -> %s)' % (year, from_date, to_date))
+
+    else:
+        quarter = int(argv[1])
+        quarter_dates = QUARTERS[quarter]
+
+        from_date = datetime.date(
+            year, quarter_dates[0][0], quarter_dates[0][1])
+        to_date = datetime.date(
+            year, quarter_dates[1][0], quarter_dates[1][1])
+
+        print_header('Quarter %sq%s (%s -> %s)' % (
+            year, quarter, from_date, to_date))
+
+    print_header('Bugzilla')
+    print_bugzilla_stats(from_date, to_date)
+
+    print_header('git')
+    print_git_stats(from_date, to_date)
+
+    print_header('Everyone')
+    print_all_people()
+
+
+if __name__ == '__main__':
+    sys.exit(main(sys.argv[1:]))


---- SAMPLE #32 ----
Generated commit : Fix bug in cloud store (#3621)

This PR fixes that issue were caused by missing external changes to
the old behaviour of this commit.
Ground-truth     : [Storage] gsutil rsync failing to sync source with dangling symlink (#2165)

* update_gsutil_rsync_cmd

* fixes

* format

* nit

* nit
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0094 / 0.0465 / 0.0469 / 0.0221 / 0.3404
Diff:
diff --git a/sky/cloud_stores.py b/sky/cloud_stores.py
index 89f8e02cb5b..33aab44501d 100644
--- a/sky/cloud_stores.py
+++ b/sky/cloud_stores.py
@@ -133,7 +133,7 @@ def is_directory(self, url: str) -> bool:
     def make_sync_dir_command(self, source: str, destination: str) -> str:
         """Downloads a directory using gsutil."""
         download_via_gsutil = (
-            f'{self._GSUTIL} -m rsync -r {source} {destination}')
+            f'{self._GSUTIL} -m rsync -e -r {source} {destination}')
         all_commands = [self._GET_GSUTIL]
         all_commands.append(download_via_gsutil)
         return ' && '.join(all_commands)
diff --git a/sky/data/storage.py b/sky/data/storage.py
index bb2c9ab2ebb..7ee08fbb674 100644
--- a/sky/data/storage.py
+++ b/sky/data/storage.py
@@ -1507,14 +1507,15 @@ def batch_gsutil_rsync(self,
 
         def get_file_sync_command(base_dir_path, file_names):
             sync_format = '|'.join(file_names)
-            sync_command = (f'gsutil -m rsync -x \'^(?!{sync_format}$).*\' '
+            sync_command = (f'gsutil -m rsync -e -x \'^(?!{sync_format}$).*\' '
                             f'{base_dir_path} gs://{self.name}')
             return sync_command
 
         def get_dir_sync_command(src_dir_path, dest_dir_name):
             # we exclude .git directory from the sync
-            sync_command = (f'gsutil -m rsync -r -x \'.git/*\' {src_dir_path} '
-                            f'gs://{self.name}/{dest_dir_name}')
+            sync_command = (
+                f'gsutil -m rsync -e -r -x \'.git/*\' {src_dir_path} '
+                f'gs://{self.name}/{dest_dir_name}')
             return sync_command
 
         # Generate message for upload


---- SAMPLE #33 ----
Generated commit : Add an integration test to check for hostname address (#2254)
Ground-truth     : Handle FxA change event without emailaddress
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / 0.1556 / 0.1736
Diff:
diff --git a/privaterelay/tests/views_tests.py b/privaterelay/tests/views_tests.py
index 4cffe86db7..66f78b7cfb 100644
--- a/privaterelay/tests/views_tests.py
+++ b/privaterelay/tests/views_tests.py
@@ -127,6 +127,27 @@ def test_update_all_data_conflict(self):
         assert sa2.extra_data == extra_data
         assert ea2.email == "user2@example.com"
 
+    def test_update_all_data_no_email_address(self):
+        user = baker.make(User)
+        sa = baker.make(
+            SocialAccount,
+            user=user,
+            provider="fxa",
+            extra_data=json.loads('{"test": "test"}'),
+        )
+        new_extra_data = json.loads('{"test": "updated"}')
+        new_email = "newemail@example.com"
+
+        response = _update_all_data(sa, new_extra_data, new_email)
+
+        assert response.status_code == 202
+
+        sa.refresh_from_db()
+        assert sa.extra_data == new_extra_data
+
+        ea = sa.user.emailaddress_set.get()
+        assert ea.email == new_email
+
 
 @pytest.mark.django_db
 def test_logout_page(client, settings):
diff --git a/privaterelay/views.py b/privaterelay/views.py
index 41da4fee99..06790d221e 100644
--- a/privaterelay/views.py
+++ b/privaterelay/views.py
@@ -351,8 +351,11 @@ def _update_all_data(
             social_account.user.email = new_email
             social_account.user.save()
             email_address_record = social_account.user.emailaddress_set.first()
-            email_address_record.email = new_email
-            email_address_record.save()
+            if email_address_record:
+                email_address_record.email = new_email
+                email_address_record.save()
+            else:
+                social_account.user.emailaddress_set.create(email=new_email)
             return HttpResponse("202 Accepted", status=202)
     except IntegrityError as e:
         sentry_sdk.capture_exception(e)


---- SAMPLE #34 ----
Generated commit : Add another possible to send the message to the email address
Ground-truth     : fix reply to header name for commbadge (bug 1075233)
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0189 / 0.1000 / 0.0543 / -0.1090 / 0.1802
Diff:
diff --git a/mkt/comm/utils_mail.py b/mkt/comm/utils_mail.py
index d99e20b79d3..1c165a47691 100644
--- a/mkt/comm/utils_mail.py
+++ b/mkt/comm/utils_mail.py
@@ -104,7 +104,7 @@ def email_recipients(recipients, note, template=None):
                         get_mail_context(note), recipient_list=[email],
                         from_email=settings.MKT_REVIEWERS_EMAIL,
                         perm_setting='app_reviewed',
-                        headers={'reply_to': reply_to})
+                        headers={'Reply-To': reply_to})
 
 
 def get_mail_context(note):


---- SAMPLE #35 ----
Generated commit : Fix missing error message when loading from env
Ground-truth     : Using the latest string format for embedded variables
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / 0.2705 / 0.1856
Diff:
diff --git a/knowledge_repo/app/deploy/gunicorn.py b/knowledge_repo/app/deploy/gunicorn.py
index 47a9d5846..456c1b3f5 100644
--- a/knowledge_repo/app/deploy/gunicorn.py
+++ b/knowledge_repo/app/deploy/gunicorn.py
@@ -38,7 +38,7 @@ def load_config(self):
         # Update the configuration with the options specified
         # via KnowledgeDeployer
         options = {
-            'bind': '{}:{}'.format(self.host, self.port),
+            'bind': f'{self.host}:{self.port}',
             'workers': self.workers,
             'timeout': self.timeout,
         }
@@ -63,13 +63,13 @@ def load_config_from_filename(self, filename):
         """
 
         if not os.path.exists(filename):
-            raise RuntimeError("%r doesn't exist" % filename)
+            raise RuntimeError(f"{filename} doesn't exist")
 
         ext = os.path.splitext(filename)[1]
 
         try:
             module_name = '__config__'
-            if ext in [".py", ".pyc"]:
+            if ext in ['.py', '.pyc']:
                 spec = importlib.util.spec_from_file_location(
                     module_name, filename)
             else:
@@ -83,7 +83,7 @@ def load_config_from_filename(self, filename):
             sys.modules[module_name] = mod
             spec.loader.exec_module(mod)
         except Exception:
-            print("Failed to read config file: %s" % filename, file=sys.stderr)
+            print(f'Failed to read config file: {filename}', file=sys.stderr)
             traceback.print_exc()
             sys.stderr.flush()
             sys.exit(1)
@@ -97,7 +97,7 @@ def load_config_from_filename(self, filename):
             try:
                 self.cfg.set(k.lower(), v)
             except Exception:
-                print("Invalid value for %s: %s\n" % (k, v), file=sys.stderr)
+                print(f'Invalid value for {k}: {v}\n', file=sys.stderr)
                 sys.stderr.flush()
                 raise
 


---- SAMPLE #36 ----
Generated commit : Docs
Ground-truth     : Add deleted address metric on profile
and create migration for field changes on profile
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / -0.1077 / 0.1691
Diff:
diff --git a/emails/migrations/0029_profile_add_deleted_metric_and_changeserver_storage_default.py b/emails/migrations/0029_profile_add_deleted_metric_and_changeserver_storage_default.py
new file mode 100644
index 0000000000..d3697f3814
--- /dev/null
+++ b/emails/migrations/0029_profile_add_deleted_metric_and_changeserver_storage_default.py
@@ -0,0 +1,29 @@
+# Generated by Django 2.2.24 on 2021-10-19 15:38
+
+from django.db import migrations, models
+import emails.models
+
+
+class Migration(migrations.Migration):
+
+    dependencies = [
+        ('emails', '0028_copy_subdomain_to_registeredsubdomain'),
+    ]
+
+    operations = [
+        migrations.AddField(
+            model_name='profile',
+            name='num_email_blocked_in_deleted_address',
+            field=models.PositiveIntegerField(default=0),
+        ),
+        migrations.AddField(
+            model_name='profile',
+            name='num_email_forwarded_in_deleted_address',
+            field=models.PositiveIntegerField(default=0),
+        ),
+        migrations.AlterField(
+            model_name='profile',
+            name='server_storage',
+            field=models.BooleanField(default=emails.models.default_server_storage),
+        ),
+    ]
diff --git a/emails/models.py b/emails/models.py
index d4886ed510..3839ed9232 100644
--- a/emails/models.py
+++ b/emails/models.py
@@ -79,6 +79,8 @@ class Profile(models.Model):
     last_hard_bounce = models.DateTimeField(
         blank=True, null=True, db_index=True
     )
+    num_email_forwarded_in_deleted_address = models.PositiveIntegerField(default=0)
+    num_email_blocked_in_deleted_address = models.PositiveIntegerField(default=0)
     subdomain = models.CharField(
         blank=True, null=True, unique=True, max_length=63, db_index=True,
         validators=[valid_available_subdomain]


---- SAMPLE #37 ----
Generated commit : Fix bug in the loading of python module (#214)
Ground-truth     : Ensure files opened by Python are closed when loading modules
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0215 / 0.2105 / 0.1515 / 0.2995 / 0.7035
Diff:
diff --git a/framework/pym/play/cmdloader.py b/framework/pym/play/cmdloader.py
index b0f10fef85..80138bd0d5 100644
--- a/framework/pym/play/cmdloader.py
+++ b/framework/pym/play/cmdloader.py
@@ -36,5 +36,10 @@ def _load_cmd_from(self, mod):
 
 def load_python_module(name, location):
     mod_desc = imp.find_module(name, [location])
-    return imp.load_module(name, mod_desc[0], mod_desc[1], mod_desc[2])
+    mod_file = mod_desc[0]
+    try:
+        return imp.load_module(name, mod_desc[0], mod_desc[1], mod_desc[2])
+    finally:
+        if mod_file is not None and not mod_file.closed:
+            mod_file.close()
 


---- SAMPLE #38 ----
Generated commit : Add tests for update cherry-pick conflict (#2272)
Ground-truth     : added a test for the Command() in update_phone_remaining_stats.py for the remaining coverage
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0162 / 0.2500 / 0.0870 / 0.1489 / 0.1962
Diff:
diff --git a/privaterelay/tests/mgmt_update_phone_remaining_stats_tests.py b/privaterelay/tests/mgmt_update_phone_remaining_stats_tests.py
index 5837a0164f..7a1ac53cc7 100644
--- a/privaterelay/tests/mgmt_update_phone_remaining_stats_tests.py
+++ b/privaterelay/tests/mgmt_update_phone_remaining_stats_tests.py
@@ -304,3 +304,15 @@ def test_phone_subscriber_with_subscription_end_date_after_reset_phone_limits_up
     assert num_profiles_updated == 1
     relay_number.refresh_from_db()
     assert relay_number.remaining_texts == settings.MAX_TEXTS_PER_BILLING_CYCLE
+
+
+@pytest.mark.django_db
+def test_update_phones_command(capsys):
+    call_command("update_phone_remaining_stats")
+    out, err = capsys.readouterr()
+
+    out = out.split(" ")
+    numProfiles, numReset = int(out[2]), int(out[4])
+
+    assert numProfiles == 0
+    assert numReset == 0


---- SAMPLE #39 ----
Generated commit : [AIRFLOW-XXX] Fix AttributeError in the correct version of `spark()` (#4337)
Ground-truth     : Databricks: fix provider name in the User-Agent string (#25873)

`databricks-aiflow` -> `databricks-airflow` (:facepalm:)
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0331 / 0.2308 / 0.2012 / 0.0669 / 0.7446
Diff:
diff --git a/airflow/providers/databricks/hooks/databricks_base.py b/airflow/providers/databricks/hooks/databricks_base.py
index a68fe5aaa4526..1f8680e41e42b 100644
--- a/airflow/providers/databricks/hooks/databricks_base.py
+++ b/airflow/providers/databricks/hooks/databricks_base.py
@@ -144,7 +144,7 @@ def user_agent_value(self) -> str:
         python_version = platform.python_version()
         system = platform.system().lower()
         ua_string = (
-            f"databricks-aiflow/{version} _/0.0.0 python/{python_version} os/{system} "
+            f"databricks-airflow/{version} _/0.0.0 python/{python_version} os/{system} "
             f"airflow/{__version__} operator/{self.caller}"
         )
         return ua_string


---- SAMPLE #40 ----
Generated commit : Update settings_base to use new CA & default (#172)
Ground-truth     : update CRONJOB_LOCK_PREFIX and ES_INDEXES to use paymentsalt.
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0507 / 0.3158 / 0.3549 / 0.1248 / 0.1813
Diff:
diff --git a/sites/paymentsalt/settings_base.py b/sites/paymentsalt/settings_base.py
index 33ff19ccf33..6440c5b1de7 100644
--- a/sites/paymentsalt/settings_base.py
+++ b/sites/paymentsalt/settings_base.py
@@ -106,14 +106,14 @@
 
 RESPONSYS_ID = private.RESPONSYS_ID
 
-CRONJOB_LOCK_PREFIX = 'marketplace-stage'
+CRONJOB_LOCK_PREFIX = 'marketplace-paymentsalt'
 
 GOOGLE_ANALYTICS_CREDENTIALS = private.GOOGLE_ANALYTICS_CREDENTIALS
 GOOGLE_API_CREDENTIALS = private.GOOGLE_API_CREDENTIALS
 
 ES_HOSTS = splitstrip(private.ES_HOSTS)
 ES_URLS = ['http://%s' % h for h in ES_HOSTS]
-ES_INDEXES = dict((k, '%s_stage' % v) for k, v in ES_INDEXES.items())
+ES_INDEXES = dict((k, '%s_paymentsalt' % v) for k, v in ES_INDEXES.items())
 
 STATSD_HOST = private.STATSD_HOST
 STATSD_PORT = private.STATSD_PORT


---- SAMPLE #41 ----
Generated commit : Allows bootstrap for creating queries
Ground-truth     : Add more type annotations to AWS hooks (#10671)

Co-authored-by: Kamil Bregua <kamil.bregula@polidea.com>
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / -0.1256 / 0.1875
Diff:
diff --git a/airflow/providers/amazon/aws/hooks/aws_dynamodb.py b/airflow/providers/amazon/aws/hooks/aws_dynamodb.py
index f197aa7aece7f..2062e528bc974 100644
--- a/airflow/providers/amazon/aws/hooks/aws_dynamodb.py
+++ b/airflow/providers/amazon/aws/hooks/aws_dynamodb.py
@@ -20,6 +20,8 @@
 """
 This module contains the AWS DynamoDB hook
 """
+from typing import Iterable, List, Optional
+
 from airflow.exceptions import AirflowException
 from airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook
 
@@ -40,12 +42,15 @@ class AwsDynamoDBHook(AwsBaseHook):
     :type table_name: str
     """
 
-    def __init__(self, *args, table_keys=None, table_name=None, **kwargs):
+    def __init__(
+        self, *args, table_keys: Optional[List] = None, table_name: Optional[str] = None, **kwargs
+    ) -> None:
         self.table_keys = table_keys
         self.table_name = table_name
-        super().__init__(resource_type='dynamodb', *args, **kwargs)
+        kwargs["resource_type"] = "dynamodb"
+        super().__init__(*args, **kwargs)
 
-    def write_batch_data(self, items):
+    def write_batch_data(self, items: Iterable):
         """
         Write batch items to DynamoDB table with provisioned throughout capacity.
         """
@@ -58,5 +63,5 @@ def write_batch_data(self, items):
             return True
         except Exception as general_error:
             raise AirflowException(
-                'Failed to insert items in dynamodb, error: {error}'.format(error=str(general_error))
+                "Failed to insert items in dynamodb, error: {error}".format(error=str(general_error))
             )
diff --git a/airflow/providers/amazon/aws/hooks/base_aws.py b/airflow/providers/amazon/aws/hooks/base_aws.py
index 674beff50b4b6..8b1a5a7639aad 100644
--- a/airflow/providers/amazon/aws/hooks/base_aws.py
+++ b/airflow/providers/amazon/aws/hooks/base_aws.py
@@ -39,7 +39,7 @@
 
 
 class _SessionFactory(LoggingMixin):
-    def __init__(self, conn: Connection, region_name: str, config: Config):
+    def __init__(self, conn: Connection, region_name: Optional[str], config: Config) -> None:
         super().__init__()
         self.conn = conn
         self.region_name = region_name
@@ -191,7 +191,7 @@ def _assume_role_with_saml(
             RoleArn=role_arn, PrincipalArn=principal_arn, SAMLAssertion=saml_assertion, **assume_role_kwargs
         )
 
-    def _fetch_saml_assertion_using_http_spegno_auth(self, saml_config: Dict[str, Any]):
+    def _fetch_saml_assertion_using_http_spegno_auth(self, saml_config: Dict[str, Any]) -> str:
         import requests
 
         # requests_gssapi will need paramiko > 2.6 since you'll need
@@ -285,9 +285,9 @@ def __init__(
         self.config = config
 
         if not (self.client_type or self.resource_type):
-            raise AirflowException('Either client_type or resource_type' ' must be provided.')
+            raise AirflowException('Either client_type or resource_type must be provided.')
 
-    def _get_credentials(self, region_name):
+    def _get_credentials(self, region_name: Optional[str]) -> Tuple[boto3.session.Session, Optional[str]]:
 
         if not self.aws_conn_id:
             session = boto3.session.Session(region_name=region_name)
@@ -327,7 +327,9 @@ def _get_credentials(self, region_name):
         session = boto3.session.Session(region_name=region_name)
         return session, None
 
-    def get_client_type(self, client_type, region_name=None, config=None):
+    def get_client_type(
+        self, client_type: str, region_name: Optional[str] = None, config: Optional[Config] = None,
+    ) -> boto3.client:
         """Get the underlying boto3 client using boto3 session"""
         session, endpoint_url = self._get_credentials(region_name)
 
@@ -338,7 +340,9 @@ def get_client_type(self, client_type, region_name=None, config=None):
 
         return session.client(client_type, endpoint_url=endpoint_url, config=config, verify=self.verify)
 
-    def get_resource_type(self, resource_type, region_name=None, config=None):
+    def get_resource_type(
+        self, resource_type: str, region_name: Optional[str] = None, config: Optional[Config] = None,
+    ) -> boto3.resource:
         """Get the underlying boto3 resource using boto3 session"""
         session, endpoint_url = self._get_credentials(region_name)
 
@@ -350,7 +354,7 @@ def get_resource_type(self, resource_type, region_name=None, config=None):
         return session.resource(resource_type, endpoint_url=endpoint_url, config=config, verify=self.verify)
 
     @cached_property
-    def conn(self):
+    def conn(self) -> Union[boto3.client, boto3.resource]:
         """
         Get the underlying boto3 client/resource (cached)
 
@@ -365,7 +369,7 @@ def conn(self):
             # Rare possibility - subclasses have not specified a client_type or resource_type
             raise NotImplementedError('Could not get boto3 connection!')
 
-    def get_conn(self):
+    def get_conn(self) -> Union[boto3.client, boto3.resource]:
         """
         Get the underlying boto3 client/resource (cached)
 
@@ -378,12 +382,12 @@ def get_conn(self):
         # Compat shim
         return self.conn
 
-    def get_session(self, region_name=None):
+    def get_session(self, region_name: Optional[str] = None) -> boto3.session.Session:
         """Get the underlying boto3.session."""
         session, _ = self._get_credentials(region_name)
         return session
 
-    def get_credentials(self, region_name=None):
+    def get_credentials(self, region_name: Optional[str] = None) -> Tuple[Optional[str], Optional[str]]:
         """
         Get the underlying `botocore.Credentials` object.
 
@@ -395,7 +399,7 @@ def get_credentials(self, region_name=None):
         # See https://stackoverflow.com/a/36291428/8283373
         return session.get_credentials().get_frozen_credentials()
 
-    def expand_role(self, role):
+    def expand_role(self, role: str) -> str:
         """
         If the IAM role is a role name, get the Amazon Resource Name (ARN) for the role.
         If IAM role is already an IAM role ARN, no change is made.
@@ -409,7 +413,9 @@ def expand_role(self, role):
             return self.get_client_type("iam").get_role(RoleName=role)["Role"]["Arn"]
 
 
-def _parse_s3_config(config_file_name, config_format="boto", profile=None):
+def _parse_s3_config(
+    config_file_name: str, config_format: Optional[str] = "boto", profile: Optional[str] = None
+) -> Tuple[Optional[str], Optional[str]]:
     """
     Parses a config file for s3 credentials. Can currently
     parse boto, s3cmd.conf and AWS SDK config formats
diff --git a/airflow/providers/amazon/aws/hooks/ec2.py b/airflow/providers/amazon/aws/hooks/ec2.py
index 34517d752e699..27563be6c3284 100644
--- a/airflow/providers/amazon/aws/hooks/ec2.py
+++ b/airflow/providers/amazon/aws/hooks/ec2.py
@@ -33,8 +33,9 @@ class EC2Hook(AwsBaseHook):
         :class:`~airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`
     """
 
-    def __init__(self, *args, **kwargs):
-        super().__init__(resource_type="ec2", *args, **kwargs)
+    def __init__(self, *args, **kwargs) -> None:
+        kwargs["resource_type"] = "ec2"
+        super().__init__(*args, **kwargs)
 
     def get_instance(self, instance_id: str):
         """
diff --git a/airflow/providers/amazon/aws/hooks/emr.py b/airflow/providers/amazon/aws/hooks/emr.py
index 6bad910f472c9..8c2ed3f8830a7 100644
--- a/airflow/providers/amazon/aws/hooks/emr.py
+++ b/airflow/providers/amazon/aws/hooks/emr.py
@@ -15,6 +15,7 @@
 # KIND, either express or implied.  See the License for the
 # specific language governing permissions and limitations
 # under the License.
+from typing import Dict, List, Optional
 
 from airflow.exceptions import AirflowException
 from airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook
@@ -32,13 +33,15 @@ class EmrHook(AwsBaseHook):
         :class:`~airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`
     """
 
-    def __init__(self, emr_conn_id=None, *args, **kwargs):
+    def __init__(self, emr_conn_id: Optional[str] = None, *args, **kwargs) -> None:
         self.emr_conn_id = emr_conn_id
-        super().__init__(client_type='emr', *args, **kwargs)
+        kwargs["client_type"] = "emr"
+        super().__init__(*args, **kwargs)
 
-    def get_cluster_id_by_name(self, emr_cluster_name, cluster_states):
+    def get_cluster_id_by_name(self, emr_cluster_name: str, cluster_states: List[str]) -> Optional[str]:
         """
-        Fetch id of EMR cluster with given name and (optional) states. Will return only if single id is found.
+        Fetch id of EMR cluster with given name and (optional) states.
+        Will return only if single id is found.
 
         :param emr_cluster_name: Name of a cluster to find
         :type emr_cluster_name: str
@@ -63,7 +66,7 @@ def get_cluster_id_by_name(self, emr_cluster_name, cluster_states):
             self.log.info('No cluster found for name %s', emr_cluster_name)
             return None
 
-    def create_job_flow(self, job_flow_overrides):
+    def create_job_flow(self, job_flow_overrides: Dict):
         """
         Creates a job flow using the config from the EMR connection.
         Keys of the json extra hash may have the arguments of the boto3
diff --git a/airflow/providers/amazon/aws/hooks/kinesis.py b/airflow/providers/amazon/aws/hooks/kinesis.py
index 1c8480a8e2f79..304f8cd3245a5 100644
--- a/airflow/providers/amazon/aws/hooks/kinesis.py
+++ b/airflow/providers/amazon/aws/hooks/kinesis.py
@@ -19,6 +19,8 @@
 """
 This module contains AWS Firehose hook
 """
+from typing import Iterable
+
 from airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook
 
 
@@ -36,11 +38,12 @@ class AwsFirehoseHook(AwsBaseHook):
     :type delivery_stream: str
     """
 
-    def __init__(self, delivery_stream, *args, **kwargs):
+    def __init__(self, delivery_stream, *args, **kwargs) -> None:
         self.delivery_stream = delivery_stream
-        super().__init__(client_type='firehose', *args, **kwargs)
+        kwargs["client_type"] = "firehose"
+        super().__init__(*args, **kwargs)
 
-    def put_records(self, records):
+    def put_records(self, records: Iterable):
         """
         Write batch records to Kinesis Firehose
         """
diff --git a/airflow/providers/amazon/aws/hooks/lambda_function.py b/airflow/providers/amazon/aws/hooks/lambda_function.py
index a1d9b6142ffdf..008aafb78f596 100644
--- a/airflow/providers/amazon/aws/hooks/lambda_function.py
+++ b/airflow/providers/amazon/aws/hooks/lambda_function.py
@@ -44,20 +44,21 @@ class AwsLambdaHook(AwsBaseHook):
 
     def __init__(
         self,
-        function_name,
-        log_type='None',
-        qualifier='$LATEST',
-        invocation_type='RequestResponse',
+        function_name: str,
+        log_type: str = 'None',
+        qualifier: str = '$LATEST',
+        invocation_type: str = 'RequestResponse',
         *args,
         **kwargs,
-    ):
+    ) -> None:
         self.function_name = function_name
         self.log_type = log_type
         self.invocation_type = invocation_type
         self.qualifier = qualifier
-        super().__init__(client_type='lambda', *args, **kwargs)
+        kwargs["client_type"] = "lambda"
+        super().__init__(*args, **kwargs)
 
-    def invoke_lambda(self, payload):
+    def invoke_lambda(self, payload: str) -> str:
         """
         Invoke Lambda Function
         """
diff --git a/airflow/providers/amazon/aws/hooks/logs.py b/airflow/providers/amazon/aws/hooks/logs.py
index 1abb83d1a82dc..16b9f941a82a4 100644
--- a/airflow/providers/amazon/aws/hooks/logs.py
+++ b/airflow/providers/amazon/aws/hooks/logs.py
@@ -20,6 +20,7 @@
 This module contains a hook (AwsLogsHook) with some very basic
 functionality for interacting with AWS CloudWatch.
 """
+from typing import Dict, Generator, Optional
 
 from airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook
 
@@ -35,10 +36,18 @@ class AwsLogsHook(AwsBaseHook):
         :class:`~airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`
     """
 
-    def __init__(self, *args, **kwargs):
-        super().__init__(client_type='logs', *args, **kwargs)
-
-    def get_log_events(self, log_group, log_stream_name, start_time=0, skip=0, start_from_head=True):
+    def __init__(self, *args, **kwargs) -> None:
+        kwargs["client_type"] = "logs"
+        super().__init__(*args, **kwargs)
+
+    def get_log_events(
+        self,
+        log_group: str,
+        log_stream_name: str,
+        start_time: int = 0,
+        skip: int = 0,
+        start_from_head: bool = True,
+    ) -> Generator:
         """
         A generator for log items in a single stream. This will yield all the
         items that are available at the current moment.
@@ -67,7 +76,7 @@ def get_log_events(self, log_group, log_stream_name, start_time=0, skip=0, start
         event_count = 1
         while event_count > 0:
             if next_token is not None:
-                token_arg = {'nextToken': next_token}
+                token_arg: Optional[Dict[str, str]] = {'nextToken': next_token}
             else:
                 token_arg = {}
 
diff --git a/airflow/providers/amazon/aws/hooks/redshift.py b/airflow/providers/amazon/aws/hooks/redshift.py
index e2d3144c472ca..c97708d5347ad 100644
--- a/airflow/providers/amazon/aws/hooks/redshift.py
+++ b/airflow/providers/amazon/aws/hooks/redshift.py
@@ -35,8 +35,9 @@ class RedshiftHook(AwsBaseHook):
         :class:`~airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`
     """
 
-    def __init__(self, *args, **kwargs):
-        super().__init__(client_type='redshift', *args, **kwargs)
+    def __init__(self, *args, **kwargs) -> None:
+        kwargs["client_type"] = "redshift"
+        super().__init__(*args, **kwargs)
 
     # TODO: Wrap create_cluster_snapshot
     def cluster_status(self, cluster_identifier: str) -> str:
diff --git a/airflow/providers/amazon/aws/hooks/sqs.py b/airflow/providers/amazon/aws/hooks/sqs.py
index 6c43f7f70b8ba..c40a432b0fbd0 100644
--- a/airflow/providers/amazon/aws/hooks/sqs.py
+++ b/airflow/providers/amazon/aws/hooks/sqs.py
@@ -19,6 +19,8 @@
 """
 This module contains AWS SQS hook
 """
+from typing import Dict, Optional
+
 from airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook
 
 
@@ -33,10 +35,11 @@ class SQSHook(AwsBaseHook):
         :class:`~airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`
     """
 
-    def __init__(self, *args, **kwargs):
-        super().__init__(client_type='sqs', *args, **kwargs)
+    def __init__(self, *args, **kwargs) -> None:
+        kwargs["client_type"] = "sqs"
+        super().__init__(*args, **kwargs)
 
-    def create_queue(self, queue_name, attributes=None):
+    def create_queue(self, queue_name: str, attributes: Optional[Dict] = None) -> Dict:
         """
         Create queue using connection object
 
@@ -52,7 +55,13 @@ def create_queue(self, queue_name, attributes=None):
         """
         return self.get_conn().create_queue(QueueName=queue_name, Attributes=attributes or {})
 
-    def send_message(self, queue_url, message_body, delay_seconds=0, message_attributes=None):
+    def send_message(
+        self,
+        queue_url: str,
+        message_body: str,
+        delay_seconds: int = 0,
+        message_attributes: Optional[Dict] = None,
+    ) -> Dict:
         """
         Send message to the queue
 
diff --git a/airflow/providers/amazon/aws/hooks/step_function.py b/airflow/providers/amazon/aws/hooks/step_function.py
index d83d1afa6257a..d120cbfae81cc 100644
--- a/airflow/providers/amazon/aws/hooks/step_function.py
+++ b/airflow/providers/amazon/aws/hooks/step_function.py
@@ -32,8 +32,9 @@ class StepFunctionHook(AwsBaseHook):
         :class:`~airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`
     """
 
-    def __init__(self, region_name=None, *args, **kwargs):
-        super().__init__(client_type='stepfunctions', *args, **kwargs)
+    def __init__(self, region_name: Optional[str] = None, *args, **kwargs) -> None:
+        kwargs["client_type"] = "stepfunctions"
+        super().__init__(*args, **kwargs)
 
     def start_execution(
         self,


---- SAMPLE #42 ----
Generated commit : [AIRFLOW-XXX] Use BaseHook for HIVEHook (#5033)
Ground-truth     : Fixing small issue with qbol operator and hook
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / -0.0367 / 0.1930
Diff:
diff --git a/airflow/contrib/hooks/qubole_hook.py b/airflow/contrib/hooks/qubole_hook.py
index 717027ccd8057..7e2fdb32dcbcc 100755
--- a/airflow/contrib/hooks/qubole_hook.py
+++ b/airflow/contrib/hooks/qubole_hook.py
@@ -1,5 +1,6 @@
 import os
 import time
+import datetime
 import logging
 
 from airflow.utils import AirflowException
diff --git a/airflow/contrib/operators/qubole_operator.py b/airflow/contrib/operators/qubole_operator.py
index d8c47b98ef1ce..063eb8c169bcb 100755
--- a/airflow/contrib/operators/qubole_operator.py
+++ b/airflow/contrib/operators/qubole_operator.py
@@ -102,10 +102,10 @@ def __init__(self, qubole_conn_id="qubole_default", *args, **kwargs):
         self.args = args
         self.kwargs = kwargs
         self.kwargs['qubole_conn_id'] = qubole_conn_id
+        self.hook = QuboleHook(*self.args, **self.kwargs)
         super(QuboleOperator, self).__init__(*args, **kwargs)
 
     def execute(self, context):
-        self.hook = QuboleHook(*self.args, **self.kwargs)
         return self.hook.execute(context)
 
     def on_kill(self, ti):


---- SAMPLE #43 ----
Generated commit : add tests
Ground-truth     : Ensure pipeline yaml files are valid YAML and can be rendered with JSON-e

Part of #613
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / 0.0063 / 0.1825
Diff:
diff --git a/tests/test_pipelines.py b/tests/test_pipelines.py
new file mode 100644
index 0000000000..7747c7f7aa
--- /dev/null
+++ b/tests/test_pipelines.py
@@ -0,0 +1,25 @@
+# -*- coding: utf-8 -*-
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this file,
+# You can obtain one at http://mozilla.org/MPL/2.0/.
+
+import os
+
+import jsone
+import pytest
+import yaml
+
+
+@pytest.mark.parametrize(
+    "pipeline_file",
+    (
+        os.path.realpath(os.path.join("infra", f))
+        for f in os.listdir("infra")
+        if f.endswith(".yml")
+    ),
+)
+def test_jsone_validates(pipeline_file):
+    with open(pipeline_file, "r") as f:
+        yaml_content = yaml.safe_load(f.read())
+
+    jsone.render(yaml_content, context={"version": "42.0"})


---- SAMPLE #44 ----
Generated commit : fixing the filtering logic to filter for users on the same name
Ground-truth     : Fix bug 1309287: Add the ability to disable Requests for projects
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0224 / 0.3478 / 0.1802 / 0.1297 / 0.1819
Diff:
diff --git a/pontoon/teams/views.py b/pontoon/teams/views.py
index 2290f697e3..bb8146a0dc 100644
--- a/pontoon/teams/views.py
+++ b/pontoon/teams/views.py
@@ -53,6 +53,7 @@ def ajax_projects(request, locale):
 
     projects = (
         Project.objects.available()
+        .filter(can_be_requested=True)
         .prefetch_latest_translation(locale)
         .order_by('name')
     )
@@ -132,7 +133,10 @@ def request_projects(request, locale):
     locale = get_object_or_404(Locale, code__iexact=locale)
 
     # Validate projects
-    project_list = Project.objects.available().filter(slug__in=slug_list)
+    project_list = (
+        Project.objects.available()
+        .filter(slug__in=slug_list, can_be_requested=True)
+    )
     if not project_list:
         return HttpResponseBadRequest('Bad Request: Non-existent projects specified')
 


---- SAMPLE #45 ----
Generated commit : fix: fix the docs in telemetry data to avoid issues with the same name
Ground-truth     : Added check of the selenium logout action (#784)

* Added check of the selenium logout action

* Selenium login and logout actions modified after review

* Changes after review

* fix of sleector after comment
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0042 / 0.0889 / 0.0456 / 0.0349 / 0.1777
Diff:
diff --git a/app/selenium_ui/bamboo/modules.py b/app/selenium_ui/bamboo/modules.py
index ef21ab4bf..a07a8d0be 100644
--- a/app/selenium_ui/bamboo/modules.py
+++ b/app/selenium_ui/bamboo/modules.py
@@ -27,7 +27,6 @@ def sub_measure():
             login_page.go_to()
 
         sub_measure()
-
         login_page.set_credentials(username=datasets['username'], password=datasets['password'])
         login_page.click_login_button()
 
@@ -128,8 +127,10 @@ def measure():
 
 
 def log_out(webdriver, datasets):
+    logout = Logout(webdriver)
+
     @print_timing("selenium_log_out")
     def measure():
-        Logout(webdriver)
-
+        logout.go_to()
+        logout.wait_for_page_loaded()
     measure()
diff --git a/app/selenium_ui/bamboo/pages/pages.py b/app/selenium_ui/bamboo/pages/pages.py
index 9126e4155..d28d38503 100644
--- a/app/selenium_ui/bamboo/pages/pages.py
+++ b/app/selenium_ui/bamboo/pages/pages.py
@@ -2,15 +2,15 @@
 
 from selenium_ui.bamboo.pages.selectors import UrlManager, LoginPageLocators, AllProjectsLocators, AllBuildsLocators, \
     PlanConfigurationLocators, BuildActivityLocators, PlanSummaryLocators, PlanHistoryLocators, BuildSummaryLocators, \
-    BuildLogsLocators, JobConfigLocators
+    BuildLogsLocators, JobConfigLocators, LogoutLocators
 
 
 class Login(BasePage):
     page_url = LoginPageLocators.login_page_url
 
     def click_login_button(self):
-        self.wait_until_visible(LoginPageLocators.login_button).click()
-        self.wait_until_invisible(LoginPageLocators.login_button)
+        self.wait_until_visible(LoginPageLocators.login_submit_button).click()
+        self.wait_until_invisible(LoginPageLocators.login_submit_button)
 
     def set_credentials(self, username, password):
         self.get_element(LoginPageLocators.login_username_field).send_keys(username)
@@ -102,4 +102,7 @@ def click_job_config_button(self):
 
 
 class Logout(BasePage):
-    UrlManager().logout_url()
+    page_url = LogoutLocators.logout_url
+
+    def wait_for_page_loaded(self):
+        self.wait_until_visible(LogoutLocators.login_button_link)
diff --git a/app/selenium_ui/bamboo/pages/selectors.py b/app/selenium_ui/bamboo/pages/selectors.py
index dc46e9395..8902a724e 100644
--- a/app/selenium_ui/bamboo/pages/selectors.py
+++ b/app/selenium_ui/bamboo/pages/selectors.py
@@ -6,7 +6,7 @@ class UrlManager:
 
     def __init__(self, build_plan_id=None):
         self.host = BAMBOO_SETTINGS.server_url
-        self.login_params = '/userlogin!doDefault.action?os_destination=%2FallPlans.action'
+        self.login_params = '/userlogin!doDefault.action'
         self.logout_params = '/userLogout.action'
         self.all_projects_params = '/allProjects.action'
         self.plan_summary_params = f'/browse/{build_plan_id}'
@@ -34,9 +34,9 @@ def logout_url(self):
 
 class LoginPageLocators:
     login_page_url = UrlManager().login_url()
-    login_button = (By.ID, "loginForm_save")
     login_username_field = (By.ID, "loginForm_os_username")
     login_password_field = (By.ID, "loginForm_os_password")
+    login_submit_button = (By.ID, "loginForm_save")
 
 
 class AllProjectsLocators:
@@ -89,4 +89,5 @@ class JobConfigLocators:
 
 
 class LogoutLocators:
-    logout = (By.XPATH, "//a[@href='/userLogout.action']")
+    logout_url = UrlManager().logout_url()
+    login_button_link = (By.ID, "login")


---- SAMPLE #46 ----
Generated commit : Fixes error message when logging level is not set
Ground-truth     : celeryd: Errors could not be logged, because of unicode errors
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0215 / 0.2105 / 0.1515 / 0.1691 / 0.1960
Diff:
diff --git a/celery/log.py b/celery/log.py
index daf7095a0dd..5128abb94b9 100644
--- a/celery/log.py
+++ b/celery/log.py
@@ -14,6 +14,7 @@
 from celery import current_app
 from celery.utils import LOG_LEVELS, isatty
 from celery.utils.compat import LoggerAdapter
+from celery.utils.encoding import safe_str
 from celery.utils.patch import ensure_process_aware_logger
 from celery.utils.term import colored
 
@@ -40,10 +41,10 @@ def format(self, record):
 
         if self.use_color and color:
             try:
-                record.msg = color(unicode(record.msg, "utf-8", "replace"))
+                record.msg = color(safe_str(record.msg))
             except Exception, exc:
                 record.msg = "<Unrepresentable %r: %r>" % (type(record.msg),
-                                                           exc)
+                                                           traceback.format_stack())
 
         # Very ugly, but have to make sure processName is supported
         # by foreign logger instances.
diff --git a/celery/utils/encoding.py b/celery/utils/encoding.py
index 5a038c3c00b..86a6406c025 100644
--- a/celery/utils/encoding.py
+++ b/celery/utils/encoding.py
@@ -1,4 +1,5 @@
 import sys
+import traceback
 
 
 def default_encoding():
@@ -13,8 +14,8 @@ def safe_str(s, errors="replace"):
         if isinstance(s, unicode):
             return s.encode(encoding, errors)
         return unicode(s, encoding, errors)
-    except Exception, exc:
-        return "<Unrepresentable %r: %r>" % (type(s), exc)
+    except Exception:
+        return "<Unrepresentable %r: %r>" % (type(s), traceback.format_stack())
 
 
 def safe_repr(o, errors="replace"):


---- SAMPLE #47 ----
Generated commit : Remove some tests (#6032)
Ground-truth     : Revert "Fix pre-mature evaluation of tasks in mapped task group (#34337)" (#35651)

This reverts commit 69938fd163045d750b8c218500d79bc89858f9c1.
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / -0.1355 / 0.2019
Diff:
diff --git a/airflow/ti_deps/deps/trigger_rule_dep.py b/airflow/ti_deps/deps/trigger_rule_dep.py
index 6203b2a79b6d1..ca2a6100a2784 100644
--- a/airflow/ti_deps/deps/trigger_rule_dep.py
+++ b/airflow/ti_deps/deps/trigger_rule_dep.py
@@ -27,7 +27,6 @@
 from airflow.models.taskinstance import PAST_DEPENDS_MET
 from airflow.ti_deps.deps.base_ti_dep import BaseTIDep
 from airflow.utils.state import TaskInstanceState
-from airflow.utils.task_group import MappedTaskGroup
 from airflow.utils.trigger_rule import TriggerRule as TR
 
 if TYPE_CHECKING:
@@ -133,20 +132,6 @@ def _get_expanded_ti_count() -> int:
             """
             return ti.task.get_mapped_ti_count(ti.run_id, session=session)
 
-        def _iter_expansion_dependencies() -> Iterator[str]:
-            from airflow.models.mappedoperator import MappedOperator
-
-            if isinstance(ti.task, MappedOperator):
-                for op in ti.task.iter_mapped_dependencies():
-                    yield op.task_id
-            task_group = ti.task.task_group
-            if task_group and task_group.iter_mapped_task_groups():
-                yield from (
-                    op.task_id
-                    for tg in task_group.iter_mapped_task_groups()
-                    for op in tg.iter_mapped_dependencies()
-                )
-
         @functools.lru_cache
         def _get_relevant_upstream_map_indexes(upstream_id: str) -> int | range | None:
             """Get the given task's map indexes relevant to the current ti.
@@ -157,9 +142,6 @@ def _get_relevant_upstream_map_indexes(upstream_id: str) -> int | range | None:
             """
             if TYPE_CHECKING:
                 assert isinstance(ti.task.dag, DAG)
-            if isinstance(ti.task.task_group, MappedTaskGroup):
-                if upstream_id not in set(_iter_expansion_dependencies()):
-                    return None
             try:
                 expanded_ti_count = _get_expanded_ti_count()
             except (NotFullyPopulated, NotMapped):
diff --git a/tests/models/test_mappedoperator.py b/tests/models/test_mappedoperator.py
index 5c2e23c1f9e30..7244c55774840 100644
--- a/tests/models/test_mappedoperator.py
+++ b/tests/models/test_mappedoperator.py
@@ -1305,8 +1305,8 @@ def file_transforms(filename):
         states = self.get_states(dr)
         expected = {
             "file_transforms.my_setup": {0: "success", 1: "failed", 2: "skipped"},
-            "file_transforms.my_work": {2: "upstream_failed", 1: "upstream_failed", 0: "upstream_failed"},
-            "file_transforms.my_teardown": {2: "success", 1: "success", 0: "success"},
+            "file_transforms.my_work": {0: "success", 1: "upstream_failed", 2: "skipped"},
+            "file_transforms.my_teardown": {0: "success", 1: "upstream_failed", 2: "skipped"},
         }
 
         assert states == expected
diff --git a/tests/ti_deps/deps/test_trigger_rule_dep.py b/tests/ti_deps/deps/test_trigger_rule_dep.py
index 1bc8808cb8b2b..00cbcd449af3e 100644
--- a/tests/ti_deps/deps/test_trigger_rule_dep.py
+++ b/tests/ti_deps/deps/test_trigger_rule_dep.py
@@ -1165,23 +1165,19 @@ def _one_scheduling_decision_iteration() -> dict[tuple[str, int], TaskInstance]:
     tis = _one_scheduling_decision_iteration()
     assert sorted(tis) == [("tg.t1", 0), ("tg.t1", 1), ("tg.t1", 2)]
 
-    # After running the first t1, the remaining t1 must be run before t2 is available.
+    # After running the first t1, the first t2 becomes immediately available.
     tis["tg.t1", 0].run()
     tis = _one_scheduling_decision_iteration()
-    assert sorted(tis) == [("tg.t1", 1), ("tg.t1", 2)]
+    assert sorted(tis) == [("tg.t1", 1), ("tg.t1", 2), ("tg.t2", 0)]
 
-    # After running all t1, t2 is available.
-    tis["tg.t1", 1].run()
+    # Similarly for the subsequent t2 instances.
     tis["tg.t1", 2].run()
     tis = _one_scheduling_decision_iteration()
-    assert sorted(tis) == [("tg.t2", 0), ("tg.t2", 1), ("tg.t2", 2)]
-
-    # Similarly for t2 instances. They both have to complete before t3 is available
-    tis["tg.t2", 0].run()
-    tis = _one_scheduling_decision_iteration()
-    assert sorted(tis) == [("tg.t2", 1), ("tg.t2", 2)]
+    assert sorted(tis) == [("tg.t1", 1), ("tg.t2", 0), ("tg.t2", 2)]
 
     # But running t2 partially does not make t3 available.
+    tis["tg.t1", 1].run()
+    tis["tg.t2", 0].run()
     tis["tg.t2", 2].run()
     tis = _one_scheduling_decision_iteration()
     assert sorted(tis) == [("tg.t2", 1)]
@@ -1411,34 +1407,3 @@ def w2():
             (status,) = self.get_dep_statuses(dr, "w2", flag_upstream_failed=True, session=session)
         assert status.reason.startswith("All setup tasks must complete successfully")
         assert self.get_ti(dr, "w2").state == expected
-
-
-def test_mapped_tasks_in_mapped_task_group_waits_for_upstreams_to_complete(dag_maker, session):
-    """Test that one failed trigger rule works well in mapped task group"""
-    with dag_maker() as dag:
-
-        @dag.task
-        def t1():
-            return [1, 2, 3]
-
-        @task_group("tg1")
-        def tg1(a):
-            @dag.task()
-            def t2(a):
-                return a
-
-            @dag.task(trigger_rule=TriggerRule.ONE_FAILED)
-            def t3(a):
-                return a
-
-            t2(a) >> t3(a)
-
-        t = t1()
-        tg1.expand(a=t)
-
-    dr = dag_maker.create_dagrun()
-    ti = dr.get_task_instance(task_id="t1")
-    ti.run()
-    dr.task_instance_scheduling_decisions()
-    ti3 = dr.get_task_instance(task_id="tg1.t3")
-    assert not ti3.state


---- SAMPLE #48 ----
Generated commit : Add test to illustrate code header (#2254)
Ground-truth     : Don't linkify the validation context (bug 1030849)
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / 0.1101 / 0.1918
Diff:
diff --git a/apps/amo/tests/test_amo_utils.py b/apps/amo/tests/test_amo_utils.py
index ce0a72c3e63..d8e8befdc57 100644
--- a/apps/amo/tests/test_amo_utils.py
+++ b/apps/amo/tests/test_amo_utils.py
@@ -272,6 +272,23 @@ def test_nested(self):
         eq_(res['list'], [expected])
         eq_(res['bool'], True)
 
+    def test_without_linkify(self):
+        value = '<button>http://firefox.com</button>'
+        expected = '&lt;button&gt;http://firefox.com&lt;/button&gt;'
+
+        test = {
+            'string': value,
+            'dict': {'x': value},
+            'list': [value],
+            'bool': True,
+        }
+        res = escape_all(test, linkify=False)
+
+        eq_(res['string'], expected)
+        eq_(res['dict'], {'x': expected})
+        eq_(res['list'], [expected])
+        eq_(res['bool'], True)
+
 
 class TestAttachTransDict(amo.tests.TestCase):
     """
diff --git a/apps/amo/utils.py b/apps/amo/utils.py
index 5e896b2d33a..7eabaedf8df 100644
--- a/apps/amo/utils.py
+++ b/apps/amo/utils.py
@@ -714,18 +714,19 @@ def no_translation(lang=None):
     translation.trans_real.activate(old_lang)
 
 
-def escape_all(v):
+def escape_all(v, linkify=True):
     """Escape html in JSON value, including nested items."""
     if isinstance(v, basestring):
         v = jinja2.escape(smart_unicode(v))
-        v = linkify_with_outgoing(v)
+        if linkify:
+            v = linkify_with_outgoing(v)
         return v
     elif isinstance(v, list):
         for i, lv in enumerate(v):
-            v[i] = escape_all(lv)
+            v[i] = escape_all(lv, linkify=linkify)
     elif isinstance(v, dict):
         for k, lv in v.iteritems():
-            v[k] = escape_all(lv)
+            v[k] = escape_all(lv, linkify=linkify)
     elif isinstance(v, Translation):
         v = jinja2.escape(smart_unicode(v.localized_string))
     return v
diff --git a/mkt/developers/tests/test_views.py b/mkt/developers/tests/test_views.py
index f60d26d48bb..d46ad0376d6 100644
--- a/mkt/developers/tests/test_views.py
+++ b/mkt/developers/tests/test_views.py
@@ -928,6 +928,62 @@ def test_detail_view(self):
                     args=['hosted', upload.uuid]))
         eq_(suite('#suite-results-tier-2').length, 1)
 
+    def test_detail_view_linkification(self):
+        uid = '9b1b3898db8a4d99a049829a46969ab4'
+        upload = FileUpload.objects.create(
+            name='something.zip',
+            validation=json.dumps({
+                u'ending_tier': 1,
+                u'success': False,
+                u'warnings': 0,
+                u'errors': 1,
+                u'notices': 0,
+                u'feature_profile': [],
+                u'messages': [
+                    {
+                        u'column': None,
+                        u'context': [
+                            u'',
+                            u'<button on-click="{{ port.name }}">uh</button>',
+                            u''
+                        ],
+                        u'description': [
+                            u'http://www.firefox.com'
+                            u'<script>alert("hi");</script>',
+                        ],
+                        u'file': u'index.html',
+                        u'id': [u'csp', u'script_attribute'],
+                        u'line': 1638,
+                        u'message': u'CSP Violation Detected',
+                        u'tier': 2,
+                        u'type': u'error',
+                        u'uid': uid,
+                    },
+                ],
+                u'metadata': {'ran_js_tests': 'yes'},
+                u'manifest': {},
+                u'feature_usage': [],
+                u'permissions': [],
+
+            }),
+        )
+        r = self.client.get(reverse('mkt.developers.standalone_upload_detail',
+                                    args=['packaged', upload.uuid]))
+        eq_(r.status_code, 200)
+        data = json.loads(r.content)
+        message = data['validation']['messages'][0]
+        description = message['description'][0]
+        eq_(description,
+            '<a rel="nofollow" href="http://outgoing.mozilla.org/v1/'
+            '680c0c59e4d87f3d21faf1d7365f0fec615076041466406aa3608fe6503aef43'
+            '/http%3A//www.firefox.com">'
+            'http://www.firefox.com</a>'
+            '&lt;script&gt;alert("hi");&lt;/script&gt;')
+        context = message['context'][1]
+        eq_(context,
+            '&lt;button on-click=&#34;{{ port.name }}&#34;&gt;'
+            'uh&lt;/button&gt;')
+
 
 def assert_json_error(request, field, msg):
     eq_(request.status_code, 400)
diff --git a/mkt/developers/views.py b/mkt/developers/views.py
index 25bf9bea318..761e8f3078f 100644
--- a/mkt/developers/views.py
+++ b/mkt/developers/views.py
@@ -680,7 +680,7 @@ def make_validation_result(data):
     if data['validation']:
         for msg in data['validation']['messages']:
             for k, v in msg.items():
-                msg[k] = escape_all(v)
+                msg[k] = escape_all(v, linkify=k in ('message', 'description'))
     return data
 
 


---- SAMPLE #49 ----
Generated commit : A few more fixes for the fact that it's now defined
Ground-truth     : Test sns_message response and how it affects email removal from S3
Bounce notification type is being checked on _get_bucket_and_key_from_s3_json since by design the notification message will not have receipt
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0037 / 0.0417 / 0.0184 / -0.0461 / 0.1715
Diff:
diff --git a/emails/tests/views_tests.py b/emails/tests/views_tests.py
index 8cf1721692..585dbd5fe0 100644
--- a/emails/tests/views_tests.py
+++ b/emails/tests/views_tests.py
@@ -227,9 +227,9 @@ def setUp(self) -> None:
         self.bucket = 'test-bucket'
         self.key = '/emails/objectkey123'
 
-        patcher = patch('emails.views._get_address', side_effect=ObjectDoesNotExist())
-        patcher.start()
-        self.addCleanup(patcher.stop)
+        self.patcher = patch('emails.views._get_address', side_effect=ObjectDoesNotExist())
+        self.patcher.start()
+        self.addCleanup(self.patcher.stop)
 
     @patch('emails.views.remove_message_from_s3')
     @patch('emails.views._handle_reply')
@@ -298,6 +298,155 @@ def test_address_does_not_exist_email_in_s3_deleted(
         assert response.status_code == 404
         assert response.content == b'Address does not exist'
 
+    @patch('emails.views.remove_message_from_s3')
+    def test_bounce_notification_not_in_s3_deleted_ignored(
+        self, mocked_message_removed
+    ):
+        response = _sns_notification(BOUNCE_SNS_BODIES['soft'])
+        mocked_message_removed.assert_called_once_with(None, None)
+        assert response.status_code == 404
+        assert response.content == b'Address does not exist'
+
+    @patch('emails.views.remove_message_from_s3')
+    def test_email_without_commonheaders_in_s3_deleted(
+        self, mocked_message_removed
+    ):
+        message_wo_commonheaders = EMAIL_SNS_BODIES['s3_stored']['Message'].replace(
+            'commonHeaders', 'invalidHeaders'
+        )
+        notification_wo_commonheaders = EMAIL_SNS_BODIES['s3_stored'].copy()
+        notification_wo_commonheaders['Message'] = message_wo_commonheaders
+        response = _sns_notification(notification_wo_commonheaders)
+        mocked_message_removed.assert_called_once_with(self.bucket, self.key)
+        assert response.status_code == 400
+        assert response.content == b'Received SNS notification without commonHeaders.'
+
+    @patch('emails.views.remove_message_from_s3')
+    def test_email_to_non_relay_domain_in_s3_deleted(
+        self, mocked_message_removed
+    ):
+        message_w_non_relay_as_recipient = EMAIL_SNS_BODIES['s3_stored']['Message'].replace(
+            'sender@test.com', 'to@not-relay.com'
+        )
+        notification_w_non_relay_domain = EMAIL_SNS_BODIES['s3_stored'].copy()
+        notification_w_non_relay_domain['Message'] = message_w_non_relay_as_recipient
+        response = _sns_notification(notification_w_non_relay_domain)
+        mocked_message_removed.assert_called_once_with(self.bucket, self.key)
+        assert response.status_code == 404
+        assert response.content == b'Address does not exist'
+
+    @patch('emails.views.remove_message_from_s3')
+    def test_malformed_to_field_email_in_s3_deleted(
+        self, mocked_message_removed
+    ):
+        message_w_malformed_to_field = EMAIL_SNS_BODIES['s3_stored']['Message'].replace(
+            'sender@test.com', 'not-relay-test.com'
+        )
+        notification_w_malformed_to_field = EMAIL_SNS_BODIES['s3_stored'].copy()
+        notification_w_malformed_to_field['Message'] = message_w_malformed_to_field
+        response = _sns_notification(notification_w_malformed_to_field)
+        mocked_message_removed.assert_called_once_with(self.bucket, self.key)
+        assert response.status_code == 400
+        assert response.content == b'Malformed to field.'
+
+    @patch('emails.views.remove_message_from_s3')
+    def test_noreply_email_in_s3_deleted(
+        self, mocked_message_removed
+    ):
+        message_w_email_to_noreply = EMAIL_SNS_BODIES['s3_stored']['Message'].replace(
+            'sender@test.com', 'noreply@default.com'
+        )
+        notification_w_email_to_noreply = EMAIL_SNS_BODIES['s3_stored'].copy()
+        notification_w_email_to_noreply['Message'] = message_w_email_to_noreply
+        response = _sns_notification(notification_w_email_to_noreply)
+        mocked_message_removed.assert_called_once_with(self.bucket, self.key)
+        assert response.status_code == 200
+        assert response.content == b'noreply address is not supported.'
+
+
+class SNSNotificationValidUserEmailsInS3Test(TestCase):
+    def setUp(self) -> None:
+        self.bucket = 'test-bucket'
+        self.key = '/emails/objectkey123'
+        self.user = baker.make(
+            User, email='sender@test.com', make_m2m=True
+        )
+        self.profile = self.user.profile_set.first()
+        self.address = baker.make(
+            RelayAddress, user=self.user, address='sender', domain=2
+        )
+
+    @patch('emails.views.remove_message_from_s3')
+    def test_auto_block_spam_true_email_in_s3_deleted(
+        self, mocked_message_removed
+    ):
+        self.profile.auto_block_spam = True
+        self.profile.save()
+        message_spamverdict_failed = EMAIL_SNS_BODIES['s3_stored']['Message'].replace(
+            '"spamVerdict":{"status":"PASS"}', '"spamVerdict":{"status":"FAIL"}'
+        )
+        notification_w_spamverdict_failed = EMAIL_SNS_BODIES['s3_stored'].copy()
+        notification_w_spamverdict_failed['Message'] = message_spamverdict_failed
+
+        response = _sns_notification(notification_w_spamverdict_failed)
+        mocked_message_removed.assert_called_once_with(self.bucket, self.key)
+        assert response.status_code == 200
+        assert response.content == b'Address rejects spam.'
+
+    @patch('emails.views.remove_message_from_s3')
+    def test_user_bounce_paused_email_in_s3_deleted(
+        self, mocked_message_removed
+    ):
+        self.profile.last_soft_bounce = datetime.now(timezone.utc)
+        self.profile.save()
+
+        response = _sns_notification(EMAIL_SNS_BODIES['s3_stored'])
+        mocked_message_removed.assert_called_once_with(self.bucket, self.key)
+        assert response.status_code == 200
+        assert response.content == b'Address is temporarily disabled.'
+
+    @patch('emails.views._reply_allowed')
+    @patch('emails.views._get_reply_record_from_lookup_key')
+    @patch('emails.views.remove_message_from_s3')
+    def test_reply_not_allowed_email_in_s3_deleted(
+        self, mocked_message_removed, mocked_reply_record, mocked_reply_allowed,
+    ):
+        # external user sending a reply to Relay user
+        # where the replies were being exchanged but now the user
+        # no longer has the premium subscription
+        mocked_reply_allowed.return_value = False
+
+        response = _sns_notification(EMAIL_SNS_BODIES['s3_stored'])
+        mocked_message_removed.assert_called_once_with(self.bucket, self.key)
+        assert response.status_code == 403
+        assert response.content == b'Relay replies require a premium account'
+
+    @patch('emails.views.remove_message_from_s3')
+    def test_relay_address_disabled_email_in_s3_deleted(
+        self, mocked_message_removed
+    ):
+        self.address.enabled = False
+        self.address.save()
+
+        response = _sns_notification(EMAIL_SNS_BODIES['s3_stored'])
+        mocked_message_removed.assert_called_once_with(self.bucket, self.key)
+        assert response.status_code == 200
+        assert response.content == b'Address is temporarily disabled.'
+
+    @patch('emails.views._check_email_from_list')
+    @patch('emails.views.remove_message_from_s3')
+    def test_blocked_list_email_in_s3_deleted(
+        self, mocked_message_removed, mocked_email_is_from_list
+    ):
+        self.address.block_list_emails = True
+        self.address.save()
+        mocked_email_is_from_list.return_value = True
+
+        response = _sns_notification(EMAIL_SNS_BODIES['s3_stored'])
+        mocked_message_removed.assert_called_once_with(self.bucket, self.key)
+        assert response.status_code == 200
+        assert response.content == b'Address is not accepting list emails.'
+
 
 class SnsMessageTest(TestCase):
     def setUp(self) -> None:
diff --git a/emails/utils.py b/emails/utils.py
index 4386b1a446..be6f22eac9 100644
--- a/emails/utils.py
+++ b/emails/utils.py
@@ -284,12 +284,16 @@ def _get_bucket_and_key_from_s3_json(message_json):
     if 'receipt' in message_json and 'action' in message_json['receipt']:
         message_json_receipt = message_json['receipt']
     else:
-        # TODO: sns inbound notification does not have 'receipt'
-        # we need to look into this more
-        logger.error(
-            'sns_inbound_message_without_receipt',
-            extra={'message_json_keys': message_json.keys()}
-        )
+        notification_type = message_json.get('notificationType')
+        event_type = message_json.get('eventType')
+        is_bounce_notification = notification_type == 'Bounce' or event_type == 'Bounce'
+        if not is_bounce_notification:
+            # TODO: sns inbound notification does not have 'receipt'
+            # we need to look into this more
+            logger.error(
+                'sns_inbound_message_without_receipt',
+                extra={'message_json_keys': message_json.keys()}
+            )
         return None, None
 
     try:
diff --git a/emails/views.py b/emails/views.py
index 5caa004de7..dbf0fe07bf 100644
--- a/emails/views.py
+++ b/emails/views.py
@@ -346,7 +346,7 @@ def _sns_message(message_json):
         [to_local_portion, to_domain_portion] = to_address.split('@')
     except ValueError:
         # TODO: Add metric
-        return HttpResponse('Malformed to field.', 400)
+        return HttpResponse('Malformed to field.', status=400)
 
     if to_local_portion == 'noreply':
         incr_if_enabled('email_for_noreply_address', 1)


---- SAMPLE #50 ----
Generated commit : Update eval script to use latest version of the latest version (#861)
Ground-truth     : Increase version number of the DBs, as we changed the format
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0399 / 0.2609 / 0.1351 / 0.1182 / 0.1957
Diff:
diff --git a/scripts/regressor_finder.py b/scripts/regressor_finder.py
index 4ca01d5b33..8997507c00 100644
--- a/scripts/regressor_finder.py
+++ b/scripts/regressor_finder.py
@@ -41,21 +41,21 @@
 db.register(
     BUG_FIXING_COMMITS_DB,
     "https://index.taskcluster.net/v1/task/project.relman.bugbug_annotate.regressor_finder.latest/artifacts/public/bug_fixing_commits.json.zst",
-    2,
+    3,
 )
 
 BUG_INTRODUCING_COMMITS_DB = "data/bug_introducing_commits.json"
 db.register(
     BUG_INTRODUCING_COMMITS_DB,
     "https://index.taskcluster.net/v1/task/project.relman.bugbug_annotate.regressor_finder.latest/artifacts/public/bug_introducing_commits.json.zst",
-    1,
+    3,
 )
 
 TOKENIZED_BUG_INTRODUCING_COMMITS_DB = "data/tokenized_bug_introducing_commits.json"
 db.register(
     TOKENIZED_BUG_INTRODUCING_COMMITS_DB,
     "https://index.taskcluster.net/v1/task/project.relman.bugbug_annotate.regressor_finder.latest/artifacts/public/tokenized_bug_introducing_commits.json.zst",
-    1,
+    3,
 )
 
 


---- SAMPLE #51 ----
Generated commit : Fixing issue with filtering by users of SQLAlchemy (#241)
Ground-truth     : Bugfix: display aggregated stats only for project locales, ommit removed ones
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / 0.0195 / 0.4391
Diff:
diff --git a/pontoon/base/models.py b/pontoon/base/models.py
index a73cc2773f..095f773b35 100644
--- a/pontoon/base/models.py
+++ b/pontoon/base/models.py
@@ -308,7 +308,9 @@ def get_projects_with_stats(projects, locale=None):
     for project in projects:
         r = Entity.objects.filter(obsolete=False).values('resource')
         resources = Resource.objects.filter(project=project, pk__in=r)
-        stats = Stats.objects.filter(resource__in=resources)
+        locales = project.locales.all()
+        stats = Stats.objects.filter(
+            resource__in=resources, locale__in=locales)
 
         if locale:
             stats = stats.filter(locale=locale)


---- SAMPLE #52 ----
Generated commit : Add function to check for getting zoo file (#2866)
Ground-truth     : [JIRA/JSM] DCA-1116:Add reindex info check (#609)

* DCA-1116:Add reindex info check

* DCA-1116:Refacotr code - add expected_status_codes for get reindex info request

* DCA-1116:Move error message into constant
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0035 / 0.1818 / 0.1209 / 0.0212 / 0.1761
Diff:
diff --git a/app/util/api/abstract_clients.py b/app/util/api/abstract_clients.py
index 6fa0bedd2..a218b9f76 100644
--- a/app/util/api/abstract_clients.py
+++ b/app/util/api/abstract_clients.py
@@ -71,10 +71,13 @@ def get(self, url: str,
             allow_redirect: bool = False,
             headers: dict = None,
             auth: tuple = None):
+
         response = self.session.get(url, verify=self.verify, timeout=self.requests_timeout,
                                     allow_redirects=allow_redirect, headers=headers if headers else self.headers,
                                     auth=auth if auth else self.base_auth)
+
         self.__verify_response(response, error_msg, expected_status_codes)
+
         return response
 
     def delete(self, url: str, error_msg: str, expected_status_codes: list = None, allow_redirect=False):
diff --git a/app/util/api/jira_clients.py b/app/util/api/jira_clients.py
index 004fc0351..8dd690d68 100644
--- a/app/util/api/jira_clients.py
+++ b/app/util/api/jira_clients.py
@@ -6,6 +6,8 @@
 BATCH_SIZE_USERS = 1000
 BATCH_SIZE_ISSUES = 1000
 
+REINDEX_ERROR_MESSAGE = 'A task could not be found for the given task id'
+
 
 class JiraRestClient(RestClient):
 
@@ -245,3 +247,13 @@ def get_service_desk_info(self):
         api_url = f'{self.host}/rest/plugins/applications/1.0/installed/jira-servicedesk'
         service_desk_info = self.get(api_url, "Could not retrieve JSM info", headers=JSM_EXPERIMENTAL_HEADERS)
         return service_desk_info.json()
+
+    def get_reindex_info(self):
+        api_url = f'{self.host}/rest/api/2/reindex'
+        reindex_info = self.get(api_url, 'Could not retrieve reindex info.', expected_status_codes=[404])
+        if REINDEX_ERROR_MESSAGE in reindex_info.text:
+            raise SystemExit('Jira is unable to perform a background re-index at this time because the index files '
+                             'are either missing or corrupted.\nPlease navigate to System > Indexing. '
+                             'Select the Full re-index option. Click Re-Index and wait until re-indexing is completed.')
+        elif not reindex_info.ok:
+            raise SystemExit('Could not retrieve reindex info.')
diff --git a/app/util/data_preparation/jira_prepare_data.py b/app/util/data_preparation/jira_prepare_data.py
index 8f9acb17a..8f47e097b 100644
--- a/app/util/data_preparation/jira_prepare_data.py
+++ b/app/util/data_preparation/jira_prepare_data.py
@@ -181,6 +181,10 @@ def __check_for_admin_permissions(jira_api):
         raise SystemExit(f"The '{jira_api.user}' user does not have admin permissions.")
 
 
+def __check_reindex_info(jira_api):
+    jira_api.get_reindex_info()
+
+
 def main():
     print("Started preparing data")
 
@@ -191,6 +195,7 @@ def main():
 
     __check_for_admin_permissions(client)
     __check_current_language(client)
+    __check_reindex_info(client)
     dataset = __create_data_set(client)
     write_test_data_to_files(dataset)
 
diff --git a/app/util/data_preparation/jsm_prepare_data.py b/app/util/data_preparation/jsm_prepare_data.py
index 6c8713b01..3e8b0087e 100644
--- a/app/util/data_preparation/jsm_prepare_data.py
+++ b/app/util/data_preparation/jsm_prepare_data.py
@@ -474,6 +474,10 @@ def __write_test_data_to_files(datasets):
     __write_to_file(JSM_DATASET_CUSTOM_ISSUES, issues)
 
 
+def __check_reindex_info(jira_api):
+    jira_api.get_reindex_info()
+
+
 @print_timing('JSM full prepare data', sep='=')
 def main():
     print("Started preparing data")
@@ -482,6 +486,7 @@ def main():
     jsm_client = JsmRestClient(url, JSM_SETTINGS.admin_login, JSM_SETTINGS.admin_password, verify=JSM_SETTINGS.secure,
                                headers=JSM_EXPERIMENTAL_HEADERS)
     jira_client = JiraRestClient(url, JSM_SETTINGS.admin_login, JSM_SETTINGS.admin_password, JSM_SETTINGS.secure)
+    __check_reindex_info(jira_client)
     dataset = __create_data_set(jira_client=jira_client, jsm_client=jsm_client)
     __write_test_data_to_files(dataset)
 


---- SAMPLE #53 ----
Generated commit : Fixing typo in error message
Ground-truth     : Temporarily disable task_fail pre-upgrade duplicates check (#22839)

I am reworking it to actually move the rows but for now we can disable it.
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / -0.0631 / 0.1852
Diff:
diff --git a/airflow/utils/db.py b/airflow/utils/db.py
index 2ebc1e51ed04e..bc4dd9ba912d5 100644
--- a/airflow/utils/db.py
+++ b/airflow/utils/db.py
@@ -1159,7 +1159,7 @@ def _check_migration_errors(session: Session = NEW_SESSION) -> Iterable[str]:
     :rtype: list[str]
     """
     check_functions: Tuple[Callable[..., Iterable[str]], ...] = (
-        check_task_fail_for_duplicates,
+        # todo: check task fail for duplicates
         check_conn_id_duplicates,
         check_conn_type_null,
         check_run_id_null,


---- SAMPLE #54 ----
Generated commit : Fix path lookup (#2187)

Co-authored-by: Ahsan Khan <ahsan.al.zaki@gmail.com>
Ground-truth     : Fix bug when .sky absolute path contains space (#1247)

Fix ~/.sky filepath with space
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0156 / 0.1429 / 0.0746 / 0.0245 / 0.2002
Diff:
diff --git a/sky/backends/backend_utils.py b/sky/backends/backend_utils.py
index 95e80d843e6..235c2ed9e60 100644
--- a/sky/backends/backend_utils.py
+++ b/sky/backends/backend_utils.py
@@ -242,8 +242,11 @@ def _optimize_file_mounts(yaml_path: str) -> None:
 
     # (For local) Move all runtime files, including the just-written yaml, to
     # local_runtime_files_dir/.
-    all_local_sources = ' '.join(
-        local_src for local_src in file_mounts.values())
+    all_local_sources = ''
+    for local_src in file_mounts.values():
+        full_local_src = str(pathlib.Path(local_src).expanduser())
+        # Add quotes for paths containing spaces.
+        all_local_sources += f'{full_local_src!r} '
     # Takes 10-20 ms on laptop incl. 3 clouds' credentials.
     subprocess.run(f'cp -r {all_local_sources} {local_runtime_files_dir}/',
                    shell=True,


---- SAMPLE #55 ----
Generated commit : fix: Updated CLI parsing error message (#2285)
Ground-truth     : Url-decode S3 keys from the event notification (#54)
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / 0.1990 / 0.1823
Diff:
diff --git a/lambda_functions/analyzer/main.py b/lambda_functions/analyzer/main.py
index b033d49..eb12f72 100644
--- a/lambda_functions/analyzer/main.py
+++ b/lambda_functions/analyzer/main.py
@@ -7,6 +7,7 @@
 # Expects a binary YARA rules file to be at './compiled_yara_rules.bin'
 import logging
 import os
+import urllib
 
 from yara import Error as YaraError
 from botocore.exceptions import ClientError as BotoError
@@ -71,7 +72,9 @@ def analyze_lambda_handler(event_data, lambda_context):
 
     LOGGER.info('Processing %d record(s)', len(event_data['S3Objects']))
     for s3_key in event_data['S3Objects']:
-        LOGGER.info('Analyzing %s', s3_key)
+        # S3 keys in event notifications are url-encoded.
+        s3_key = urllib.parse.unquote_plus(s3_key)
+        LOGGER.info('Analyzing "%s"', s3_key)
 
         with binary_info.BinaryInfo(os.environ['S3_BUCKET_NAME'], s3_key, ANALYZER) as binary:
             result[binary.s3_identifier] = binary.summary()
diff --git a/tests/lambda_functions/analyzer/main_test.py b/tests/lambda_functions/analyzer/main_test.py
index c1401c9..370301f 100644
--- a/tests/lambda_functions/analyzer/main_test.py
+++ b/tests/lambda_functions/analyzer/main_test.py
@@ -4,6 +4,7 @@
 import os
 import unittest
 from unittest import mock
+import urllib
 
 import boto3
 from pyfakefs import fake_filesystem_unittest
@@ -18,7 +19,7 @@
     'observed_path': '/path/to/mock-evil.exe',
     'reported_md5': 'REPORTED MD5'
 }
-MOCK_S3_OBJECT_KEY = 'random-uuid'
+MOCK_S3_OBJECT_KEY = 'space plus+file.test'
 
 # Mimics minimal parts of S3:ObjectAdded event that triggers the lambda function.
 LAMBDA_VERSION = 1
@@ -77,7 +78,10 @@ def setUp(self):
         boto3.client = mock.MagicMock(side_effect=self._boto3_client_mock)
 
         # Create test event.
-        self._test_event = {'S3Objects': [MOCK_S3_OBJECT_KEY], 'SQSReceipts': MOCK_SQS_RECEIPTS}
+        self._test_event = {
+            'S3Objects': [urllib.parse.quote_plus(MOCK_S3_OBJECT_KEY)],
+            'SQSReceipts': MOCK_SQS_RECEIPTS
+        }
 
     def tearDown(self):
         """Restore boto3.client to its original."""


---- SAMPLE #56 ----
Generated commit : Fixing bug in filter_mel_element method (#707)
Ground-truth     : handle feed items in ES that don't have feed element data (bug 1088001)
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0127 / 0.1818 / 0.0407 / 0.1407 / 0.1897
Diff:
diff --git a/mkt/feed/views.py b/mkt/feed/views.py
index dd0d75dccb0..f6f6e6873c1 100644
--- a/mkt/feed/views.py
+++ b/mkt/feed/views.py
@@ -483,6 +483,10 @@ def filter_feed_element(self, request, feed_element, item_type):
         If a feed element does not have enough apps, return None.
         Else return the feed element.
         """
+        if not feed_element:
+            # Handle edge case where the ES index might get stale.
+            return None
+
         if request.QUERY_PARAMS.get('filtering', '1') == '0':
             # Without filtering
             return feed_element


---- SAMPLE #57 ----
Generated commit : Fixing typo in pull request (#233)
Ground-truth     : Restore access to the All Projects Translate view (#2914)

By restoring dedentation.
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / 0.1285 / 0.1930
Diff:
diff --git a/pontoon/translate/views.py b/pontoon/translate/views.py
index 5d7cf906ac..c0752ba7f6 100644
--- a/pontoon/translate/views.py
+++ b/pontoon/translate/views.py
@@ -56,9 +56,10 @@ def translate(request, locale, project, resource):
         )
         if isinstance(project, HttpResponseRedirect):
             return project
-    # Validate ProjectLocale
-    if locale not in project.locales.all():
-        raise Http404
+
+        # Validate ProjectLocale
+        if locale not in project.locales.all():
+            raise Http404
 
     context = {
         "locale": get_preferred_locale(request),


---- SAMPLE #58 ----
Generated commit : Pidbox now uses the new signals API to report on errors
Ground-truth     : Bugreport now available as a command and broadcast command

- Get it from a Python repl:

    >>> import celery
    >>> print(celery.bugreport())

- Use celeryctl:

    $ celeryctl report

- Get it from remote workers:

    $ celeryctl inspect report
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0021 / 0.1395 / 0.0436 / -0.1675 / 0.1896
Diff:
diff --git a/celery/app/base.py b/celery/app/base.py
index ad89dea3a38..bd457f81d87 100644
--- a/celery/app/base.py
+++ b/celery/app/base.py
@@ -20,6 +20,7 @@
 from contextlib import contextmanager
 from copy import deepcopy
 from functools import wraps
+from pprint import pformat
 
 from kombu.clocks import LamportClock
 
@@ -27,7 +28,7 @@
 from .. import platforms
 from ..exceptions import AlwaysEagerIgnored
 from ..local import maybe_evaluate
-from ..utils import cached_property, instantiate, lpmerge
+from ..utils import cached_property, instantiate, lpmerge, qualname
 
 from .defaults import DEFAULTS, find_deprecated_settings, find
 
@@ -35,10 +36,16 @@
 if kombu.VERSION < (2, 0):
     raise ImportError("Celery requires Kombu version 1.1.0 or higher.")
 
+SETTINGS_INFO = """%s %s"""
+
 BUGREPORT_INFO = """
 platform -> system:%(system)s arch:%(arch)s imp:%(py_i)s
 software -> celery:%(celery_v)s kombu:%(kombu_v)s py:%(py_v)s
+loader   -> %(loader)s
 settings -> transport:%(transport)s results:%(results)s
+
+%(human_settings)s
+
 """
 
 
@@ -350,7 +357,32 @@ def bugreport(self):
                                  "kombu_v": kombu.__version__,
                                  "py_v": _platform.python_version(),
                                  "transport": self.conf.BROKER_TRANSPORT,
-                                 "results": self.conf.CELERY_RESULT_BACKEND}
+                                 "results": self.conf.CELERY_RESULT_BACKEND,
+                                 "human_settings": self.human_settings(),
+                                 "loader": qualname(self.loader.__class__)}
+
+    def _pformat(self, value, width=80, nl_width=80, **kw):
+
+        if isinstance(value, dict):
+            return "{\n %s" % (
+                                pformat(value, width=nl_width, indent=4, **kw)[1:])
+        elif isinstance(value, tuple):
+            return "\n%s%s" % (' ' * 4,
+                                pformat(value, width=nl_width, **kw))
+        else:
+            return pformat(value, width=width, **kw)
+
+    def human_settings(self):
+        return "\n".join(SETTINGS_INFO % (key + ':',
+                                          self._pformat(value, width=50))
+                    for key, value in self.filter_user_settings().iteritems())
+
+    def filter_user_settings(self):
+        user_settings = {}
+        # the last stash is the default settings, so just skip that
+        for stash in self.conf._order[:-1]:
+            user_settings.update(stash)
+        return user_settings
 
     @property
     def pool(self):
diff --git a/celery/app/control.py b/celery/app/control.py
index ee81f8fa8f0..ba17a8644fa 100644
--- a/celery/app/control.py
+++ b/celery/app/control.py
@@ -49,6 +49,9 @@ def _request(self, command, **kwargs):
                                       callback=self.callback,
                                       timeout=self.timeout, reply=True))
 
+    def report(self):
+        return self._request("report")
+
     def active(self, safe=False):
         return self._request("dump_active", safe=safe)
 
diff --git a/celery/bin/celeryctl.py b/celery/bin/celeryctl.py
index 057fd18250a..cd7de1f8150 100644
--- a/celery/bin/celeryctl.py
+++ b/celery/bin/celeryctl.py
@@ -291,7 +291,8 @@ class inspect(Command):
                "disable_events": 1.0,
                "ping": 0.2,
                "add_consumer": 1.0,
-               "cancel_consumer": 1.0}
+               "cancel_consumer": 1.0,
+               "report": 1.0}
     option_list = Command.option_list + (
                 Option("--timeout", "-t", type="float", dest="timeout",
                     default=None,
@@ -497,6 +498,14 @@ def run(self, *args, **kwargs):
 help = command(help)
 
 
+class report(Command):
+
+    def run(self, *args, **kwargs):
+        print(self.app.bugreport())
+        return EX_OK
+report = command(report)
+
+
 class celeryctl(CeleryCommand):
     commands = commands
     enable_config_from_cmdline = True
diff --git a/celery/worker/control.py b/celery/worker/control.py
index f32413fe017..b8411646e18 100644
--- a/celery/worker/control.py
+++ b/celery/worker/control.py
@@ -50,6 +50,11 @@ def revoke(panel, task_id, terminate=False, signal=None, **kwargs):
     return {"ok": "task %s %s" % (task_id, action)}
 
 
+@Panel.register
+def report(panel):
+    return {"ok": panel.app.bugreport()}
+
+
 @Panel.register
 def enable_events(panel):
     dispatcher = panel.consumer.event_dispatcher


---- SAMPLE #59 ----
Generated commit : Fix web template rendering (#392)
Ground-truth     : web editors can save even if not author (editors are admin)
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0162 / 0.1250 / 0.0481 / 0.0829 / 0.1829
Diff:
diff --git a/knowledge_repo/app/routes/web_editor.py b/knowledge_repo/app/routes/web_editor.py
index 99aa4cfd0..e6dccfb6b 100644
--- a/knowledge_repo/app/routes/web_editor.py
+++ b/knowledge_repo/app/routes/web_editor.py
@@ -139,8 +139,8 @@ def save_post():
     kp = None
     if path in current_repo:
         kp = current_repo.post(path)
-        if g.user.username not in kp.headers['authors']:
-            return json.dumps({'msg': ("Post with path {} already exists and you are not an author!",
+        if g.user.username not in kp.headers['authors'] and g.user.username not in current_repo.config.editors:
+            return json.dumps({'msg': ("Post with path {} already exists and you are not an author!"
                                        "\nPlease try a different path").format(path),
                                'success': False})
 


---- SAMPLE #60 ----
Generated commit : [AIRFLOW-3264] Fix incorrect `ValueError` for GCP extra link when using GCS URL (#4346)
Ground-truth     : Revert "Fix `BIGQUERY_JOB_DETAILS_LINK_FMT` in `BigQueryConsoleLink` (#31457)" (#31935)

This reverts commit c7072c0490cb80b448622a27eb1056576d6b92a4.
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.1379 / 0.0000 / -0.1286 / 0.1957
Diff:
diff --git a/airflow/providers/google/cloud/operators/bigquery.py b/airflow/providers/google/cloud/operators/bigquery.py
index 6fb94f21d7496..2fd6aacf5c618 100644
--- a/airflow/providers/google/cloud/operators/bigquery.py
+++ b/airflow/providers/google/cloud/operators/bigquery.py
@@ -58,7 +58,7 @@
     from airflow.models.taskinstancekey import TaskInstanceKey
     from airflow.utils.context import Context
 
-BIGQUERY_JOB_DETAILS_LINK_FMT = "https://console.cloud.google.com/bigquery?j={project_id}:{location}:{job_id}"
+BIGQUERY_JOB_DETAILS_LINK_FMT = "https://console.cloud.google.com/bigquery?j={job_id}"
 
 
 class BigQueryUIColors(enum.Enum):
@@ -90,17 +90,8 @@ def get_link(
         *,
         ti_key: TaskInstanceKey,
     ):
-        job_id_params = XCom.get_value(key="job_id_params", ti_key=ti_key)
-
-        return (
-            BIGQUERY_JOB_DETAILS_LINK_FMT.format(
-                job_id=job_id_params["job_id"],
-                project_id=job_id_params["project_id"],
-                location=job_id_params["location"],
-            )
-            if job_id_params
-            else ""
-        )
+        job_id = XCom.get_value(key="job_id", ti_key=ti_key)
+        return BIGQUERY_JOB_DETAILS_LINK_FMT.format(job_id=job_id) if job_id else ""
 
 
 @attr.s(auto_attribs=True)
@@ -119,16 +110,13 @@ def get_link(
         *,
         ti_key: TaskInstanceKey,
     ):
-        job_ids_params = XCom.get_value(key="job_id_params", ti_key=ti_key)
-        job_ids = job_ids_params["job_id"]
+        job_ids = XCom.get_value(key="job_id", ti_key=ti_key)
         if not job_ids:
             return None
         if len(job_ids) < self.index:
             return None
         job_id = job_ids[self.index]
-        return BIGQUERY_JOB_DETAILS_LINK_FMT.format(
-            job_id=job_id, project_id=job_ids_params["project_id"], location=job_ids_params["location"]
-        )
+        return BIGQUERY_JOB_DETAILS_LINK_FMT.format(job_id=job_id)
 
 
 class _BigQueryDbHookMixin:
@@ -1196,13 +1184,7 @@ def execute(self, context: Context):
             ]
         else:
             raise AirflowException(f"argument 'sql' of type {type(str)} is neither a string nor an iterable")
-        job_id_params = {
-            "job_id": job_id,
-            "project_id": self.hook.project_id,
-            "location": self.location if self.location else "US",
-        }
-        context["task_instance"].xcom_push(key="job_id_params", value=job_id_params)
-        return job_id
+        context["task_instance"].xcom_push(key="job_id", value=job_id)
 
     def on_kill(self) -> None:
         super().on_kill()
@@ -2745,13 +2727,9 @@ def execute(self, context: Any):
                                 persist_kwargs["dataset_id"] = table["datasetId"]
                                 persist_kwargs["project_id"] = table["projectId"]
                             BigQueryTableLink.persist(**persist_kwargs)
+
         self.job_id = job.job_id
-        job_id_params = {
-            "job_id": job_id,
-            "project_id": self.project_id or self.hook.project_id,
-            "location": self.location if self.location else "US",
-        }
-        context["ti"].xcom_push(key="job_id_params", value=job_id_params)
+        context["ti"].xcom_push(key="job_id", value=self.job_id)
         # Wait for the job to complete
         if not self.deferrable:
             job.result(timeout=self.result_timeout, retry=self.result_retry)
diff --git a/tests/providers/google/cloud/operators/test_bigquery.py b/tests/providers/google/cloud/operators/test_bigquery.py
index 367ef99cbbe1b..25b341a4c38a5 100644
--- a/tests/providers/google/cloud/operators/test_bigquery.py
+++ b/tests/providers/google/cloud/operators/test_bigquery.py
@@ -83,10 +83,6 @@
 }
 TEST_TABLE = "test-table"
 GCP_CONN_ID = "google_cloud_default"
-TEST_JOB_ID_1 = "test-job-id"
-TEST_JOB_ID_2 = "test-123"
-TEST_FULL_JOB_ID = f"{TEST_GCP_PROJECT_ID}:{TEST_DATASET_LOCATION}:{TEST_JOB_ID_1}"
-TEST_FULL_JOB_ID_2 = f"{TEST_GCP_PROJECT_ID}:{TEST_DATASET_LOCATION}:{TEST_JOB_ID_2}"
 
 
 class TestBigQueryCreateEmptyTableOperator:
@@ -676,15 +672,11 @@ def test_bigquery_operator_extra_serialized_field_when_single_query(
 
         # Check DeSerialized version of operator link
         assert isinstance(list(simple_task.operator_extra_links)[0], BigQueryConsoleLink)
-        test_job_id_params = {
-            "job_id": TEST_JOB_ID_1,
-            "project_id": TEST_GCP_PROJECT_ID,
-            "location": TEST_DATASET_LOCATION,
-        }
-        ti.xcom_push("job_id_params", test_job_id_params)
+
+        ti.xcom_push("job_id", 12345)
 
         url = simple_task.get_extra_links(ti, BigQueryConsoleLink.name)
-        assert url == f"https://console.cloud.google.com/bigquery?j={TEST_FULL_JOB_ID}"
+        assert url == "https://console.cloud.google.com/bigquery?j=12345"
 
     @pytest.mark.need_serialized_dag
     def test_bigquery_operator_extra_serialized_field_when_multiple_queries(
@@ -719,23 +711,17 @@ def test_bigquery_operator_extra_serialized_field_when_multiple_queries(
         # Check DeSerialized version of operator link
         assert isinstance(list(simple_task.operator_extra_links)[0], BigQueryConsoleIndexableLink)
 
-        test_job_id_params = {
-            "job_id": [TEST_JOB_ID_1, TEST_JOB_ID_2],
-            "project_id": TEST_GCP_PROJECT_ID,
-            "location": TEST_DATASET_LOCATION,
-        }
-        ti.xcom_push(key="job_id_params", value=test_job_id_params)
+        job_id = ["123", "45"]
+        ti.xcom_push(key="job_id", value=job_id)
 
         assert {"BigQuery Console #1", "BigQuery Console #2"} == simple_task.operator_extra_link_dict.keys()
 
-        assert (
-            f"https://console.cloud.google.com/bigquery?j={TEST_FULL_JOB_ID}"
-            == simple_task.get_extra_links(ti, "BigQuery Console #1")
+        assert "https://console.cloud.google.com/bigquery?j=123" == simple_task.get_extra_links(
+            ti, "BigQuery Console #1"
         )
 
-        assert (
-            f"https://console.cloud.google.com/bigquery?j={TEST_FULL_JOB_ID_2}"
-            == simple_task.get_extra_links(ti, "BigQuery Console #2")
+        assert "https://console.cloud.google.com/bigquery?j=45" == simple_task.get_extra_links(
+            ti, "BigQuery Console #2"
         )
 
     @mock.patch("airflow.providers.google.cloud.operators.bigquery.BigQueryHook")
@@ -754,9 +740,7 @@ def test_bigquery_operator_extra_link_when_missing_job_id(
 
     @mock.patch("airflow.providers.google.cloud.operators.bigquery.BigQueryHook")
     def test_bigquery_operator_extra_link_when_single_query(
-        self,
-        mock_hook,
-        create_task_instance_of_operator,
+        self, mock_hook, create_task_instance_of_operator
     ):
         ti = create_task_instance_of_operator(
             BigQueryExecuteQueryOperator,
@@ -767,15 +751,11 @@ def test_bigquery_operator_extra_link_when_single_query(
         )
         bigquery_task = ti.task
 
-        test_job_id_params = {
-            "job_id": TEST_JOB_ID_1,
-            "project_id": TEST_GCP_PROJECT_ID,
-            "location": TEST_DATASET_LOCATION,
-        }
-        ti.xcom_push(key="job_id_params", value=test_job_id_params)
-        assert (
-            f"https://console.cloud.google.com/bigquery?j={TEST_FULL_JOB_ID}"
-            == bigquery_task.get_extra_links(ti, BigQueryConsoleLink.name)
+        job_id = "12345"
+        ti.xcom_push(key="job_id", value=job_id)
+
+        assert f"https://console.cloud.google.com/bigquery?j={job_id}" == bigquery_task.get_extra_links(
+            ti, BigQueryConsoleLink.name
         )
 
     @mock.patch("airflow.providers.google.cloud.operators.bigquery.BigQueryHook")
@@ -791,22 +771,17 @@ def test_bigquery_operator_extra_link_when_multiple_query(
         )
         bigquery_task = ti.task
 
-        test_job_id_params = {
-            "job_id": [TEST_JOB_ID_1, TEST_JOB_ID_2],
-            "project_id": TEST_GCP_PROJECT_ID,
-            "location": TEST_DATASET_LOCATION,
-        }
-        ti.xcom_push(key="job_id_params", value=test_job_id_params)
+        job_id = ["123", "45"]
+        ti.xcom_push(key="job_id", value=job_id)
+
         assert {"BigQuery Console #1", "BigQuery Console #2"} == bigquery_task.operator_extra_link_dict.keys()
 
-        assert (
-            f"https://console.cloud.google.com/bigquery?j={TEST_FULL_JOB_ID}"
-            == bigquery_task.get_extra_links(ti, "BigQuery Console #1")
+        assert "https://console.cloud.google.com/bigquery?j=123" == bigquery_task.get_extra_links(
+            ti, "BigQuery Console #1"
         )
 
-        assert (
-            f"https://console.cloud.google.com/bigquery?j={TEST_FULL_JOB_ID_2}"
-            == bigquery_task.get_extra_links(ti, "BigQuery Console #2")
+        assert "https://console.cloud.google.com/bigquery?j=45" == bigquery_task.get_extra_links(
+            ti, "BigQuery Console #2"
         )
 
 


---- SAMPLE #61 ----
Generated commit : [AIRFLOW-XXX] Remove deprecated docstring (#3936)
Ground-truth     : Pymssql is maintained again (#11537)

See https://github.com/pymssql/pymssql/commit/20457589b2878800cb40c1dd4f3045a4bb259675
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / -0.2035 / 0.1867
Diff:
diff --git a/airflow/providers/microsoft/mssql/hooks/mssql.py b/airflow/providers/microsoft/mssql/hooks/mssql.py
index 10416aae4a698..55c1f2b188c8d 100644
--- a/airflow/providers/microsoft/mssql/hooks/mssql.py
+++ b/airflow/providers/microsoft/mssql/hooks/mssql.py
@@ -16,10 +16,7 @@
 # specific language governing permissions and limitations
 # under the License.
 """
-This module is deprecated due to the discontinuation of the pymssql project.
-See https://github.com/pymssql/pymssql/issues/668.
-Support for pymssql will be removed in Airflow 2.0.
-Please use :mod:`~airflow.providers.odbc.hooks.odbc`.
+Microsoft SQLServer hook module
 """
 
 import warnings
@@ -28,25 +25,9 @@
 
 from airflow.hooks.dbapi_hook import DbApiHook
 
-warnings.warn(
-    (
-        "This module is deprecated due to the discontinuation of the pymssql project.\n"
-        "See https://github.com/pymssql/pymssql/issues/668.\n"
-        "Support for pymssql will be removed in airflow 2.0.\n"
-        "Please use `airflow.providers.odbc.hooks.odbc`.\n"
-    ),
-    DeprecationWarning,
-    stacklevel=2,
-)
-
 
 class MsSqlHook(DbApiHook):
     """
-    .. warning::
-
-        This class is deprecated.
-        Please use :py:class:`~airflow.providers.odbc.hooks.odbc.OdbcHook`.
-
     Interact with Microsoft SQL Server.
     """
 


---- SAMPLE #62 ----
Generated commit : Fixing minor issues in airflow
Ground-truth     : Follow symlinks in DAG and Plugin folders
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0360 / 0.1667 / 0.0735 / 0.0785 / 0.1877
Diff:
diff --git a/airflow/models.py b/airflow/models.py
index 3cf7942c24fa0..59aae74707ab0 100644
--- a/airflow/models.py
+++ b/airflow/models.py
@@ -233,7 +233,7 @@ def collect_dags(
             self.process_file(dag_folder, only_if_updated=only_if_updated)
         elif os.path.isdir(dag_folder):
             patterns = []
-            for root, dirs, files in os.walk(dag_folder):
+            for root, dirs, files in os.walk(dag_folder, followlinks=True):
                 ignore_file = [f for f in files if f == '.airflowignore']
                 if ignore_file:
                     f = open(os.path.join(root, ignore_file[0]), 'r')
diff --git a/airflow/plugins_manager.py b/airflow/plugins_manager.py
index 60ac4432d31be..9dc114108c704 100644
--- a/airflow/plugins_manager.py
+++ b/airflow/plugins_manager.py
@@ -35,7 +35,7 @@ def validate(cls):
 plugins = []
 
 # Crawl through the plugins folder to find AirflowPlugin derivatives
-for root, dirs, files in os.walk(plugins_folder):
+for root, dirs, files in os.walk(plugins_folder, followlinks=True):
     for f in files:
         try:
             filepath = os.path.join(root, f)


---- SAMPLE #63 ----
Generated commit : Merge branch 'master' of https://github.com/openxla/intelligence
Ground-truth     : Add term_vector to mapping for highlighted content
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / -0.0907 / 0.1729
Diff:
diff --git a/apps/forums/es_search.py b/apps/forums/es_search.py
index b6845c47f4c..2b55fbe9dc8 100644
--- a/apps/forums/es_search.py
+++ b/apps/forums/es_search.py
@@ -18,6 +18,8 @@
 TYPE = 'type'
 ANALYZER = 'analyzer'
 INDEX = 'index'
+TERM_VECTOR = 'term_vector'
+STORE = 'store'
 
 INTEGER = 'integer'
 STRING = 'string'
@@ -26,8 +28,12 @@
 
 ANALYZED = 'analyzed'
 
+YES = 'yes'
+
 SNOWBALL = 'snowball'
 
+WITH_POS_OFFSETS = 'with_positions_offsets'
+
 
 def setup_mapping(index):
     from forums.models import Post
@@ -46,7 +52,8 @@ def setup_mapping(index):
             'is_locked': {TYPE: BOOLEAN},
             'author_id': {TYPE: INTEGER},
             'author_ord': {TYPE: STRING},
-            'content': {TYPE: STRING, INDEX: ANALYZED, ANALYZER: SNOWBALL},
+            'content': {TYPE: STRING, INDEX: ANALYZED, ANALYZER: SNOWBALL,
+                        STORE: YES, TERM_VECTOR: WITH_POS_OFFSETS},
             'created': {TYPE: DATE},
             'updated': {TYPE: DATE},
             'age': {TYPE: INTEGER},
diff --git a/apps/questions/es_search.py b/apps/questions/es_search.py
index 08d897602ac..7b8d315c5ee 100644
--- a/apps/questions/es_search.py
+++ b/apps/questions/es_search.py
@@ -18,6 +18,8 @@
 TYPE = 'type'
 ANALYZER = 'analyzer'
 INDEX = 'index'
+STORE = 'store'
+TERM_VECTOR = 'term_vector'
 
 LONG = 'long'
 INTEGER = 'integer'
@@ -29,6 +31,10 @@
 
 SNOWBALL = 'snowball'
 
+YES = 'yes'
+
+WITH_POS_OFFSETS = 'with_positions_offsets'
+
 
 def setup_mapping(index):
     from questions.models import Question
@@ -43,7 +49,8 @@ def setup_mapping(index):
             'question_id': {TYPE: LONG},
             'title': {TYPE: STRING, INDEX: ANALYZED, ANALYZER: SNOWBALL},
             'question_content':
-                {TYPE: STRING, INDEX: ANALYZED, ANALYZER: SNOWBALL},
+                {TYPE: STRING, INDEX: ANALYZED, ANALYZER: SNOWBALL,
+                 STORE: YES, TERM_VECTOR: WITH_POS_OFFSETS},
             'answer_content':
                 {TYPE: STRING, INDEX: ANALYZED, ANALYZER: SNOWBALL},
             'replies': {TYPE: INTEGER},


---- SAMPLE #64 ----
Generated commit : fix: Updated container label to make it possible for running images (#1870)
Ground-truth     : Add support for specific Docker tag in spawn_data_pipeline.py (#489)
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0170 / 0.0833 / 0.1075 / 0.2415 / 0.4526
Diff:
diff --git a/infra/spawn_data_pipeline.py b/infra/spawn_data_pipeline.py
index 3d3cb637be..a38b76fc52 100644
--- a/infra/spawn_data_pipeline.py
+++ b/infra/spawn_data_pipeline.py
@@ -69,6 +69,8 @@ def main():
 
     id_mapping = {}
 
+    docker_tag = os.getenv("TAG", None)
+
     # First pass, do the template rendering and dependencies resolution
     tasks = []
 
@@ -105,6 +107,20 @@ def main():
 
         payload["dependencies"] = new_dependencies
 
+        # Override the Docker image tag if needed
+        if docker_tag:
+            base_image = payload["payload"]["image"]
+            splitted_image = base_image.rsplit(":", 1)
+
+            if len(splitted_image) > 1:
+                err_msg = "Docker tag should be None or 'latest', not {!r}"
+                assert splitted_image[1] == "latest", err_msg.format(splitted_image[1])
+
+            tagless_image = splitted_image[0]
+
+            new_image = "{}:{}".format(tagless_image, docker_tag)
+            payload["payload"]["image"] = new_image
+
         tasks.append((task_id, payload))
 
     # Now sends them


---- SAMPLE #65 ----
Generated commit : Fix bug in some places where the wrong type is used by hamminging the year
Ground-truth     : Fix /api/v1/profile for user with mult. RealPhones
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0132 / 0.0833 / 0.0641 / -0.0892 / 0.1800
Diff:
diff --git a/privaterelay/models.py b/privaterelay/models.py
index 39764f3bc7..5ac0bd6189 100644
--- a/privaterelay/models.py
+++ b/privaterelay/models.py
@@ -378,7 +378,7 @@ def date_phone_registered(self) -> datetime | None:
         from phones.models import RealPhone, RelayNumber
 
         try:
-            real_phone = RealPhone.objects.get(user=self.user)
+            real_phone = RealPhone.objects.get(user=self.user, verified=True)
             relay_number = RelayNumber.objects.get(user=self.user)
         except RealPhone.DoesNotExist:
             return None
diff --git a/privaterelay/tests/model_tests.py b/privaterelay/tests/model_tests.py
index e97640fa82..c552dcfac2 100644
--- a/privaterelay/tests/model_tests.py
+++ b/privaterelay/tests/model_tests.py
@@ -263,6 +263,20 @@ def test_real_phone_no_relay_number_returns_verified_date(self) -> None:
         )
         assert self.profile.date_phone_registered == datetime_now
 
+    def test_two_real_phones_returns_verified_date(self) -> None:
+        self.upgrade_to_phone()
+        datetime_now = datetime.now(UTC)
+        RealPhone.objects.create(
+            user=self.profile.user, number="+12223335555", verified=False
+        )
+        RealPhone.objects.create(
+            user=self.profile.user,
+            number="+12223334444",
+            verified=True,
+            verified_date=datetime_now,
+        )
+        assert self.profile.date_phone_registered == datetime_now
+
     def test_real_phone_and_relay_number_w_created_at_returns_created_at_date(
         self,
     ) -> None:


---- SAMPLE #66 ----
Generated commit : Add missing import and add tests (#609)
Ground-truth     : Add a method to retrieve the testing tag associated to a Phabricator revision
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0140 / 0.2000 / 0.0806 / 0.1136 / 0.1840
Diff:
diff --git a/bugbug/phabricator.py b/bugbug/phabricator.py
index 5f5929b4be..6af772af9d 100644
--- a/bugbug/phabricator.py
+++ b/bugbug/phabricator.py
@@ -8,7 +8,7 @@
 
 PHABRICATOR_API = None
 
-PROJECTS = {
+TESTING_PROJECTS = {
     "PHID-PROJ-h7y4cs7m2o67iczw62pp": "testing-approved",
     "PHID-PROJ-e4fcjngxcws3egiecv3r": "testing-exception-elsewhere",
     "PHID-PROJ-iciyosoekrczpf2a4emw": "testing-exception-other",
@@ -46,3 +46,11 @@ def get(rev_ids):
             progress_bar.update(len(rev_ids_group))
 
     return data
+
+
+def get_testing_projects(rev):
+    return (
+        TESTING_PROJECTS[projectPHID]
+        for projectPHID in rev["attachments"]["projects"]["projectPHIDs"]
+        if projectPHID in TESTING_PROJECTS
+    )


---- SAMPLE #67 ----
Generated commit : Remove unused code path
Ground-truth     : Adapt QueryInterface specifically for localities since it isn't used for events anymore
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / 0.1266 / 0.1773
Diff:
diff --git a/alerts/geomodel/locality.py b/alerts/geomodel/locality.py
index 97ed24fb0..e120c35b8 100644
--- a/alerts/geomodel/locality.py
+++ b/alerts/geomodel/locality.py
@@ -5,7 +5,6 @@
 from mozdef_util.query_models import SearchQuery, TermMatch
 
 import alerts.geomodel.config as config
-import alerts.geomodel.query as query
 
 
 # Default radius (in Kilometres) that a locality should have.
@@ -13,10 +12,6 @@
 
 # TODO: Switch to dataclasses when we move to Python3.7+
 
-def _dict_take(dictionary, keys):
-    return {key: dictionary[key] for key in keys}
-
-
 class Locality(NamedTuple):
     '''Represents a specific locality.
     '''
@@ -29,7 +24,6 @@ class Locality(NamedTuple):
     longitude: float
     radius: int
 
-
 class State(NamedTuple):
     '''Represents the state tracked for each user regarding their localities.
     '''
@@ -49,23 +43,6 @@ class Entry(NamedTuple):
     identifier: Optional[str]
     state: State
 
-JournalInterface = Callable[[List[Entry], str], None]
-
-def wrap_journal(client: ESClient) -> JournalInterface:
-    '''Wrap an `ElasticsearchClient` in a closure of type `JournalInterface`.
-    '''
-
-    def wrapper(entries: List[Entry], esindex: str):
-        for entry in entries:
-            document = dict(entry.state._asdict())
-
-            client.save_object(
-                index=esindex,
-                body=document,
-                doc_id=entry.identifier)
-
-    return wrapper
-
 class Update(NamedTuple):
     '''Produced by calls to functions operating on lists of `State`s to
     indicate when an update was applied without having to maintain distinct
@@ -86,6 +63,12 @@ def flat_map(fn: Callable[[State], 'Update'], u: 'Update') -> 'Update':
 
         return Update(new.state, u.did_update or new.did_update)
 
+JournalInterface = Callable[[List[Entry], str], None]
+QueryInterface = Callable[[SearchQuery, str], List[Entry]]
+
+def _dict_take(dictionary, keys):
+    return {key: dictionary[key] for key in keys}
+
 def _update(state: State, from_evt: State) -> Update:
     did_update = False
 
@@ -115,6 +98,52 @@ def _update(state: State, from_evt: State) -> Update:
 
     return Update(state, did_update)
 
+def wrap_journal(client: ESClient) -> JournalInterface:
+    '''Wrap an `ElasticsearchClient` in a closure of type `JournalInterface`.
+    '''
+
+    def wrapper(entries: List[Entry], esindex: str):
+        for entry in entries:
+            document = dict(entry.state._asdict())
+
+            client.save_object(
+                index=esindex,
+                body=document,
+                doc_id=entry.identifier)
+
+    return wrapper
+
+def wrap_query(client: ESClient) -> QueryInterface:
+    '''Wrap an `ElasticsearchClient` in a closure of type `QueryInterface`.
+    '''
+
+    def to_state(result: Dict[str, Any]) -> Optional[State]:
+        try:
+            result['localities'] = [
+                Locality(**_dict_take(loc, Locality._fields))
+                for loc in result['localities']
+            ]
+
+            return State(**_dict_take(result, State._fields))
+        except TypeError:
+            return None
+        except KeyError:
+            return None
+
+    def wrapper(query: SearchQuery, esindex: str) -> List[Entry]:
+        results = query.execute(client, indices=[esindex]).get('hits', [])
+
+        entries = []
+        for event in results:
+            opt_state = to_state(event.get('_source', {}))
+
+            if opt_state is not None:
+                entries.append(Entry(event['_id'], opt_state))
+
+        return entries
+
+    return wrapper
+
 def from_event(
         event: Dict[str, Any],
         radius=_DEFAULT_RADIUS_KM
@@ -151,38 +180,16 @@ def from_event(
         radius)
 
 def find_all(
-        query_es: query.QueryInterface,
+        query_es: QueryInterface,
         locality: config.Localities
 ) -> List[Entry]:
     '''Retrieve all locality state from ElasticSearch.
     '''
 
-    def to_state(result: Dict[str, Any]) -> Optional[State]:
-        try:
-            result['localities'] = [
-                Locality(**_dict_take(loc, Locality._fields))
-                for loc in result['localities']
-            ]
-
-            return State(**_dict_take(result, State._fields))
-        except TypeError:
-            return None
-        except KeyError:
-            return None
-
     search = SearchQuery()
     search.add_must([TermMatch('type_', 'locality')])
 
-    results = query_es(search, locality.es_index)
-
-    entries = []
-    for result in results:
-        state = to_state(result['_source'])
-
-        if state is not None:
-            entries.append(Entry(result['_id'], state))
-
-    return entries
+    return query_es(search, locality.es_index)
 
 def merge(persisted: List[State], event_sourced: List[State]) -> List[Update]:
     '''Merge together a list of states already stored in ElasticSearch
diff --git a/alerts/geomodel/query.py b/alerts/geomodel/query.py
deleted file mode 100644
index 26a39d8c0..000000000
--- a/alerts/geomodel/query.py
+++ /dev/null
@@ -1,23 +0,0 @@
-'''To make GeoModel code more testable, we abstract interaction with
-ElasticSearch away via a "query interface".  This is just a function that,
-called with an ES index and a `SearchQuery`, produces a list of dictionaries
-as output.
-'''
-
-from typing import Any, Callable, Dict, List
-
-from mozdef_util.elasticsearch_client import ElasticsearchClient as ESClient
-from mozdef_util.query_models import SearchQuery
-
-
-QueryInterface = Callable[[SearchQuery, str], List[Dict[str, Any]]]
-
-
-def wrap(client: ESClient) -> QueryInterface:
-    '''Wrap an `ElasticsearchClient` in a closure of type `QueryInterface`.
-    '''
-
-    def wrapper(query: SearchQuery, esindex: str) -> List[Dict[str, Any]]:
-        return query.execute(client, indices=[esindex]).get('hits', [])
-
-    return wrapper
diff --git a/tests/alerts/geomodel/test_locality.py b/tests/alerts/geomodel/test_locality.py
index 0a47cf48b..5fa7d78db 100644
--- a/tests/alerts/geomodel/test_locality.py
+++ b/tests/alerts/geomodel/test_locality.py
@@ -1,15 +1,25 @@
 from datetime import datetime, timedelta
 import pytz
+from typing import Any, Dict, List
 import unittest
 
+from mozdef_util.query_models import SearchQuery
+
 import alerts.geomodel.config as config
 import alerts.geomodel.locality as locality
-import alerts.geomodel.query as query
 
-from tests.alerts.geomodel.util import query_interface
 from tests.unit_test_suite import UnitTestSuite
 
 
+def query_interface(results: List[locality.Entry]) -> locality.QueryInterface:
+    '''Produce a `QueryInterface` that just returns the provided results.
+    '''
+
+    def closure(q: SearchQuery, esi: str) -> List[locality.Entry]:
+        return results
+
+    return closure
+
 class TestLocalityElasticSearch(UnitTestSuite):
     '''Tests for the `locality` module that interact with ES.
     '''
@@ -38,7 +48,7 @@ def test_simple_query(self):
 
         self.refresh(self.event_index_name)
 
-        query_iface = query.wrap(self.es_client)
+        query_iface = locality.wrap_query(self.es_client)
         loc_cfg = config.Localities(self.event_index_name, 30, 50.0)
 
         entries = locality.find_all(query_iface, loc_cfg)
@@ -88,7 +98,7 @@ def test_journaling(self):
         
         self.refresh(self.event_index_name)
 
-        query_iface = query.wrap(self.es_client)
+        query_iface = locality.wrap_query(self.es_client)
         loc_cfg = config.Localities(self.event_index_name, 30, 50.0)
 
         retrieved = locality.find_all(query_iface, loc_cfg)
@@ -102,42 +112,26 @@ class TestLocality(unittest.TestCase):
 
     def test_find_all_retrieves_all_states(self):
         query_iface = query_interface([
-            {
-                '_id': 'id1',
-                '_source': {
-                    'type_': 'locality',
-                    'username': 'tester1',
-                    'localities': [
-                        {
-                            'sourceipaddress': '1.2.3.4',
-                            'city': 'Toronto',
-                            'country': 'CA',
-                            'lastaction': datetime.utcnow(),
-                            'latitude': 43.6529,
-                            'longitude': -79.3849,
-                            'radius': 50
-                        }
-                    ]
-                }
-            },
-            {
-                '_id': 'id2',
-                '_source': {
-                    'type_': 'locality',
-                    'username': 'tester2',
-                    'localities': [
-                        {
-                            'sourceipaddress': '4.3.2.1',
-                            'city': 'San Francisco',
-                            'country': 'USA',
-                            'lastaction': datetime.utcnow(),
-                            'latitude': 37.773972,
-                            'longitude': -122.431297,
-                            'radius': 50
-                        }
-                    ]
-                }
-            }
+            locality.Entry('id1', locality.State('locality', 'tester1', [
+                locality.Locality(
+                    sourceipaddress='1.2.3.4',
+                    city='Toronto',
+                    country='CA',
+                    lastaction=datetime.utcnow() - timedelta(minutes=3),
+                    latitude=43.6529,
+                    longitude=-79.3849,
+                    radius=50)
+            ])),
+            locality.Entry('id2', locality.State('locality', 'tester2', [
+                locality.Locality(
+                    sourceipaddress='4.3.2.1',
+                    city='San Francisco',
+                    country='USA',
+                    lastaction=datetime.utcnow(),
+                    latitude=37.773972,
+                    longitude=-122.431297,
+                    radius=50)
+            ]))
         ])
         loc_cfg = config.Localities('localities', 30, 50.0)
 
@@ -152,55 +146,6 @@ def test_find_all_retrieves_all_states(self):
         assert len(entries[0].state.localities) == 1
         assert len(entries[1].state.localities) == 1
 
-    def test_find_all_ignores_invalid_data(self):
-        query_iface = query_interface([
-            # Invalid top-level State
-            {
-                '_id': 'id1',
-                '_source': {
-                    'type__': 'locality',  # Should have only one underscore (_)
-                    'username': 'tester',
-                    'localities': []
-                }
-            },
-            # Valid State
-            {
-                '_id': 'id2',
-                '_source': {
-                    'type_': 'locality',
-                    'username': 'validtester',
-                    'localities': []
-                }
-            },
-            # Invalid locality data
-            {
-                '_id': 'id3',
-                '_source': {
-                    'type_': 'locality',
-                    'username': 'tester2',
-                    'localities': [
-                        {
-                            # Should be sourceipaddress; missing a 'd'
-                            'sourceipadress': '1.2.3.4',
-                            'city': 'San Francisco',
-                            'country': 'USA',
-                            'lastaction': datetime.utcnow(),
-                            'latitude': 37.773972,
-                            'longitude': -122.431297,
-                            'radius': 50
-                        }
-                    ]
-                }
-            }
-        ])
-        loc_cfg = config.Localities('localities', 30, 50.0)
-
-        entries = locality.find_all(query_iface, loc_cfg)
-        usernames = [entry.state.username for entry in entries]
-
-        assert len(entries) == 1
-        assert usernames == ['validtester']
-
     def test_merge_updates_localities(self):
         from_es = [
             locality.State('locality', 'user1', [
diff --git a/tests/alerts/geomodel/util.py b/tests/alerts/geomodel/util.py
deleted file mode 100644
index 9dd88df9a..000000000
--- a/tests/alerts/geomodel/util.py
+++ /dev/null
@@ -1,15 +0,0 @@
-from typing import Any, Dict, List
-
-from mozdef_util.query_models import SearchQuery
-
-import alerts.geomodel.query as query
-
-
-def query_interface(results: List[Dict[str, Any]]) -> query.QueryInterface:
-    '''Produce a `QueryInterface` that just returns the provided results.
-    '''
-
-    def closure(q: SearchQuery, esi: str) -> List[Dict[str, Any]]:
-        return results
-
-    return closure


---- SAMPLE #68 ----
Generated commit : Task: Sentrate limiter as it was not being set
Ground-truth     : Removed Task.rate_limit_queue_type, as it was not really useful and makes refactoring easier.
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.2139 / 0.4800 / 0.3392 / 0.1557 / 0.1957
Diff:
diff --git a/celery/task/base.py b/celery/task/base.py
index 06b0c2df671..2c7ceee5064 100644
--- a/celery/task/base.py
+++ b/celery/task/base.py
@@ -1,7 +1,6 @@
 import sys
 import warnings
 from datetime import datetime, timedelta
-from Queue import Queue
 
 from billiard.serialization import pickle
 
@@ -128,12 +127,6 @@ class Task(object):
         limit), ``"100/s"`` (hundred tasks a second), ``"100/m"`` (hundred
         tasks a minute), ``"100/h"`` (hundred tasks an hour)
 
-    .. attribute:: rate_limit_queue_type
-
-        Type of queue used by the rate limiter for this kind of tasks.
-        Default is a :class:`Queue.Queue`, but you can change this to
-        a :class:`Queue.LifoQueue` or an invention of your own.
-
     .. attribute:: ignore_result
 
         Don't store the return value of this task.
@@ -200,7 +193,6 @@ class Task(object):
     default_retry_delay = 3 * 60
     serializer = conf.TASK_SERIALIZER
     rate_limit = conf.DEFAULT_RATE_LIMIT
-    rate_limit_queue_type = Queue
     backend = default_backend
     exchange_type = conf.DEFAULT_EXCHANGE_TYPE
     delivery_mode = conf.DEFAULT_DELIVERY_MODE


---- SAMPLE #69 ----
Generated commit : Add CALP model stubs
Ground-truth     : [WIP] Create API endpoint that aggregates Darjeeling collections (bug 994304)
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / 0.0392 / 0.1766
Diff:
diff --git a/mkt/api/v1/urls.py b/mkt/api/v1/urls.py
index d2dae1540b2..fc091337452 100644
--- a/mkt/api/v1/urls.py
+++ b/mkt/api/v1/urls.py
@@ -57,6 +57,7 @@
 
 urlpatterns = patterns('',
     url('', include('mkt.fireplace.urls')),
+    url('', include('mkt.darjeeling.urls')),
     url(r'^apps/', include(apps.urls)),
     url(r'^apps/app/', include(subapps.urls)),
     url(r'^apps/search/featured/', FeaturedSearchView.as_view(),
diff --git a/mkt/darjeeling/__init__.py b/mkt/darjeeling/__init__.py
new file mode 100644
index 00000000000..e69de29bb2d
diff --git a/mkt/darjeeling/tests/__init__.py b/mkt/darjeeling/tests/__init__.py
new file mode 100644
index 00000000000..e69de29bb2d
diff --git a/mkt/darjeeling/urls.py b/mkt/darjeeling/urls.py
new file mode 100644
index 00000000000..d2e92549d9b
--- /dev/null
+++ b/mkt/darjeeling/urls.py
@@ -0,0 +1,10 @@
+from django.conf.urls import patterns, url
+
+from mkt.darjeeling.views import DarjeelingAppList
+
+
+urlpatterns = patterns('',
+    url(r'^darjeeling/list/',
+        DarjeelingAppList.as_view(),
+        name='darjeeling-list'),
+)
diff --git a/mkt/darjeeling/views.py b/mkt/darjeeling/views.py
new file mode 100644
index 00000000000..68ac3559678
--- /dev/null
+++ b/mkt/darjeeling/views.py
@@ -0,0 +1,79 @@
+import json
+
+from django.http import HttpResponse
+
+from rest_framework.generics import ListAPIView
+from rest_framework.serializers import SerializerMethodField
+
+from mkt.api.base import CORSMixin, MarketplaceView
+from mkt.collections.models import Collection
+from mkt.fireplace.api import (FireplaceCollectionMembershipField,
+                               FireplaceESAppSerializer)
+
+
+class FakeCollection(object):
+    def __init__(self, pk):
+        self.pk = pk
+
+
+class DarjeelingESAppSerializer(FireplaceESAppSerializer):
+    featured = SerializerMethodField('is_featured')
+
+    class Meta(FireplaceESAppSerializer.Meta):
+        fields = sorted(FireplaceESAppSerializer.Meta.fields + ['featured'])
+        exclude = FireplaceESAppSerializer.Meta.exclude
+
+    def is_featured(self, obj):
+        collections = [c['id'] for c in obj.es_data.get('collection', [])]
+        return self.context['featured_pk'] in collections
+
+
+class DarjeelingCollectionMembershipField(FireplaceCollectionMembershipField):
+    app_serializer_classes = {
+        'es': DarjeelingESAppSerializer,
+    }
+
+
+class DarjeelingAppList(CORSMixin, MarketplaceView, ListAPIView):
+    """
+    Endpoint that darjeeling client consumes to fetch its app list. The list is
+    actually made of 2 things:
+    - One collection called "darjeeling-apps" containg all apps;
+    - One collection called "darjeeling-featured" containing all homepage apps.
+
+    The first list is returned directly (without pagination) and since the
+    second one is just supposed to be a subset of the first, only the app ids
+    are returned.
+    """
+    cors_allowed_methods = ['get']
+    authentication_classes = []
+    permission_classes = []
+
+    def get_collection(self, slug):
+        """
+        Return a Fake Collection object with only the pk, for use with
+        CollectionMembershipField. We can't simply do a Collection.objects.only
+        query, because transforms get in the way (no_transforms doesn't remove
+        translations atm)
+        """
+        pk = Collection.objects.filter(slug=slug).values_list('pk', flat=True)
+        return FakeCollection(pk[0])
+
+    def get_queryset(self):
+        """
+        Fetch (and directly serialize using fireplace serializer) all apps
+        belonging to the 'all' collection by querying ES.
+        """
+        collection_all = self.get_collection('darjeeling-apps')
+        membership = DarjeelingCollectionMembershipField(many=True)
+        membership.context = self.get_serializer_context()
+        membership.context['use-es-for-apps'] = True
+        membership.context['featured_pk'] = (
+            self.get_collection('darjeeling-featured').pk)
+        return membership.field_to_native_es(collection_all, self.request)
+
+    def list(self, request, *args, **kwargs):
+        data = {}
+        data['all'] = self.get_queryset()
+        data['featured'] = [d['id'] for d in data['all'] if d['featured']]
+        return HttpResponse(json.dumps(data), content_type='application/json')


---- SAMPLE #70 ----
Generated commit : Search distance calculation to be compatible
Ground-truth     : Refactor similarity classes moving the duplicate model evaluation directly in the function to find similar bugs (#1588)
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0065 / 0.0870 / 0.0314 / 0.0124 / 0.1927
Diff:
diff --git a/bugbug/similarity.py b/bugbug/similarity.py
index 4c39427d23..e324a5d7d5 100644
--- a/bugbug/similarity.py
+++ b/bugbug/similarity.py
@@ -82,7 +82,11 @@ def download_and_load_similarity_model(model_name):
 
 class BaseSimilarity(abc.ABC):
     def __init__(
-        self, cleanup_urls=True, nltk_tokenizer=False, confidence_threshold=0.8
+        self,
+        cleanup_urls=True,
+        nltk_tokenizer=False,
+        confidence_threshold=0.8,
+        end_to_end=False,
     ):
         self.cleanup_functions = [
             feature_cleanup.responses(),
@@ -98,6 +102,10 @@ def __init__(
         self.nltk_tokenizer = nltk_tokenizer
         self.confidence_threshold = confidence_threshold
 
+        self.duplicatemodel = (
+            DuplicateModel.load("duplicatemodel") if end_to_end else None
+        )
+
     def get_text(self, bug, all_comments=False):
         if all_comments:
             comments = " ".join(comment["text"] for comment in bug["comments"])
@@ -106,6 +114,29 @@ def get_text(self, bug, all_comments=False):
 
         return "{} {}".format(bug["summary"], comments)
 
+    def get_similar_bugs(self, bug):
+        similar_bug_ids = self.search_similar_bugs(bug)
+        if self.duplicatemodel:
+            similar_bugs = [
+                bug for bug in bugzilla.get_bugs() if bug["id"] in similar_bug_ids
+            ]
+            bug_couples = [(bug, similar_bug) for similar_bug in similar_bugs]
+            probs_bug_couples = sorted(
+                zip(
+                    self.duplicatemodel.classify(bug_couples, probabilities=True),
+                    bug_couples,
+                ),
+                key=lambda v: -v[0][1],
+            )
+
+            similar_bug_ids = [
+                similar_bug["id"]
+                for prob, (bug, similar_bug) in probs_bug_couples
+                if prob[1] > self.confidence_threshold
+            ]
+
+        return similar_bug_ids
+
     def text_preprocess(self, text, stemming=True, lemmatization=False, join=False):
 
         for func in self.cleanup_functions:
@@ -134,7 +165,7 @@ def text_preprocess(self, text, stemming=True, lemmatization=False, join=False):
             return " ".join(word for word in text)
         return text
 
-    def evaluation(self, end_to_end=False):
+    def evaluation(self):
         # A map from bug ID to its duplicate IDs
         duplicates = defaultdict(set)
         all_ids = set(
@@ -144,9 +175,6 @@ def evaluation(self, end_to_end=False):
             and "dupeme" not in bug["keywords"]
         )
 
-        if end_to_end:
-            duplicatemodel = DuplicateModel.load("duplicatemodel")
-
         for bug in bugzilla.get_bugs():
             dupes = [entry for entry in bug["duplicates"] if entry in all_ids]
             if bug["dupe_of"] in all_ids:
@@ -176,17 +204,6 @@ def evaluation(self, end_to_end=False):
                 num_hits = 0
                 queries += 1
                 similar_bugs = self.get_similar_bugs(bug)[:10]
-                if end_to_end:
-                    sim_bugs = [
-                        bug for bug in bugzilla.get_bugs() if bug["id"] in similar_bugs
-                    ]
-                    bug_couples = [(bug, sim_bugs[bug_id]) for bug_id in sim_bugs]
-                    probs = duplicatemodel.classify(bug_couples, probabilities=True)
-                    similar_bugs = [
-                        similar_bugs[idx]
-                        for idx, prob in enumerate(probs)
-                        if prob[1] > self.confidence_threshold
-                    ]
 
                 # Recall
                 for idx, item in enumerate(duplicates[bug["id"]]):
@@ -280,7 +297,7 @@ def __init__(
             output_prefix="simdata.shdat", corpus=corpus_lsi, num_features=300
         )
 
-    def get_similar_bugs(self, query, k=10):
+    def search_similar_bugs(self, query, k=10):
         query_summary = "{} {}".format(query["summary"], query["comments"][0]["text"])
         query_summary = self.text_preprocess(query_summary)
 
@@ -329,7 +346,7 @@ def __init__(
         self.vectorizer.fit(text)
         self.similarity_calculator.fit(self.vectorizer.transform(text))
 
-    def get_similar_bugs(self, query):
+    def search_similar_bugs(self, query):
 
         processed_query = self.vectorizer.transform([self.get_text(query)])
         _, indices = self.similarity_calculator.kneighbors(processed_query)
@@ -387,7 +404,7 @@ def __init__(
 
     # word2vec.wmdistance calculates only the euclidean distance. To get the cosine distance,
     # we're using the function with a few subtle changes. We compute the cosine distances
-    # in the get_similar_bugs method and use this inside the wmdistance method.
+    # in the search_similar_bugs method and use this inside the wmdistance method.
     def wmdistance(self, document1, document2, all_distances, distance_metric="cosine"):
         model = self.w2vmodel
         if len(document1) == 0 or len(document2) == 0:
@@ -446,7 +463,7 @@ def calculate_all_distances(self, words):
             dtype=np.double,
         )
 
-    def get_similar_bugs(self, query):
+    def search_similar_bugs(self, query):
 
         words = self.text_preprocess(self.get_text(query))
         words = [word for word in words if word in self.w2vmodel.wv.vocab]
@@ -530,7 +547,7 @@ def __init__(
         self.dictionary = Dictionary(self.corpus)
         self.tfidf = TfidfModel(dictionary=self.dictionary)
 
-    def get_similar_bugs(self, query):
+    def search_similar_bugs(self, query):
 
         query = self.text_preprocess(self.get_text(query))
         words = [
@@ -646,7 +663,7 @@ def __init__(
             bow, similarity_matrix, num_best=10
         )
 
-    def get_similar_bugs(self, query):
+    def search_similar_bugs(self, query):
         similarities = self.softcosinesimilarity[
             self.dictionary.doc2bow(self.text_preprocess(self.get_text(query)))
         ]
@@ -683,7 +700,7 @@ def __init__(
 
         self.model = BM25(self.corpus)
 
-    def get_similar_bugs(self, query):
+    def search_similar_bugs(self, query):
         distances = self.model.get_scores(self.text_preprocess(self.get_text(query)))
         id_dist = zip(self.bug_ids, distances)
 
@@ -719,7 +736,7 @@ def __init__(
 
         self.model = LdaModel([self.dictionary.doc2bow(text) for text in self.corpus])
 
-    def get_similar_bugs(self, query):
+    def search_similar_bugs(self, query):
         query = self.text_preprocess(self.get_text(query))
 
         dense1 = sparse2full(
@@ -775,7 +792,7 @@ def index(self):
         self.elastic_search.indices.delete(index="bugbug", ignore=[400, 404])
         bulk(self.elastic_search, self.make_documents())
 
-    def get_similar_bugs(self, query):
+    def search_similar_bugs(self, query):
         find_similar = self.text_preprocess(
             self.get_text(query, all_comments=True), stemming=False, join=True
         )


---- SAMPLE #71 ----
Generated commit : Remove unused imports and fix pylint warnings
Ground-truth     : Remove unnecessary try/except around MiddlewareMixin

That was only needed if supporting multiple older versions of Django.
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0091 / 0.0833 / 0.0331 / 0.0748 / 0.1905
Diff:
diff --git a/kitsune/inproduct/middleware.py b/kitsune/inproduct/middleware.py
index 5da589f8291..d5f2b838689 100644
--- a/kitsune/inproduct/middleware.py
+++ b/kitsune/inproduct/middleware.py
@@ -1,7 +1,4 @@
-try:
-    from django.utils.deprecation import MiddlewareMixin
-except ImportError:
-    MiddlewareMixin = object
+from django.utils.deprecation import MiddlewareMixin
 
 
 class EuBuildMiddleware(MiddlewareMixin):
diff --git a/kitsune/sumo/anonymous.py b/kitsune/sumo/anonymous.py
index f7bd4fad1e9..534fbffc5e4 100644
--- a/kitsune/sumo/anonymous.py
+++ b/kitsune/sumo/anonymous.py
@@ -27,14 +27,10 @@
 import time
 
 from django.conf import settings
+from django.utils.deprecation import MiddlewareMixin
 from django.utils.http import cookie_date
 
 
-try:
-    from django.utils.deprecation import MiddlewareMixin
-except ImportError:
-    MiddlewareMixin = object
-
 # Use the system (hardware-based) random number generator if it exists.
 if hasattr(random, 'SystemRandom'):
     randrange = random.SystemRandom().randrange
diff --git a/kitsune/sumo/middleware.py b/kitsune/sumo/middleware.py
index 7f3f47ff71a..90c63470124 100644
--- a/kitsune/sumo/middleware.py
+++ b/kitsune/sumo/middleware.py
@@ -16,8 +16,8 @@
 from django.http.request import split_domain_port
 from django.shortcuts import render
 from django.utils import translation
-from django.utils.cache import (add_never_cache_headers,
-                                patch_response_headers, patch_vary_headers)
+from django.utils.cache import add_never_cache_headers, patch_response_headers, patch_vary_headers
+from django.utils.deprecation import MiddlewareMixin
 from django.utils.encoding import iri_to_uri, smart_bytes, smart_text
 import commonware.middleware
 import commonware.request.middleware
@@ -28,11 +28,6 @@
 from kitsune.sumo.urlresolvers import Prefixer, set_url_prefixer, split_path
 from kitsune.sumo.views import handle403
 
-try:
-    from django.utils.deprecation import MiddlewareMixin
-except ImportError:
-    MiddlewareMixin = object
-
 
 class EnforceHostIPMiddleware(EnforceHostMiddleware):
     """Modify the `EnforceHostMiddleware` to allow IP addresses"""
diff --git a/kitsune/twitter/middleware.py b/kitsune/twitter/middleware.py
index 88633e34090..c0e7e3d8e5a 100644
--- a/kitsune/twitter/middleware.py
+++ b/kitsune/twitter/middleware.py
@@ -3,6 +3,7 @@
 
 from django import http
 from django.conf import settings
+from django.utils.deprecation import MiddlewareMixin
 
 from kitsune.twitter import url, Session, REQUEST_KEY_NAME, REQUEST_SECRET_NAME
 from twython import TwythonError, TwythonAuthError
@@ -10,12 +11,6 @@
 from kitsune.twitter import get_twitter_api
 
 
-try:
-    from django.utils.deprecation import MiddlewareMixin
-except ImportError:
-    MiddlewareMixin = object
-
-
 log = logging.getLogger("k")
 
 
diff --git a/kitsune/users/middleware.py b/kitsune/users/middleware.py
index be5a6676659..55cb1372a80 100644
--- a/kitsune/users/middleware.py
+++ b/kitsune/users/middleware.py
@@ -1,15 +1,11 @@
 from django.contrib.auth import logout
 from django.http import HttpResponseRedirect
+from django.utils.deprecation import MiddlewareMixin
+from django.utils.translation import ugettext_lazy as _lazy
 
 from kitsune.sumo.urlresolvers import reverse
 
 
-try:
-    from django.utils.deprecation import MiddlewareMixin
-except ImportError:
-    MiddlewareMixin = object
-
-
 class LogoutDeactivatedUsersMiddleware(MiddlewareMixin):
     """Verifies that user.is_active == True.
 


---- SAMPLE #72 ----
Generated commit : Remove super method
Ground-truth     : Fix test for to match removed validator max_length decimal localization

This doesn't seem important enough to have to monkey patch the field.
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0741 / 0.0249 / 0.1129 / 0.1775
Diff:
diff --git a/kitsune/questions/tests/test_templates.py b/kitsune/questions/tests/test_templates.py
index 35e73a5b1e5..bca4b0a9d5e 100644
--- a/kitsune/questions/tests/test_templates.py
+++ b/kitsune/questions/tests/test_templates.py
@@ -121,8 +121,8 @@ def test_long_answer(self):
         doc = pq(response.content)
         error_msg = doc('ul.errorlist li a')[0]
         eq_(error_msg.text, 'Please keep the length of your content to ' +
-                            '10,000 characters or less. It is currently ' +
-                            '10,001 characters.')
+                            '10000 characters or less. It is currently ' +
+                            '10001 characters.')
 
     def test_solve_unsolve(self):
         """Test accepting a solution and undoing."""
diff --git a/kitsune/sumo/form_fields.py b/kitsune/sumo/form_fields.py
index e9ee88d4901..b7dea3509b9 100644
--- a/kitsune/sumo/form_fields.py
+++ b/kitsune/sumo/form_fields.py
@@ -1,14 +1,9 @@
 from django import forms
-from django.conf import settings
 from django.contrib.auth.models import User
 from django.core import validators
 from django.core.exceptions import ValidationError
-from django.utils import translation
 from django.utils.translation import ugettext as _
 
-from babel import Locale, localedata
-from babel.support import Format
-
 
 class TypedMultipleChoiceField(forms.MultipleChoiceField):
     """Coerce choices to a specific type and don't validate them.
@@ -78,32 +73,3 @@ def to_python(self, value):
                     raise forms.ValidationError(msg.format(username=username))
 
         return users
-
-
-class BaseValidator(validators.BaseValidator):
-    """Override the BaseValidator from django to format numbers."""
-    def __call__(self, value):
-        cleaned = self.clean(value)
-        params = {'limit_value': _format_decimal(self.limit_value),
-                  'show_value': _format_decimal(cleaned)}
-        if self.compare(cleaned, self.limit_value):
-            raise ValidationError(
-                self.message % params,
-                code=self.code,
-                params=params,
-            )
-
-
-def _format_decimal(num, format=None):
-    """Returns the string of a number formatted for the current language.
-
-    Uses django's translation.get_language() to find the current language from
-    the request.
-    Falls back to the default language if babel does not support the current.
-
-    """
-    lang = translation.get_language()
-    if not localedata.exists(lang):
-        lang = settings.LANGUAGE_CODE
-    locale = Locale(translation.to_locale(lang))
-    return Format(locale).decimal(num, format)
diff --git a/kitsune/sumo/tests/test_form_fields.py b/kitsune/sumo/tests/test_form_fields.py
index d3a7466f7e7..86ece2e0476 100644
--- a/kitsune/sumo/tests/test_form_fields.py
+++ b/kitsune/sumo/tests/test_form_fields.py
@@ -1,41 +1,11 @@
 from django.core.exceptions import ValidationError
-from django.utils import translation
 
 from nose.tools import eq_
 
-from kitsune.sumo.form_fields import _format_decimal, TypedMultipleChoiceField
+from kitsune.sumo.form_fields import TypedMultipleChoiceField
 from kitsune.sumo.tests import TestCase
 
 
-class TestFormatDecimal(TestCase):
-
-    def test_default_locale(self):
-        """Default locale just works"""
-        num = _format_decimal(1234.567)
-        eq_('1,234.567', num)
-
-    def test_fr_locale(self):
-        """French locale returns french format"""
-        translation.activate('fr')
-        num = _format_decimal(1234.567)
-        eq_(u'1\xa0234,567', num)
-
-    def test_xx_YY_locale(self):
-        """Falls back to English-like formatting for unknown locales"""
-        translation.activate('xx-YY')
-        num = _format_decimal(1234.567)
-        eq_('1,234.567', num)
-
-    def test_fy_NL_locale(self):
-        """Falls back to English for unknown babel locales"""
-        # Note: if this starts to fail for no apparent reason, it's probably
-        # because babel learned about fy-NL since this test was written.
-        translation.activate('fy-NL')
-        eq_('fy-nl', translation.get_language())
-        num = _format_decimal(1234.567)
-        eq_('1,234.567', num)
-
-
 class TypedMultipleChoiceFieldTestCase(TestCase):
     """TypedMultipleChoiceField is just like MultipleChoiceField
     except, instead of validating, it coerces types."""


---- SAMPLE #73 ----
Generated commit : Remove the way of using `Client` as a parameter
Ground-truth     : Make AWS SecretsManager tests compatible with `moto>=4.2.1` (#34046)
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / 0.0991 / 0.1788
Diff:
diff --git a/tests/providers/amazon/aws/hooks/test_secrets_manager.py b/tests/providers/amazon/aws/hooks/test_secrets_manager.py
index 788bbb5e043a4..c5af2001ecb77 100644
--- a/tests/providers/amazon/aws/hooks/test_secrets_manager.py
+++ b/tests/providers/amazon/aws/hooks/test_secrets_manager.py
@@ -33,20 +33,15 @@ def test_get_conn_returns_a_boto3_connection(self):
 
     def test_get_secret_string(self):
         secret_name = "arn:aws:secretsmanager:us-east-2:999999999999:secret:db_cluster-YYYYYYY"
-        secret_value = '{"user": "test"}'
+        secret_value = "test"
         hook = SecretsManagerHook(aws_conn_id="aws_default")
 
         create_param = {
             "Name": secret_name,
-        }
-
-        put_param = {
-            "SecretId": secret_name,
             "SecretString": secret_value,
         }
 
         hook.get_conn().create_secret(**create_param)
-        hook.get_conn().put_secret_value(**put_param)
 
         secret = hook.get_secret(secret_name)
         assert secret == secret_value
@@ -58,15 +53,10 @@ def test_get_secret_dict(self):
 
         create_param = {
             "Name": secret_name,
-        }
-
-        put_param = {
-            "SecretId": secret_name,
             "SecretString": secret_value,
         }
 
         hook.get_conn().create_secret(**create_param)
-        hook.get_conn().put_secret_value(**put_param)
 
         secret = hook.get_secret_as_dict(secret_name)
         assert secret == json.loads(secret_value)
@@ -75,17 +65,9 @@ def test_get_secret_binary(self):
         secret_name = "arn:aws:secretsmanager:us-east-2:999999999999:secret:db_cluster-YYYYYYY"
         secret_value_binary = base64.b64encode(b'{"username": "test"}')
         hook = SecretsManagerHook(aws_conn_id="aws_default")
-        create_param = {
-            "Name": secret_name,
-        }
-
-        put_param = {
-            "SecretId": secret_name,
-            "SecretBinary": secret_value_binary,
-        }
+        create_param = {"Name": secret_name, "SecretBinary": secret_value_binary}
 
         hook.get_conn().create_secret(**create_param)
-        hook.get_conn().put_secret_value(**put_param)
 
         secret = hook.get_secret(secret_name)
         assert secret == base64.b64decode(secret_value_binary)
diff --git a/tests/providers/amazon/aws/secrets/test_secrets_manager.py b/tests/providers/amazon/aws/secrets/test_secrets_manager.py
index fa70a12fcbf41..cb5dbd0210992 100644
--- a/tests/providers/amazon/aws/secrets/test_secrets_manager.py
+++ b/tests/providers/amazon/aws/secrets/test_secrets_manager.py
@@ -37,16 +37,11 @@ def test_get_conn_value_full_url_mode(self):
         secret_id = "airflow/connections/test_postgres"
         create_param = {
             "Name": secret_id,
-        }
-
-        param = {
-            "SecretId": secret_id,
             "SecretString": "postgresql://airflow:airflow@host:5432/airflow",
         }
 
         secrets_manager_backend = SecretsManagerBackend()
         secrets_manager_backend.client.create_secret(**create_param)
-        secrets_manager_backend.client.put_secret_value(**param)
 
         returned_uri = secrets_manager_backend.get_conn_value(conn_id="test_postgres")
         assert "postgresql://airflow:airflow@host:5432/airflow" == returned_uri
@@ -63,10 +58,6 @@ def test_get_connection_broken_field_mode_url_encoding(self, are_secret_values_u
         secret_id = "airflow/connections/test_postgres"
         create_param = {
             "Name": secret_id,
-        }
-
-        param = {
-            "SecretId": secret_id,
             "SecretString": json.dumps(
                 {
                     "conn_type": "postgresql",
@@ -82,7 +73,6 @@ def test_get_connection_broken_field_mode_url_encoding(self, are_secret_values_u
             are_secret_values_urlencoded=are_secret_values_urlencoded
         )
         secrets_manager_backend.client.create_secret(**create_param)
-        secrets_manager_backend.client.put_secret_value(**param)
 
         conn = secrets_manager_backend.get_connection(conn_id="test_postgres")
 
@@ -97,10 +87,6 @@ def test_get_connection_broken_field_mode_extra_allows_nested_json(self):
         secret_id = "airflow/connections/test_postgres"
         create_param = {
             "Name": secret_id,
-        }
-
-        param = {
-            "SecretId": secret_id,
             "SecretString": json.dumps(
                 {
                     "conn_type": "postgresql",
@@ -114,7 +100,6 @@ def test_get_connection_broken_field_mode_extra_allows_nested_json(self):
 
         secrets_manager_backend = SecretsManagerBackend(full_url_mode=False)
         secrets_manager_backend.client.create_secret(**create_param)
-        secrets_manager_backend.client.put_secret_value(**param)
 
         conn = secrets_manager_backend.get_connection(conn_id="test_postgres")
         assert conn.extra_dejson["foo"] == "bar"
@@ -124,17 +109,14 @@ def test_get_conn_value_broken_field_mode(self):
         secret_id = "airflow/connections/test_postgres"
         create_param = {
             "Name": secret_id,
-        }
-
-        param = {
-            "SecretId": secret_id,
-            "SecretString": '{"user": "airflow", "pass": "airflow", "host": "host", '
-            '"port": 5432, "schema": "airflow", "engine": "postgresql"}',
+            "SecretString": (
+                '{"user": "airflow", "pass": "airflow", "host": "host", '
+                '"port": 5432, "schema": "airflow", "engine": "postgresql"}'
+            ),
         }
 
         secrets_manager_backend = SecretsManagerBackend(full_url_mode=False)
         secrets_manager_backend.client.create_secret(**create_param)
-        secrets_manager_backend.client.put_secret_value(**param)
 
         conn = secrets_manager_backend.get_connection(conn_id="test_postgres")
         returned_uri = conn.get_uri()
@@ -145,19 +127,16 @@ def test_get_conn_value_broken_field_mode_extra_words_added(self):
         secret_id = "airflow/connections/test_postgres"
         create_param = {
             "Name": secret_id,
-        }
-
-        param = {
-            "SecretId": secret_id,
-            "SecretString": '{"usuario": "airflow", "pass": "airflow", "host": "host", '
-            '"port": 5432, "schema": "airflow", "engine": "postgresql"}',
+            "SecretString": (
+                '{"usuario": "airflow", "pass": "airflow", "host": "host", '
+                '"port": 5432, "schema": "airflow", "engine": "postgresql"}'
+            ),
         }
 
         secrets_manager_backend = SecretsManagerBackend(
             full_url_mode=False, extra_conn_words={"user": ["usuario"]}
         )
         secrets_manager_backend.client.create_secret(**create_param)
-        secrets_manager_backend.client.put_secret_value(**param)
 
         conn = secrets_manager_backend.get_connection(conn_id="test_postgres")
         returned_uri = conn.get_uri()
@@ -174,16 +153,11 @@ def test_get_conn_value_non_existent_key(self):
         secret_id = "airflow/connections/test_postgres"
         create_param = {
             "Name": secret_id,
-        }
-
-        param = {
-            "SecretId": secret_id,
             "SecretString": "postgresql://airflow:airflow@host:5432/airflow",
         }
 
         secrets_manager_backend = SecretsManagerBackend()
         secrets_manager_backend.client.create_secret(**create_param)
-        secrets_manager_backend.client.put_secret_value(**param)
 
         assert secrets_manager_backend.get_conn_value(conn_id=conn_id) is None
         assert secrets_manager_backend.get_connection(conn_id=conn_id) is None
@@ -192,15 +166,10 @@ def test_get_conn_value_non_existent_key(self):
     def test_get_variable(self):
 
         secret_id = "airflow/variables/hello"
-        create_param = {
-            "Name": secret_id,
-        }
-
-        param = {"SecretId": secret_id, "SecretString": "world"}
+        create_param = {"Name": secret_id, "SecretString": "world"}
 
         secrets_manager_backend = SecretsManagerBackend()
         secrets_manager_backend.client.create_secret(**create_param)
-        secrets_manager_backend.client.put_secret_value(**param)
 
         returned_uri = secrets_manager_backend.get_variable("hello")
         assert "world" == returned_uri
@@ -212,14 +181,10 @@ def test_get_variable_non_existent_key(self):
         SystemsManagerParameterStoreBackend.get_variables should return None
         """
         secret_id = "airflow/variables/hello"
-        create_param = {
-            "Name": secret_id,
-        }
-        param = {"SecretId": secret_id, "SecretString": "world"}
+        create_param = {"Name": secret_id, "SecretString": "world"}
 
         secrets_manager_backend = SecretsManagerBackend()
         secrets_manager_backend.client.create_secret(**create_param)
-        secrets_manager_backend.client.put_secret_value(**param)
 
         assert secrets_manager_backend.get_variable("test_mysql") is None
 
@@ -230,14 +195,10 @@ def test_get_config_non_existent_key(self):
         SystemsManagerParameterStoreBackend.get_config should return None
         """
         secret_id = "airflow/config/hello"
-        create_param = {
-            "Name": secret_id,
-        }
-        param = {"SecretId": secret_id, "SecretString": "world"}
+        create_param = {"Name": secret_id, "SecretString": "world"}
 
         secrets_manager_backend = SecretsManagerBackend()
         secrets_manager_backend.client.create_secret(**create_param)
-        secrets_manager_backend.client.put_secret_value(**param)
 
         assert secrets_manager_backend.get_config("test") is None
 


---- SAMPLE #74 ----
Generated commit : Improve error handling of errors when called
Ground-truth     : Skip heavy rate limit unittests if SKIP_RLIMITS is set
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / -0.0373 / 0.1784
Diff:
diff --git a/celery/tests/test_buckets.py b/celery/tests/test_buckets.py
index 4eaf8051013..6764884674a 100644
--- a/celery/tests/test_buckets.py
+++ b/celery/tests/test_buckets.py
@@ -5,10 +5,15 @@
 import unittest
 from itertools import chain, izip
 
-from celery.worker import buckets
+from billiard.utils.functional import curry
+
+from celery.task.base import Task
 from celery.utils import gen_unique_id
+from celery.tests.utils import skip_if_environ
+from celery.worker import buckets
 from celery.registry import TaskRegistry
-from celery.task.base import Task
+
+skip_if_disabled = curry(skip_if_environ("SKIP_RLIMITS"))
 
 
 class MockJob(object):
@@ -35,10 +40,12 @@ def __repr__(self):
 
 class TestTokenBucketQueue(unittest.TestCase):
 
+    @skip_if_disabled
     def empty_queue_yields_QueueEmpty(self):
         x = buckets.TokenBucketQueue(fill_rate=10)
         self.assertRaises(buckets.QueueEmpty, x.get)
 
+    @skip_if_disabled
     def test_bucket__put_get(self):
         x = buckets.TokenBucketQueue(fill_rate=10)
         x.put("The quick brown fox")
@@ -48,6 +55,7 @@ def test_bucket__put_get(self):
         time.sleep(0.2)
         self.assertEquals(x.get_nowait(), "The lazy dog")
 
+    @skip_if_disabled
     def test_fill_rate(self):
         x = buckets.TokenBucketQueue(fill_rate=10)
         # 20 items should take at least one second to complete
@@ -58,6 +66,7 @@ def test_fill_rate(self):
             x.wait()
         self.assertTrue(time.time() - time_start > 1.5)
 
+    @skip_if_disabled
     def test_can_consume(self):
         x = buckets.TokenBucketQueue(fill_rate=1)
         x.put("The quick brown fox")
@@ -67,12 +76,14 @@ def test_can_consume(self):
         x.put("The lazy dog")
         self.assertRaises(x.RateLimitExceeded, x.get)
 
+    @skip_if_disabled
     def test_expected_time(self):
         x = buckets.TokenBucketQueue(fill_rate=1)
         x.put_nowait("The quick brown fox")
         self.assertEqual(x.get_nowait(), "The quick brown fox")
         self.assertTrue(x.expected_time())
 
+    @skip_if_disabled
     def test_qsize(self):
         x = buckets.TokenBucketQueue(fill_rate=1)
         x.put("The quick brown fox")
@@ -82,6 +93,7 @@ def test_qsize(self):
 
 class TestRateLimitString(unittest.TestCase):
 
+    @skip_if_disabled
     def test_conversion(self):
         self.assertEquals(buckets.parse_ratelimit_string(999), 999)
         self.assertEquals(buckets.parse_ratelimit_string("1456/s"), 1456)
@@ -123,6 +135,7 @@ def setUp(self):
         for task_cls in self.task_classes:
             self.registry.register(task_cls)
 
+    @skip_if_disabled
     def test_auto_add_on_missing(self):
         b = buckets.TaskBucket(task_registry=self.registry)
         for task_cls in self.task_classes:
@@ -132,6 +145,7 @@ def test_auto_add_on_missing(self):
         self.assertTrue(TaskD.name in b.buckets.keys())
         self.registry.unregister(TaskD)
 
+    @skip_if_disabled
     def test_has_rate_limits(self):
         b = buckets.TaskBucket(task_registry=self.registry)
         self.assertEqual(b.buckets[TaskA.name].fill_rate, 10)
@@ -144,17 +158,20 @@ def test_has_rate_limits(self):
         finally:
             self.registry.unregister(TaskD)
 
+    @skip_if_disabled
     def test_on_empty_buckets__get_raises_empty(self):
         b = buckets.TaskBucket(task_registry=self.registry)
         self.assertRaises(buckets.QueueEmpty, b.get)
         self.assertEqual(b.qsize(), 0)
 
+    @skip_if_disabled
     def test_put__get(self):
         b = buckets.TaskBucket(task_registry=self.registry)
         job = MockJob(gen_unique_id(), TaskA.name, ["theqbf"], {"foo": "bar"})
         b.put(job)
         self.assertEquals(b.get(), job)
 
+    @skip_if_disabled
     def test_fill_rate(self):
         b = buckets.TaskBucket(task_registry=self.registry)
 
@@ -171,6 +188,7 @@ def test_fill_rate(self):
             self.assertEqual(b.get(), job)
         self.assertTrue(time.time() - time_start > 1.5)
 
+    @skip_if_disabled
     def test__very_busy_queue_doesnt_block_others(self):
         b = buckets.TaskBucket(task_registry=self.registry)
 
@@ -187,6 +205,7 @@ def test__very_busy_queue_doesnt_block_others(self):
 
         self.assertTrue(got_ajobs > 2)
 
+    @skip_if_disabled
     def test_thorough__multiple_types(self):
         self.registry.register(TaskD)
         try:
diff --git a/celery/tests/utils.py b/celery/tests/utils.py
index 8154c7a01a0..837088f5591 100644
--- a/celery/tests/utils.py
+++ b/celery/tests/utils.py
@@ -1,19 +1,37 @@
 from __future__ import with_statement
 
 import sys
+import os
 import __builtin__
 from StringIO import StringIO
 from functools import wraps
 from contextlib import contextmanager
 
 
+def skip_if_environ(env_var_name):
+
+    def _wrap_test(fun):
+
+        @wraps(fun)
+        def _skips_if_environ(*args, **kwargs):
+            if os.environ.get(env_var_name):
+                sys.stderr.write("SKIP %s: %s set\n" % (
+                    fun.__name__, env_var_name))
+                return
+            return fun(*args, **kwargs)
+
+        return _skips_if_environ
+
+    return _wrap_test
+
+
 def _skip_test(reason, sign):
 
     def _wrap_test(fun):
 
         @wraps(fun)
         def _skipped_test(*args, **kwargs):
-            sys.stderr.write("(%s: %s) " % (sign, reason))
+            sys.stderr.write("%s: %s " % (sign, reason))
 
         return _skipped_test
     return _wrap_test


---- SAMPLE #75 ----
Generated commit : refactor: remove deps of completed jobs (#4321)
Ground-truth     : Bug 1513486 - Remove RunnableJobsClient

Since it's never called with the `task_id` parameter, and when the task ID
is not set, the return value is identical to calling `list_runnable_jobs`
directly.
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.1000 / 0.0361 / 0.0347 / 0.1904
Diff:
diff --git a/tests/seta/test_update_job_priority.py b/tests/seta/test_update_job_priority.py
index 8b8b08f1426..a6a9eacb160 100644
--- a/tests/seta/test_update_job_priority.py
+++ b/tests/seta/test_update_job_priority.py
@@ -4,7 +4,6 @@
 from mock import patch
 
 from treeherder.seta.models import JobPriority
-from treeherder.seta.runnable_jobs import RunnableJobsClient
 from treeherder.seta.update_job_priority import (_initialize_values,
                                                  _sanitize_data,
                                                  _unique_key,
@@ -37,9 +36,9 @@ def test_sanitize_data(runnable_jobs_data):
     assert tc_jobs == 2
 
 
-@patch.object(RunnableJobsClient, 'query_runnable_jobs')
-def test_query_sanitized_data(query_runnable_jobs, runnable_jobs_data, sanitized_data):
-    query_runnable_jobs.return_value = runnable_jobs_data
+@patch('treeherder.seta.update_job_priority.list_runnable_jobs')
+def test_query_sanitized_data(list_runnable_jobs, runnable_jobs_data, sanitized_data):
+    list_runnable_jobs.return_value = runnable_jobs_data
     data = query_sanitized_data()
     assert data == sanitized_data
 
diff --git a/treeherder/seta/runnable_jobs.py b/treeherder/seta/runnable_jobs.py
deleted file mode 100644
index 32f597fa3ba..00000000000
--- a/treeherder/seta/runnable_jobs.py
+++ /dev/null
@@ -1,40 +0,0 @@
-# TODO: Get rid of this module (bug 1330649)
-import logging
-
-from treeherder.etl.runnable_jobs import (list_runnable_jobs,
-                                          query_latest_gecko_decision_task_id)
-
-logger = logging.getLogger(__name__)
-
-
-class RunnableJobsClient(object):
-    def __init__(self):
-        self.cache = {}
-
-    def query_runnable_jobs(self, repo_name, task_id=None):
-        '''Return runnable_jobs data
-
-        Calling this repo without task_id is optimal as we then only cache the latest runnable jobs.
-
-        We do caching because the data does not really change and because bug 1288028 makes calls to
-        the runnable_jobs API very slow.
-        '''
-        if repo_name not in self.cache:
-            self.cache[repo_name] = {}
-
-        if not task_id:
-            task_id = query_latest_gecko_decision_task_id(repo_name)
-            self.cache[repo_name]['latest'] = self._query_runnable_jobs(repo_name=repo_name, task_id=task_id)
-            return self.cache[repo_name]['latest']
-        else:
-            if task_id in self.cache:
-                # XXX: In previous code, we were returning None; what should we do for this case?
-                logger.info("We have already processed the data from this task (%s).", task_id)
-                return self.cache[repo_name][task_id]
-            else:
-                logger.info("We're going to fetch new runnable jobs data.")
-                self.cache[repo_name][task_id] = self._query_runnable_jobs(repo_name=repo_name, task_id=task_id)
-                return self.cache[repo_name][task_id]
-
-    def _query_runnable_jobs(self, repo_name, task_id):
-        return list_runnable_jobs(repo_name, task_id)
diff --git a/treeherder/seta/update_job_priority.py b/treeherder/seta/update_job_priority.py
index 6ff69be85a0..aee605f1c74 100644
--- a/treeherder/seta/update_job_priority.py
+++ b/treeherder/seta/update_job_priority.py
@@ -12,12 +12,12 @@
 import datetime
 import logging
 
+from treeherder.etl.runnable_jobs import list_runnable_jobs
 from treeherder.etl.seta import (parse_testtype,
                                  valid_platform)
 from treeherder.seta.common import (job_priority_index,
                                     unique_key)
 from treeherder.seta.models import JobPriority
-from treeherder.seta.runnable_jobs import RunnableJobsClient
 from treeherder.seta.settings import (SETA_HIGH_VALUE_PRIORITY,
                                       SETA_LOW_VALUE_PRIORITY)
 
@@ -102,10 +102,7 @@ def _sanitize_data(runnable_jobs_data):
 def query_sanitized_data(repo_name='mozilla-inbound'):
     """Return sanitized jobs data based on runnable api. None if failed to obtain or no new data.
 
-     We need to find the latest gecko decision task ID (by querying the index [1][2])
-     in order to know which task ID to pass to the runnable api [3].
-
-     It stores the minimal sanitized data from runnable apis under ~/.mozilla/seta/<task_id>.json
+     We need to find the latest gecko decision task ID (by querying the index [1][2]).
 
      [1] https://index.taskcluster.net/v1/task/gecko.v2.%s.latest.taskgraph.decision/
      [2] Index's data structure:
@@ -116,9 +113,8 @@ def query_sanitized_data(repo_name='mozilla-inbound'):
         "data": {},
         "expires": "2017-10-06T18:30:18.428Z"
       }
-     [3] https://treeherder.mozilla.org/api/project/mozilla-inbound/runnable_jobs/?decision_task_id=Pp7ZxoH0SKyU6wnhX_Fp0g&format=json
     """
-    runnable_jobs = RunnableJobsClient().query_runnable_jobs(repo_name)
+    runnable_jobs = list_runnable_jobs(repo_name)
     return _sanitize_data(runnable_jobs)
 
 


---- SAMPLE #76 ----
Generated commit : A few more fixes for bug in #731
Ground-truth     : Tasks can now raise celery.exceptions.Ignore to not update any state
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / -0.0159 / 0.1784
Diff:
diff --git a/celery/exceptions.py b/celery/exceptions.py
index 87c1afcd8dd..f4ce8449a44 100644
--- a/celery/exceptions.py
+++ b/celery/exceptions.py
@@ -25,6 +25,10 @@ class SecurityError(Exception):
     """
 
 
+class Ignore(Exception):
+    """A task can raise this to ignore doing state updates."""
+
+
 class SystemTerminate(SystemExit):
     """Signals that the worker should terminate."""
 
diff --git a/celery/states.py b/celery/states.py
index a1d787f6e8c..4563fb72f7c 100644
--- a/celery/states.py
+++ b/celery/states.py
@@ -126,6 +126,7 @@ def __le__(self, other):
 FAILURE = 'FAILURE'
 REVOKED = 'REVOKED'
 RETRY = 'RETRY'
+IGNORED = 'IGNORED'
 
 READY_STATES = frozenset([SUCCESS, FAILURE, REVOKED])
 UNREADY_STATES = frozenset([PENDING, RECEIVED, STARTED, RETRY])
diff --git a/celery/task/trace.py b/celery/task/trace.py
index 85977abb65f..7263e7c568a 100644
--- a/celery/task/trace.py
+++ b/celery/task/trace.py
@@ -29,7 +29,7 @@
 from celery.app import set_default_app
 from celery.app.task import Task as BaseTask, Context
 from celery.datastructures import ExceptionInfo
-from celery.exceptions import RetryTaskError
+from celery.exceptions import Ignore, RetryTaskError
 from celery.utils.serialization import get_pickleable_exception
 from celery.utils.log import get_logger
 
@@ -43,6 +43,7 @@
 success_receivers = signals.task_success.receivers
 STARTED = states.STARTED
 SUCCESS = states.SUCCESS
+IGNORED = states.IGNORED
 RETRY = states.RETRY
 FAILURE = states.FAILURE
 EXCEPTION_STATES = states.EXCEPTION_STATES
@@ -222,6 +223,8 @@ def trace_task(uuid, args, kwargs, request=None):
                 try:
                     R = retval = fun(*args, **kwargs)
                     state = SUCCESS
+                except Ignore, exc:
+                    I, R = Info(IGNORED, exc), ExceptionInfo(internal=True)
                 except RetryTaskError, exc:
                     I = Info(RETRY, exc)
                     state, retval = I.state, I.retval
diff --git a/celery/worker/job.py b/celery/worker/job.py
index 64e6c70c378..9739369cdd4 100644
--- a/celery/worker/job.py
+++ b/celery/worker/job.py
@@ -23,7 +23,7 @@
 from celery import signals
 from celery.app import app_or_default
 from celery.datastructures import ExceptionInfo
-from celery.exceptions import TaskRevokedError
+from celery.exceptions import Ignore, TaskRevokedError
 from celery.platforms import signals as _signals
 from celery.task.trace import (
     trace_task,
@@ -64,8 +64,9 @@ class Request(object):
                  'eventer', 'connection_errors',
                  'task', 'eta', 'expires',
                  'request_dict', 'acknowledged', 'success_msg',
-                 'error_msg', 'retry_msg', 'time_start', 'worker_pid',
-                 '_already_revoked', '_terminate_on_ack', '_tzlocal')
+                 'error_msg', 'retry_msg', 'ignore_msg',
+                 'time_start', 'worker_pid', '_already_revoked',
+                 '_terminate_on_ack', '_tzlocal')
 
     #: Format string used to log task success.
     success_msg = """\
@@ -82,6 +83,10 @@ class Request(object):
         Task %(name)s[%(id)s] INTERNAL ERROR: %(exc)s
     """
 
+    ignored_msg = """\
+        Task %(name)s[%(id)s] ignored
+    """
+
     #: Format string used to log task retry.
     retry_msg = """Task %(name)s[%(id)s] retry: %(exc)s"""
 
@@ -380,9 +385,15 @@ def _log_error(self, einfo):
                          traceback=traceback)
 
         if internal:
-            format = self.internal_error_msg
-            description = 'INTERNAL ERROR'
-            severity = logging.CRITICAL
+            if isinstance(einfo.exception, Ignore):
+                format = self.ignored_msg
+                description = 'ignored'
+                severity = logging.INFO
+                exc_info = None
+            else:
+                format = self.internal_error_msg
+                description = 'INTERNAL ERROR'
+                severity = logging.CRITICAL
 
         context = {
             'hostname': self.hostname,


---- SAMPLE #77 ----
Generated commit : Add missing comma to the error message for invalid user
Ground-truth     : Ignore current app's app_domain  when uploading new versions (bug 892694)
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / 0.0235 / 0.1696
Diff:
diff --git a/mkt/submit/forms.py b/mkt/submit/forms.py
index 0000c338e6a..94ce2bcfc30 100644
--- a/mkt/submit/forms.py
+++ b/mkt/submit/forms.py
@@ -186,7 +186,8 @@ def clean(self):
             origin = pkg.get('origin')
             if origin:
                 try:
-                    origin = verify_app_domain(origin, packaged=True)
+                    origin = verify_app_domain(origin, packaged=True,
+                                               exclude=self.addon)
                 except forms.ValidationError, e:
                     self._errors['upload'] = self.error_class(e.messages)
                     return
diff --git a/mkt/submit/tests/test_forms.py b/mkt/submit/tests/test_forms.py
index e8cb8c0631b..b4018baf0e3 100644
--- a/mkt/submit/tests/test_forms.py
+++ b/mkt/submit/tests/test_forms.py
@@ -146,6 +146,26 @@ def test_verify_app_domain_called(self, _verify):
         assert form.is_valid(), form.errors
         assert _verify.called
 
+    @mock.patch('mkt.submit.forms.parse_addon',
+                lambda *args: {"origin": "app://hy.fr"})
+    def test_verify_app_domain_exclude_same(self):
+        app = amo.tests.app_factory(app_domain='app://hy.fr')
+        form = forms.NewWebappVersionForm(
+            {'upload': self.file.uuid}, request=self.request, is_packaged=True,
+            addon=app)
+        assert form.is_valid(), form.errors
+
+    @mock.patch('mkt.submit.forms.parse_addon',
+                lambda *args: {"origin": "app://hy.fr"})
+    def test_verify_app_domain_exclude_different(self):
+        app = amo.tests.app_factory(app_domain='app://yo.lo')
+        amo.tests.app_factory(app_domain='app://hy.fr')
+        form = forms.NewWebappVersionForm(
+            {'upload': self.file.uuid}, request=self.request, is_packaged=True,
+            addon=app)
+        assert not form.is_valid(), form.errors
+        assert 'An app already exists' in ''.join(form.errors['upload'])
+
 
 class TestAppDetailsBasicForm(amo.tests.TestCase):
     fixtures = fixture('user_999', 'webapp_337141')


---- SAMPLE #78 ----
Generated commit : Fix error message for invalid s3 URL (#4332)
Ground-truth     : Handle `UnboundLocalError` while parsing invalid `s3_url` (#32120)
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0278 / 0.3750 / 0.0704 / 0.4254 / 0.2205
Diff:
diff --git a/airflow/providers/amazon/aws/hooks/s3.py b/airflow/providers/amazon/aws/hooks/s3.py
index 0489fa63128ad..044dc2c9a9eaa 100644
--- a/airflow/providers/amazon/aws/hooks/s3.py
+++ b/airflow/providers/amazon/aws/hooks/s3.py
@@ -212,11 +212,16 @@ def parse_s3_url(s3url: str) -> tuple[str, str]:
         :param s3url: The S3 Url to parse.
         :return: the parsed bucket name and key
         """
+        valid_s3_format = "S3://bucket-name/key-name"
+        valid_s3_virtual_hosted_format = "https://bucket-name.s3.region-code.amazonaws.com/key-name"
         format = s3url.split("//")
         if re.match(r"s3[na]?:", format[0], re.IGNORECASE):
             parsed_url = urlsplit(s3url)
             if not parsed_url.netloc:
-                raise S3HookUriParseFailure(f'Please provide a bucket name using a valid format: "{s3url}"')
+                raise S3HookUriParseFailure(
+                    "Please provide a bucket name using a valid format of the form: "
+                    + f'{valid_s3_format} or {valid_s3_virtual_hosted_format} but provided: "{s3url}"'
+                )
 
             bucket_name = parsed_url.netloc
             key = parsed_url.path.lstrip("/")
@@ -229,8 +234,16 @@ def parse_s3_url(s3url: str) -> tuple[str, str]:
             elif temp_split[1] == "s3":
                 bucket_name = temp_split[0]
                 key = "/".join(format[1].split("/")[1:])
+            else:
+                raise S3HookUriParseFailure(
+                    "Please provide a bucket name using a valid virtually hosted format which should"
+                    + f' be of the form: {valid_s3_virtual_hosted_format} but provided: "{s3url}"'
+                )
         else:
-            raise S3HookUriParseFailure(f'Please provide a bucket name using a valid format: "{s3url}"')
+            raise S3HookUriParseFailure(
+                "Please provide a bucket name using a valid format of the form: "
+                + f'{valid_s3_format} or {valid_s3_virtual_hosted_format} but provided: "{s3url}"'
+            )
         return bucket_name, key
 
     @staticmethod
diff --git a/tests/providers/amazon/aws/hooks/test_s3.py b/tests/providers/amazon/aws/hooks/test_s3.py
index 001584ccbacc3..4a42d828da8ba 100644
--- a/tests/providers/amazon/aws/hooks/test_s3.py
+++ b/tests/providers/amazon/aws/hooks/test_s3.py
@@ -34,6 +34,7 @@
 
 from airflow.exceptions import AirflowException
 from airflow.models import Connection
+from airflow.providers.amazon.aws.exceptions import S3HookUriParseFailure
 from airflow.providers.amazon.aws.hooks.s3 import (
     S3Hook,
     provide_bucket_name,
@@ -94,6 +95,15 @@ def test_parse_s3_url_virtual_hosted_style(self):
         parsed = S3Hook.parse_s3_url("https://DOC-EXAMPLE-BUCKET1.s3.us-west-2.amazonaws.com/test.png")
         assert parsed == ("DOC-EXAMPLE-BUCKET1", "test.png"), "Incorrect parsing of the s3 url"
 
+    def test_parse_invalid_s3_url_virtual_hosted_style(self):
+        with pytest.raises(
+            S3HookUriParseFailure,
+            match="Please provide a bucket name using a valid virtually hosted format which should"
+            + " be of the form: https://bucket-name.s3.region-code.amazonaws.com/key-name but "
+            + 'provided: "https://DOC-EXAMPLE-BUCKET1.us-west-2.amazonaws.com/test.png"',
+        ):
+            S3Hook.parse_s3_url("https://DOC-EXAMPLE-BUCKET1.us-west-2.amazonaws.com/test.png")
+
     def test_parse_s3_object_directory(self):
         parsed = S3Hook.parse_s3_url("s3://test/this/is/not/a-real-s3-directory/")
         assert parsed == ("test", "this/is/not/a-real-s3-directory/"), "Incorrect parsing of the s3 url"


---- SAMPLE #79 ----
Generated commit : Fix app update
Ground-truth     : Hide non-public apps inside collections (bug 941337)
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.1818 / 0.0758 / 0.1821 / 0.1882
Diff:
diff --git a/mkt/collections/models.py b/mkt/collections/models.py
index ad6efa8d010..271d3042d26 100644
--- a/mkt/collections/models.py
+++ b/mkt/collections/models.py
@@ -5,7 +5,7 @@
 
 import amo.models
 import mkt.regions
-from addons.models import Category, clean_slug
+from addons.models import Addon, Category, clean_slug
 from amo.decorators import use_master
 from amo.utils import to_language
 from mkt.webapps.models import Webapp
@@ -76,7 +76,15 @@ def image_path(self):
                             'app_collection_%s.png' % (self.pk,))
 
     def apps(self):
-        return self._apps.order_by('collectionmembership')
+        """
+        Public apps on the collection, ordered by their position in the
+        CollectionMembership model.
+
+        Use this method everytime you want to display apps for a collection to
+        an user.
+        """
+        return self._apps.filter(disabled_by_user=False,
+            status=amo.STATUS_PUBLIC).order_by('collectionmembership')
 
     def add_app(self, app, order=None):
         """
@@ -159,5 +167,16 @@ class Meta:
         ordering = ('order',)
 
 
+def remove_deleted_apps(*args, **kwargs):
+    instance = kwargs.get('instance')
+    CollectionMembership.objects.filter(app_id=instance.pk).delete()
+
+
+# Save translations when saving a Collection.
 models.signals.pre_save.connect(save_signal, sender=Collection,
                                 dispatch_uid='collection_translations')
+
+# Delete collection membership when deleting an app (sender needs to be Addon,
+# not Webapp, because that's the real model underneath).
+models.signals.post_delete.connect(remove_deleted_apps, sender=Addon,
+                                   dispatch_uid='apps_collections_cleanup')
diff --git a/mkt/collections/serializers.py b/mkt/collections/serializers.py
index 30c056f1ee3..72b7452a8a4 100644
--- a/mkt/collections/serializers.py
+++ b/mkt/collections/serializers.py
@@ -34,14 +34,15 @@
 
 class CollectionMembershipField(serializers.RelatedField):
     """
-    RelatedField subclass that serializes an M2M to CollectionMembership into
-    a list of apps, rather than a list of CollectionMembership objects.
+    RelatedField subclass that serializes apps in a Collection, taking into
+    account feature profile and optionally relying on ElasticSearch to find
+    the apps instead of making a DB query.
 
     Specifically created for use with CollectionSerializer; you probably don't
     want to use this elsewhere.
     """
     def to_native(self, value):
-        return AppSerializer(value.app, context=self.context).data
+        return AppSerializer(value, context=self.context).data
 
     def field_to_native(self, obj, field_name):
         if not hasattr(self, 'context') or not 'request' in self.context:
@@ -56,15 +57,15 @@ def field_to_native(self, obj, field_name):
             and waffle.switch_is_active('collections-use-es-for-apps')):
             return self.field_to_native_es(obj, request)
 
-        value = get_component(obj, self.source)
+        qs = get_component(obj, self.source)
 
         # Filter apps based on feature profiles.
         profile = get_feature_profile(request)
         if profile:
-            value = value.filter(**profile.to_kwargs(
-                prefix='app___current_version__features__has_'))
+            qs = qs.filter(**profile.to_kwargs(
+                prefix='_current_version__features__has_'))
 
-        return [self.to_native(item) for item in value.all()]
+        return [self.to_native(app) for app in qs]
 
     def field_to_native_es(self, obj, request):
         """
@@ -114,8 +115,7 @@ class CollectionSerializer(serializers.ModelSerializer):
     description = TranslationSerializerField()
     slug = serializers.CharField(required=False)
     collection_type = serializers.IntegerField()
-    apps = CollectionMembershipField(many=True,
-                                     source='collectionmembership_set')
+    apps = CollectionMembershipField(many=True, source='apps')
     image = HyperlinkedRelatedOrNullField(
         source='*',
         view_name='collection-image-detail',
diff --git a/mkt/collections/tests/test_models.py b/mkt/collections/tests/test_models.py
index 786108ead06..4fcfac9a21d 100644
--- a/mkt/collections/tests/test_models.py
+++ b/mkt/collections/tests/test_models.py
@@ -64,6 +64,53 @@ def test_apps(self):
         eq_(list(CollectionMembership.objects.values_list('order', flat=True)),
             [0, 1, 2, 3])
 
+    def test_app_deleted(self):
+        collection = self.collection
+        app = amo.tests.app_factory()
+        collection.add_app(app)
+        self.assertSetEqual(collection.apps(), [app])
+        self.assertSetEqual(collection.collectionmembership_set.all(),
+            [CollectionMembership.objects.get(collection=collection, app=app)])
+
+        app.delete()
+
+        self.assertSetEqual(collection.apps(), [])
+        self.assertSetEqual(collection.collectionmembership_set.all(), [])
+
+    def test_app_disabled_by_user(self):
+        collection = self.collection
+        app = amo.tests.app_factory()
+        collection.add_app(app)
+        self.assertSetEqual(collection.apps(), [app])
+        self.assertSetEqual(collection.collectionmembership_set.all(),
+            [CollectionMembership.objects.get(collection=collection, app=app)])
+
+        app.update(disabled_by_user=True)
+
+        self.assertSetEqual(collection.apps(), [])
+
+        # The collection membership still exists here, the app is not deleted,
+        # only disabled.
+        self.assertSetEqual(collection.collectionmembership_set.all(),
+            [CollectionMembership.objects.get(collection=collection, app=app)])
+
+    def test_app_pending(self):
+        collection = self.collection
+        app = amo.tests.app_factory()
+        collection.add_app(app)
+        self.assertSetEqual(collection.apps(), [app])
+        self.assertSetEqual(collection.collectionmembership_set.all(),
+            [CollectionMembership.objects.get(collection=collection, app=app)])
+
+        app.update(status=amo.STATUS_PENDING)
+
+        self.assertSetEqual(collection.apps(), [])
+
+        # The collection membership still exists here, the app is not deleted,
+        # just not public.
+        self.assertSetEqual(collection.collectionmembership_set.all(),
+            [CollectionMembership.objects.get(collection=collection, app=app)])
+
     def test_mixed_ordering(self):
         self._generate_apps()
 
diff --git a/mkt/collections/tests/test_serializers.py b/mkt/collections/tests/test_serializers.py
index af0d60bb10a..6efd07a8029 100644
--- a/mkt/collections/tests/test_serializers.py
+++ b/mkt/collections/tests/test_serializers.py
@@ -54,7 +54,7 @@ def test_to_native(self):
         resource = AppSerializer(self.app)
         resource.context = {'request': request}
         self.field.context['request'] = request
-        native = self.field.to_native(self.membership)
+        native = self.field.to_native(self.collection.apps()[0])
         for key, value in native.iteritems():
             if key == 'resource_uri':
                 eq_(value, self.app.get_api_url(pk=self.app.pk))
@@ -64,11 +64,10 @@ def test_to_native(self):
     def _field_to_native_profile(self, profile='0.0'):
         request = self.get_request({'pro': profile, 'dev': 'firefoxos'})
         self.field.parent = self.collection
-        self.field.source = 'collectionmembership_set'
+        self.field.source = 'apps'
         self.field.context['request'] = request
 
-        return self.field.field_to_native(self.collection,
-                                          'collectionmembership_set')
+        return self.field.field_to_native(self.collection, 'apps')
 
     def test_ordering(self):
         self.app2 = amo.tests.app_factory()
@@ -78,6 +77,21 @@ def test_ordering(self):
         eq_(int(result[0]['id']), self.app2.id)
         eq_(int(result[1]['id']), self.app.id)
 
+    def test_app_delete(self):
+        self.app.delete()
+        result = self._field_to_native_profile()
+        eq_(len(result), 0)
+
+    def test_app_disable(self):
+        self.app.update(disabled_by_user=True)
+        result = self._field_to_native_profile()
+        eq_(len(result), 0)
+
+    def test_app_pending(self):
+        self.app.update(status=amo.STATUS_PENDING)
+        result = self._field_to_native_profile()
+        eq_(len(result), 0)
+
     def test_field_to_native_profile(self):
         result = self._field_to_native_profile(self.profile)
         eq_(len(result), 1)
@@ -135,6 +149,24 @@ def test_ordering(self):
         eq_(int(result[0]['id']), self.app2.id)
         eq_(int(result[1]['id']), self.app.id)
 
+    def test_app_delete(self):
+        self.app.delete()
+        self.refresh('webapp')
+        result = self._field_to_native_profile()
+        eq_(len(result), 0)
+
+    def test_app_disable(self):
+        self.app.update(disabled_by_user=True)
+        self.refresh('webapp')
+        result = self._field_to_native_profile()
+        eq_(len(result), 0)
+
+    def test_app_pending(self):
+        self.app.update(status=amo.STATUS_PENDING)
+        self.refresh('webapp')
+        result = self._field_to_native_profile()
+        eq_(len(result), 0)
+
 
 class TestCollectionSerializer(CollectionDataMixin, amo.tests.TestCase):
 
diff --git a/mkt/collections/views.py b/mkt/collections/views.py
index 9f6788da354..517b917cdc4 100644
--- a/mkt/collections/views.py
+++ b/mkt/collections/views.py
@@ -193,18 +193,18 @@ def reorder(self, request, pk=None):
         """
         Reorder the specified collection.
         """
-        collection = self.get_object()
         def membership(app):
             f = CollectionMembershipField()
             f.context = {'request': request}
             return f.to_native(app)
+
+        collection = self.get_object()
         try:
             collection.reorder(request.DATA)
         except ValueError:
             return Response({
                 'detail': self.exceptions['app_mismatch'],
-                'apps': [membership(a) for a in
-                         collection.collectionmembership_set.all()]
+                'apps': [membership(a) for a in collection.apps()]
             }, status=status.HTTP_400_BAD_REQUEST, exception=True)
         return self.return_updated(status.HTTP_200_OK)
 
diff --git a/mkt/webapps/models.py b/mkt/webapps/models.py
index 155c669261e..8e4c0a16f4d 100644
--- a/mkt/webapps/models.py
+++ b/mkt/webapps/models.py
@@ -1506,8 +1506,11 @@ def extract_document(cls, pk, obj=None):
         d['app_type'] = obj.app_type_id
         d['author'] = obj.developer_name
         d['category'] = list(obj.categories.values_list('slug', flat=True))
-        d['collection'] = [{'id': cms.collection_id, 'order': cms.order}
-                           for cms in obj.collectionmembership_set.all()]
+        if obj.is_public:
+            d['collection'] = [{'id': cms.collection_id, 'order': cms.order}
+                               for cms in obj.collectionmembership_set.all()]
+        else:
+            d['collection'] = []
         d['content_ratings'] = (obj.get_content_ratings_by_region(es=True) or
                                 None)
         d['content_descriptors'] = obj.get_descriptors(es=True)


---- SAMPLE #80 ----
Generated commit : Add additional tests for GCP products (#241)
Ground-truth     : Bug 1227622 - Add performance framework API endpoint

Also add information about performance frameworks to the list of performance
signatures we return.
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0039 / 0.0714 / 0.0244 / 0.1556 / 0.1794
Diff:
diff --git a/treeherder/webapp/api/performance_data.py b/treeherder/webapp/api/performance_data.py
index 7d05c8e0baf..5574c3ff089 100644
--- a/treeherder/webapp/api/performance_data.py
+++ b/treeherder/webapp/api/performance_data.py
@@ -10,11 +10,13 @@
 from rest_framework.response import Response
 
 from performance_serializers import (PerformanceAlertSerializer,
-                                     PerformanceAlertSummarySerializer)
+                                     PerformanceAlertSummarySerializer,
+                                     PerformanceFrameworkSerializer)
 from treeherder.model import models
 from treeherder.perf.models import (PerformanceAlert,
                                     PerformanceAlertSummary,
                                     PerformanceDatum,
+                                    PerformanceFramework,
                                     PerformanceSignature)
 from treeherder.webapp.api.permissions import IsStaffOrReadOnly
 
@@ -50,13 +52,15 @@ def list(self, request, project):
                 platform__in=platforms)
 
         ret = {}
-        for (signature_hash, option_collection_hash, platform, suite, test,
-             lower_is_better, extra_properties) in signature_data.values_list(
+        for (signature_hash, option_collection_hash, platform, framework,
+             suite, test, lower_is_better,
+             extra_properties) in signature_data.values_list(
                  'signature_hash',
                  'option_collection__option_collection_hash',
-                 'platform__platform', 'suite',
+                 'platform__platform', 'framework', 'suite',
                  'test', 'lower_is_better', 'extra_properties').distinct():
             ret[signature_hash] = {
+                'framework_id': framework,
                 'option_collection_hash': option_collection_hash,
                 'machine_platform': platform,
                 'suite': suite
@@ -85,6 +89,13 @@ def list(self, request, project):
                 'platform__platform', flat=True).distinct())
 
 
+class PerformanceFrameworkViewSet(viewsets.ReadOnlyModelViewSet):
+    queryset = PerformanceFramework.objects.all()
+    serializer_class = PerformanceFrameworkSerializer
+    filter_backends = [filters.OrderingFilter]
+    ordering = 'id'
+
+
 class PerformanceDatumViewSet(viewsets.ViewSet):
     """
     This view serves performance test result data
diff --git a/treeherder/webapp/api/performance_serializers.py b/treeherder/webapp/api/performance_serializers.py
index e0a4f79698e..0b57a6e2804 100644
--- a/treeherder/webapp/api/performance_serializers.py
+++ b/treeherder/webapp/api/performance_serializers.py
@@ -2,9 +2,16 @@
 
 from treeherder.perf.models import (PerformanceAlert,
                                     PerformanceAlertSummary,
+                                    PerformanceFramework,
                                     PerformanceSignature)
 
 
+class PerformanceFrameworkSerializer(serializers.ModelSerializer):
+    class Meta:
+        model = PerformanceFramework
+        fields = ['id', 'name']
+
+
 class TestOptionsSerializer(serializers.JSONField):
     def to_representation(self, obj):
         # if extra_propeties is blank, just return nothing
diff --git a/treeherder/webapp/api/urls.py b/treeherder/webapp/api/urls.py
index dd3ece7c045..2ddd0e5bafe 100644
--- a/treeherder/webapp/api/urls.py
+++ b/treeherder/webapp/api/urls.py
@@ -105,6 +105,9 @@
 default_router.register(r'performance/alert',
                         performance_data.PerformanceAlertViewSet,
                         base_name='performance-alerts')
+default_router.register(r'performance/framework',
+                        performance_data.PerformanceFrameworkViewSet,
+                        base_name='performance-frameworks')
 
 urlpatterns = [
     url(r'^project/(?P<project>[\w-]{0,50})/',


---- SAMPLE #81 ----
Generated commit : [AIRFLOW-300] Add support for SQL expressions (#5184)
Ground-truth     : Update utils.py

add support for Data Profiling with Oracle SQL
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0504 / 0.4211 / 0.3866 / 0.1332 / 0.1934
Diff:
diff --git a/airflow/www/utils.py b/airflow/www/utils.py
index 8b7357a593586..e9a7993f7e251 100644
--- a/airflow/www/utils.py
+++ b/airflow/www/utils.py
@@ -52,6 +52,13 @@ def limit_sql(sql, limit, conn_type):
             {sql}
             ) qry
             """.format(**locals())
+        elif conn_type in ['oracle']:
+            sql = """\
+            SELECT * FROM (
+            {sql}
+            ) qry
+            WHERE ROWNUM <= {limit}
+            """.format(**locals())
         else:
             sql = """\
             SELECT * FROM (


---- SAMPLE #82 ----
Generated commit : Fixed typo in celery task missing CELERYCHAR
Ground-truth     : Fixed worker exception e-mail sending broken due to a bad body string and interpolation.
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0122 / 0.0909 / 0.0376 / 0.0283 / 0.1906
Diff:
diff --git a/celery/worker.py b/celery/worker.py
index 36c5a49e7fb..1023c95c1bb 100644
--- a/celery/worker.py
+++ b/celery/worker.py
@@ -30,7 +30,7 @@
 
 %%(traceback)s
 
-%%(EMAIL_SIGNATURE_SEP)s
+%(EMAIL_SIGNATURE_SEP)s
 Just thought I'd let you know!
 celeryd at %%(hostname)s.
 """ % {"EMAIL_SIGNATURE_SEP": EMAIL_SIGNATURE_SEP}


---- SAMPLE #83 ----
Generated commit : Add device_name to the list of `DeviceType` types
Ground-truth     : Set device_types in website API (bug 1173048)
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.2353 / 0.0000 / 0.1466 / 0.2087
Diff:
diff --git a/mkt/websites/models.py b/mkt/websites/models.py
index 1131bb9053d..443b69c3f05 100644
--- a/mkt/websites/models.py
+++ b/mkt/websites/models.py
@@ -101,6 +101,10 @@ def devices(self):
         # devices.
         return [device.id for device in DEVICE_TYPE_LIST]
 
+    @property
+    def device_names(self):
+        return [device.api_name for device in DEVICE_TYPE_LIST]
+
     def is_dummy_content_for_qa(self):
         """
         Returns whether this app is a dummy app used for testing only or not.
diff --git a/mkt/websites/serializers.py b/mkt/websites/serializers.py
index 37125e5b5f8..393e85539e7 100644
--- a/mkt/websites/serializers.py
+++ b/mkt/websites/serializers.py
@@ -11,6 +11,7 @@
 class WebsiteSerializer(serializers.ModelSerializer):
     categories = ListField(serializers.CharField())
     description = TranslationSerializerField()
+    device_types = ListField(serializers.CharField(), source='device_names')
     id = serializers.IntegerField(source='pk')
     short_name = TranslationSerializerField()
     keywords = serializers.SerializerMethodField('get_keywords')
@@ -20,8 +21,9 @@ class WebsiteSerializer(serializers.ModelSerializer):
 
     class Meta:
         model = Website
-        fields = ['categories', 'description', 'icons', 'id', 'keywords',
-                  'mobile_url', 'name', 'short_name', 'title', 'url']
+        fields = ['categories', 'description', 'device_types', 'icons', 'id',
+                  'keywords', 'mobile_url', 'name', 'short_name', 'title',
+                  'url']
 
     def get_icons(self, obj):
         return dict([(icon_size, obj.get_icon_url(icon_size))


---- SAMPLE #84 ----
Generated commit : fix flake8 tests
Ground-truth     : AWS RDS integration fixes (#22125)

Co-authored-by: Dmytro Kazanzhy <dkazanzhy@demandbase.com>
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.1250 / 0.0595 / -0.0531 / 0.1832
Diff:
diff --git a/airflow/providers/amazon/aws/example_dags/example_rds.py b/airflow/providers/amazon/aws/example_dags/example_rds.py
index 5cf6d85e42646..f30404b9d6867 100644
--- a/airflow/providers/amazon/aws/example_dags/example_rds.py
+++ b/airflow/providers/amazon/aws/example_dags/example_rds.py
@@ -15,10 +15,6 @@
 # KIND, either express or implied.  See the License for the
 # specific language governing permissions and limitations
 # under the License.
-"""
-This is an example dag for using `RedshiftSQLOperator` to authenticate with Amazon Redshift
-then execute a simple select statement
-"""
 
 from datetime import datetime
 
@@ -81,6 +77,7 @@
         export_task_identifier='export-auth-db-snap-{{ ds }}',
         source_arn='arn:aws:rds:<region>:<account number>:snapshot:auth-db-snap',
         s3_bucket_name='my_s3_bucket',
+        s3_prefix='some/prefix',
         iam_role_arn='arn:aws:iam:<region>:<account number>:role/MyRole',
         kms_key_id='arn:aws:kms:<region>:<account number>:key/*****-****-****-****-********',
         aws_conn_id='aws_default',
@@ -105,7 +102,7 @@
     # [START howto_guide_rds_create_subscription]
     create_subscription = RdsCreateEventSubscriptionOperator(
         task_id='create_subscription',
-        subscription_name='my_topic_subscription',
+        subscription_name='my-topic-subscription',
         sns_topic_arn='arn:aws:sns:<region>:<account number>:MyTopic',
         source_type='db-instance',
         source_ids=['auth-db'],
@@ -118,7 +115,7 @@
     # [START howto_guide_rds_delete_subscription]
     delete_subscription = RdsDeleteEventSubscriptionOperator(
         task_id='delete_subscription',
-        subscription_name='my_topic_subscription',
+        subscription_name='my-topic-subscription',
         aws_conn_id='aws_default',
         hook_params={'region_name': 'us-east-1'},
     )
@@ -144,6 +141,7 @@
     export_sensor = RdsExportTaskExistenceSensor(
         task_id='export_sensor',
         export_task_identifier='export-auth-db-snap-{{ ds }}',
+        target_statuses=['starting', 'in_progress', 'complete', 'canceling', 'canceled'],
         aws_conn_id='aws_default',
         hook_params={'region_name': 'us-east-1'},
     )
diff --git a/airflow/providers/amazon/aws/operators/rds.py b/airflow/providers/amazon/aws/operators/rds.py
index e14df928dcbad..a527107e80a46 100644
--- a/airflow/providers/amazon/aws/operators/rds.py
+++ b/airflow/providers/amazon/aws/operators/rds.py
@@ -82,14 +82,15 @@ def _await_status(
             if len(items) > 1:
                 raise AirflowException(f"There are {len(items)} {item_type} with identifier {item_name}")
 
-            if wait_statuses and items[0]['Status'] in wait_statuses:
+            if wait_statuses and items[0]['Status'].lower() in wait_statuses:
+                time.sleep(self._await_interval)
                 continue
-            elif ok_statuses and items[0]['Status'] in ok_statuses:
+            elif ok_statuses and items[0]['Status'].lower() in ok_statuses:
                 break
-            elif error_statuses and items[0]['Status'] in error_statuses:
+            elif error_statuses and items[0]['Status'].lower() in error_statuses:
                 raise AirflowException(f"Item has error status ({error_statuses}): {items[0]}")
-
-            time.sleep(self._await_interval)
+            else:
+                raise AirflowException(f"Item has uncertain status: {items[0]}")
 
         return None
 
@@ -118,7 +119,7 @@ class RdsCreateDbSnapshotOperator(RdsBaseOperator):
         `USER Tagging <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Tagging.html>`__
     """
 
-    template_fields = ("db_snapshot_identifier", "db_instance_identifier", "tags")
+    template_fields = ("db_snapshot_identifier", "db_identifier", "tags")
 
     def __init__(
         self,
@@ -257,7 +258,7 @@ def execute(self, context: 'Context') -> str:
             self._await_status(
                 'instance_snapshot',
                 self.target_db_snapshot_identifier,
-                wait_statuses=['copying'],
+                wait_statuses=['creating'],
                 ok_statuses=['available'],
             )
         else:
@@ -392,7 +393,8 @@ def execute(self, context: 'Context') -> str:
             'export_task',
             self.export_task_identifier,
             wait_statuses=['starting', 'in_progress'],
-            ok_statuses=['available', 'complete'],
+            ok_statuses=['complete'],
+            error_statuses=['canceling', 'canceled'],
         )
 
         return json.dumps(start_export, default=str)
@@ -506,7 +508,7 @@ def execute(self, context: 'Context') -> str:
             'event_subscription',
             self.subscription_name,
             wait_statuses=['creating'],
-            ok_statuses=['created', 'available'],
+            ok_statuses=['active'],
         )
 
         return json.dumps(create_subscription, default=str)
diff --git a/airflow/providers/amazon/aws/sensors/rds.py b/airflow/providers/amazon/aws/sensors/rds.py
index da57e7030a29a..1c74d5ae8fc14 100644
--- a/airflow/providers/amazon/aws/sensors/rds.py
+++ b/airflow/providers/amazon/aws/sensors/rds.py
@@ -62,7 +62,7 @@ def _check_item(self, item_type: str, item_name: str) -> bool:
         except ClientError:
             return False
         else:
-            return bool(items) and any(map(lambda s: items[0]['Status'] == s, self.target_statuses))
+            return bool(items) and any(map(lambda s: items[0]['Status'].lower() == s, self.target_statuses))
 
 
 class RdsSnapshotExistenceSensor(RdsBaseSensor):
@@ -80,7 +80,7 @@ class RdsSnapshotExistenceSensor(RdsBaseSensor):
 
     template_fields: Sequence[str] = (
         'db_snapshot_identifier',
-        'target_status',
+        'target_statuses',
     )
 
     def __init__(
@@ -121,7 +121,7 @@ class RdsExportTaskExistenceSensor(RdsBaseSensor):
 
     template_fields: Sequence[str] = (
         'export_task_identifier',
-        'target_status',
+        'target_statuses',
     )
 
     def __init__(
@@ -135,7 +135,13 @@ def __init__(
         super().__init__(aws_conn_id=aws_conn_id, **kwargs)
 
         self.export_task_identifier = export_task_identifier
-        self.target_statuses = target_statuses or ['available']
+        self.target_statuses = target_statuses or [
+            'starting',
+            'in_progress',
+            'complete',
+            'canceling',
+            'canceled',
+        ]
 
     def poke(self, context: 'Context'):
         self.log.info(
diff --git a/setup.py b/setup.py
index 2e4f3ef72fd69..667819e1fc521 100644
--- a/setup.py
+++ b/setup.py
@@ -612,7 +612,7 @@ def write_version(filename: str = os.path.join(*[my_dir, "airflow", "git_version
     'jira',
     'jsondiff',
     'mongomock',
-    'moto>=3.0.7',
+    'moto>=3.1.0',
     'parameterized',
     'paramiko',
     'pipdeptree',
diff --git a/tests/providers/amazon/aws/operators/test_rds.py b/tests/providers/amazon/aws/operators/test_rds.py
index 0989736ff7981..d952fbc11a93c 100644
--- a/tests/providers/amazon/aws/operators/test_rds.py
+++ b/tests/providers/amazon/aws/operators/test_rds.py
@@ -35,9 +35,9 @@
 from airflow.utils import timezone
 
 try:
-    from moto import mock_rds2
+    from moto import mock_rds
 except ImportError:
-    mock_rds2 = None
+    mock_rds = None
 
 
 DEFAULT_DATE = timezone.datetime(2019, 1, 1)
@@ -165,7 +165,7 @@ def test_await_status_ok(self):
         )
 
 
-@pytest.mark.skipif(mock_rds2 is None, reason='mock_rds2 package not present')
+@pytest.mark.skipif(mock_rds is None, reason='mock_rds package not present')
 class TestRdsCreateDbSnapshotOperator:
     @classmethod
     def setup_class(cls):
@@ -177,7 +177,7 @@ def teardown_class(cls):
         del cls.dag
         del cls.hook
 
-    @mock_rds2
+    @mock_rds
     def test_create_db_instance_snapshot(self):
         _create_db_instance(self.hook)
         instance_snapshot_operator = RdsCreateDbSnapshotOperator(
@@ -196,7 +196,7 @@ def test_create_db_instance_snapshot(self):
         assert instance_snapshots
         assert len(instance_snapshots) == 1
 
-    @mock_rds2
+    @mock_rds
     def test_create_db_cluster_snapshot(self):
         _create_db_cluster(self.hook)
         cluster_snapshot_operator = RdsCreateDbSnapshotOperator(
@@ -216,7 +216,7 @@ def test_create_db_cluster_snapshot(self):
         assert len(cluster_snapshots) == 1
 
 
-@pytest.mark.skipif(mock_rds2 is None, reason='mock_rds2 package not present')
+@pytest.mark.skipif(mock_rds is None, reason='mock_rds package not present')
 class TestRdsCopyDbSnapshotOperator:
     @classmethod
     def setup_class(cls):
@@ -228,7 +228,7 @@ def teardown_class(cls):
         del cls.dag
         del cls.hook
 
-    @mock_rds2
+    @mock_rds
     def test_copy_db_instance_snapshot(self):
         _create_db_instance(self.hook)
         _create_db_instance_snapshot(self.hook)
@@ -248,7 +248,7 @@ def test_copy_db_instance_snapshot(self):
         assert instance_snapshots
         assert len(instance_snapshots) == 1
 
-    @mock_rds2
+    @mock_rds
     def test_copy_db_cluster_snapshot(self):
         _create_db_cluster(self.hook)
         _create_db_cluster_snapshot(self.hook)
@@ -271,7 +271,7 @@ def test_copy_db_cluster_snapshot(self):
         assert len(cluster_snapshots) == 1
 
 
-@pytest.mark.skipif(mock_rds2 is None, reason='mock_rds2 package not present')
+@pytest.mark.skipif(mock_rds is None, reason='mock_rds package not present')
 class TestRdsDeleteDbSnapshotOperator:
     @classmethod
     def setup_class(cls):
@@ -283,7 +283,7 @@ def teardown_class(cls):
         del cls.dag
         del cls.hook
 
-    @mock_rds2
+    @mock_rds
     def test_delete_db_instance_snapshot(self):
         _create_db_instance(self.hook)
         _create_db_instance_snapshot(self.hook)
@@ -300,7 +300,7 @@ def test_delete_db_instance_snapshot(self):
         with pytest.raises(self.hook.conn.exceptions.ClientError):
             self.hook.conn.describe_db_snapshots(DBSnapshotIdentifier=DB_CLUSTER_SNAPSHOT)
 
-    @mock_rds2
+    @mock_rds
     def test_delete_db_cluster_snapshot(self):
         _create_db_cluster(self.hook)
         _create_db_cluster_snapshot(self.hook)
@@ -318,7 +318,7 @@ def test_delete_db_cluster_snapshot(self):
             self.hook.conn.describe_db_cluster_snapshots(DBClusterSnapshotIdentifier=DB_CLUSTER_SNAPSHOT)
 
 
-@pytest.mark.skipif(mock_rds2 is None, reason='mock_rds2 package not present')
+@pytest.mark.skipif(mock_rds is None, reason='mock_rds package not present')
 class TestRdsStartExportTaskOperator:
     @classmethod
     def setup_class(cls):
@@ -330,7 +330,7 @@ def teardown_class(cls):
         del cls.dag
         del cls.hook
 
-    @mock_rds2
+    @mock_rds
     def test_start_export_task(self):
         _create_db_instance(self.hook)
         _create_db_instance_snapshot(self.hook)
@@ -352,10 +352,10 @@ def test_start_export_task(self):
 
         assert export_tasks
         assert len(export_tasks) == 1
-        assert export_tasks[0]['Status'] == 'available'
+        assert export_tasks[0]['Status'] == 'complete'
 
 
-@pytest.mark.skipif(mock_rds2 is None, reason='mock_rds2 package not present')
+@pytest.mark.skipif(mock_rds is None, reason='mock_rds package not present')
 class TestRdsCancelExportTaskOperator:
     @classmethod
     def setup_class(cls):
@@ -367,7 +367,7 @@ def teardown_class(cls):
         del cls.dag
         del cls.hook
 
-    @mock_rds2
+    @mock_rds
     def test_cancel_export_task(self):
         _create_db_instance(self.hook)
         _create_db_instance_snapshot(self.hook)
@@ -389,7 +389,7 @@ def test_cancel_export_task(self):
         assert export_tasks[0]['Status'] == 'canceled'
 
 
-@pytest.mark.skipif(mock_rds2 is None, reason='mock_rds2 package not present')
+@pytest.mark.skipif(mock_rds is None, reason='mock_rds package not present')
 class TestRdsCreateEventSubscriptionOperator:
     @classmethod
     def setup_class(cls):
@@ -401,7 +401,7 @@ def teardown_class(cls):
         del cls.dag
         del cls.hook
 
-    @mock_rds2
+    @mock_rds
     def test_create_event_subscription(self):
         _create_db_instance(self.hook)
 
@@ -421,10 +421,10 @@ def test_create_event_subscription(self):
 
         assert subscriptions
         assert len(subscriptions) == 1
-        assert subscriptions[0]['Status'] == 'available'
+        assert subscriptions[0]['Status'] == 'active'
 
 
-@pytest.mark.skipif(mock_rds2 is None, reason='mock_rds2 package not present')
+@pytest.mark.skipif(mock_rds is None, reason='mock_rds package not present')
 class TestRdsDeleteEventSubscriptionOperator:
     @classmethod
     def setup_class(cls):
@@ -436,7 +436,7 @@ def teardown_class(cls):
         del cls.dag
         del cls.hook
 
-    @mock_rds2
+    @mock_rds
     def test_delete_event_subscription(self):
         _create_event_subscription(self.hook)
 
diff --git a/tests/providers/amazon/aws/sensors/test_rds.py b/tests/providers/amazon/aws/sensors/test_rds.py
index 1dbeeaea85eec..f93b93174eded 100644
--- a/tests/providers/amazon/aws/sensors/test_rds.py
+++ b/tests/providers/amazon/aws/sensors/test_rds.py
@@ -28,9 +28,9 @@
 from airflow.utils import timezone
 
 try:
-    from moto import mock_rds2
+    from moto import mock_rds
 except ImportError:
-    mock_rds2 = None
+    mock_rds = None
 
 
 DEFAULT_DATE = timezone.datetime(2019, 1, 1)
@@ -132,7 +132,7 @@ def test_check_item_false(self):
         assert not self.base_sensor._check_item(item_type='instance_snapshot', item_name='')
 
 
-@pytest.mark.skipif(mock_rds2 is None, reason='mock_rds2 package not present')
+@pytest.mark.skipif(mock_rds is None, reason='mock_rds package not present')
 class TestRdsSnapshotExistenceSensor:
     @classmethod
     def setup_class(cls):
@@ -144,7 +144,7 @@ def teardown_class(cls):
         del cls.dag
         del cls.hook
 
-    @mock_rds2
+    @mock_rds
     def test_db_instance_snapshot_poke_true(self):
         _create_db_instance_snapshot(self.hook)
         op = RdsSnapshotExistenceSensor(
@@ -156,7 +156,7 @@ def test_db_instance_snapshot_poke_true(self):
         )
         assert op.poke(None)
 
-    @mock_rds2
+    @mock_rds
     def test_db_instance_snapshot_poke_false(self):
         op = RdsSnapshotExistenceSensor(
             task_id='test_instance_snap_false',
@@ -167,7 +167,7 @@ def test_db_instance_snapshot_poke_false(self):
         )
         assert not op.poke(None)
 
-    @mock_rds2
+    @mock_rds
     def test_db_instance_cluster_poke_true(self):
         _create_db_cluster_snapshot(self.hook)
         op = RdsSnapshotExistenceSensor(
@@ -179,7 +179,7 @@ def test_db_instance_cluster_poke_true(self):
         )
         assert op.poke(None)
 
-    @mock_rds2
+    @mock_rds
     def test_db_instance_cluster_poke_false(self):
         op = RdsSnapshotExistenceSensor(
             task_id='test_cluster_snap_false',
@@ -191,7 +191,7 @@ def test_db_instance_cluster_poke_false(self):
         assert not op.poke(None)
 
 
-@pytest.mark.skipif(mock_rds2 is None, reason='mock_rds2 package not present')
+@pytest.mark.skipif(mock_rds is None, reason='mock_rds package not present')
 class TestRdsExportTaskExistenceSensor:
     @classmethod
     def setup_class(cls):
@@ -203,7 +203,7 @@ def teardown_class(cls):
         del cls.dag
         del cls.hook
 
-    @mock_rds2
+    @mock_rds
     def test_export_task_poke_true(self):
         _create_db_instance_snapshot(self.hook)
         _start_export_task(self.hook)
@@ -215,7 +215,7 @@ def test_export_task_poke_true(self):
         )
         assert op.poke(None)
 
-    @mock_rds2
+    @mock_rds
     def test_export_task_poke_false(self):
         _create_db_instance_snapshot(self.hook)
         op = RdsExportTaskExistenceSensor(


---- SAMPLE #85 ----
Generated commit : Add support for certain documentation
Ground-truth     : averez-56-healthinfo: store frontend stats in mongo
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / -0.0961 / 0.1788
Diff:
diff --git a/cron/healthToMongo.py b/cron/healthToMongo.py
new file mode 100644
index 000000000..a399aaf65
--- /dev/null
+++ b/cron/healthToMongo.py
@@ -0,0 +1,148 @@
+#!/usr/bin/env python
+
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+# Copyright (c) 2014 Mozilla Corporation
+#
+# Contributors:
+# Anthony Verez averez@mozilla.com
+
+import json
+import logging
+import os
+import pyes
+import pytz
+import requests
+import sys
+from datetime import datetime
+from datetime import timedelta
+from configlib import getConfig, OptionParser
+from logging.handlers import SysLogHandler
+from dateutil.parser import parse
+from pymongo import MongoClient
+
+logger = logging.getLogger(sys.argv[0])
+
+
+def loggerTimeStamp(self, record, datefmt=None):
+    return toUTC(datetime.now()).isoformat()
+
+
+def initLogger():
+    logger.level = logging.INFO
+    formatter = logging.Formatter(
+        '%(asctime)s - %(name)s - %(levelname)s - %(message)s')
+    formatter.formatTime = loggerTimeStamp
+    if options.output == 'syslog':
+        logger.addHandler(
+            SysLogHandler(
+                address=(options.sysloghostname,
+                    options.syslogport)))
+    else:
+        sh = logging.StreamHandler(sys.stderr)
+        sh.setFormatter(formatter)
+        logger.addHandler(sh)
+
+
+def toUTC(suspectedDate, localTimeZone="US/Pacific"):
+    '''make a UTC date out of almost anything'''
+    utc = pytz.UTC
+    objDate = None
+    if type(suspectedDate) == str:
+        objDate = parse(suspectedDate, fuzzy=True)
+    elif type(suspectedDate) == datetime:
+        objDate = suspectedDate
+
+    if objDate.tzinfo is None:
+        objDate = pytz.timezone(localTimeZone).localize(objDate)
+        objDate = utc.normalize(objDate)
+    else:
+        objDate = utc.normalize(objDate)
+    if objDate is not None:
+        objDate = utc.normalize(objDate)
+
+    return objDate
+
+
+def getFrontendStats(es):
+    begindateUTC = toUTC(datetime.now() - timedelta(minutes=1))
+    enddateUTC = toUTC(datetime.now())
+    qDate = pyes.RangeQuery(qrange=pyes.ESRange('utctimestamp',
+        from_value=begindateUTC, to_value=enddateUTC))
+    qType = pyes.TermFilter('_type', 'mozdefhealth')
+    qMozdef = pyes.TermsFilter('category', ['mozdef'])
+    pyesresults = es.search(pyes.ConstantScoreQuery(pyes.BoolFilter(
+        must=[qType, qDate, qMozdef])),
+        indices='events')
+    return pyesresults._search_raw()['hits']['hits']
+
+
+def writeFrontendStats(data, mongo):
+    for host in data:
+        for key in host['_source']['details'].keys():
+            # remove unwanted data
+            if '.' in key:
+                del host['_source']['details'][key]
+        mongo.healthfrontend.insert(host['_source'])
+        # print host['_source']['hostname']
+        # print host['_source']['details']['loadaverage']
+        # for key in host['_source']['details'].keys():
+            # if key not in ('username', 'loadaverage'):
+                # print key
+                # print host['_source']['details'][key]['publish_eps']
+                # print host['_source']['details'][key]['messages_ready']
+                # print host['_source']['details'][key]['messages_unacknowledged']
+                # if 'deliver_eps' in host['_source']['details'][key].keys():
+                    # print host['_source']['details'][key]['deliver_eps']
+        # print ''
+
+
+def main():
+    logger.debug('starting')
+    logger.debug(options)
+    try:
+        es = pyes.ES(server=(list('{0}'.format(s) for s in options.esservers)))
+        client = MongoClient(options.mongohost, options.mongoport)
+        # use meteor db
+        mongo = client.meteor
+        writeFrontendStats(getFrontendStats(es), mongo)
+    except Exception as e:
+        logger.error("Exception %r sending health to mongo" % e)
+
+
+def initConfig():
+    # output our log to stdout or syslog
+    options.output = getConfig('output', 'stdout', options.configfile)
+    # syslog hostname
+    options.sysloghostname = getConfig('sysloghostname', 'localhost',
+        options.configfile)
+    # syslog port
+    options.syslogport = getConfig('syslogport', 514, options.configfile)
+
+    options.mqservers = list(getConfig('mqservers', 'localhost',
+        options.configfile).split(','))
+    options.mquser = getConfig('mquser', 'guest', options.configfile)
+    options.mqpassword = getConfig('mqpassword', 'guest', options.configfile)
+    # port of the rabbitmq json management interface
+    options.mqapiport = getConfig('mqapiport', 15672, options.configfile)
+
+    # elastic search server settings
+    options.esservers = list(getConfig('esservers', 'http://localhost:9200',
+        options.configfile).split(','))
+    options.mongohost = getConfig('mongohost', 'localhost', options.configfile)
+    options.mongoport = getConfig('mongoport', 3001, options.configfile)
+
+
+if __name__ == '__main__':
+    parser = OptionParser()
+    parser.add_option(
+        "-c",
+        dest='configfile',
+        default=sys.argv[0].replace('.py', '.conf'),
+        help="configuration file to use")
+    (options, args) = parser.parse_args()
+    initConfig()
+    initLogger()
+    main()
+


---- SAMPLE #86 ----
Generated commit : Fix bug in sample data collection
Ground-truth     : Use the new getkey helper in more places.

Also use ApiKey.querykeys inside the monitoring task.
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0091 / 0.0909 / 0.0355 / 0.1387 / 0.1822
Diff:
diff --git a/ichnaea/data/observation.py b/ichnaea/data/observation.py
index 88ca3ecb9..e3fda0fac 100644
--- a/ichnaea/data/observation.py
+++ b/ichnaea/data/observation.py
@@ -66,7 +66,7 @@ def insert(self, entries, userid=None):
         for key, observations in station_observations.items():
             first_blacklisted = None
             incomplete = False
-            station = self.station_model.querykey(self.session, key).first()
+            station = self.station_model.getkey(self.session, key)
 
             if station is None:
                 # Drop observations for blacklisted stations.
diff --git a/ichnaea/data/tasks.py b/ichnaea/data/tasks.py
index f31320989..650d1abfb 100644
--- a/ichnaea/data/tasks.py
+++ b/ichnaea/data/tasks.py
@@ -35,7 +35,7 @@ def insert_measures(self, items=None, nickname='', email='',
 
     reports = kombu_loads(items)
     with self.db_session() as session:
-        api_key = api_key_text and session.query(ApiKey).filter(ApiKey.valid_key==api_key_text).first()
+        api_key = api_key_text and ApiKey.getkey(session, api_key_text)
 
         queue = ReportQueueV1(self, session,
                               api_key,
diff --git a/ichnaea/models/content.py b/ichnaea/models/content.py
index fa629de0a..ba84a333e 100644
--- a/ichnaea/models/content.py
+++ b/ichnaea/models/content.py
@@ -73,7 +73,7 @@ class Score(IdMixin, HashKeyMixin, _Model):
 
     @classmethod
     def incr(cls, session, key, value):
-        score = cls.querykey(session, key).first()
+        score = cls.getkey(session, key)
         if score is not None:
             score.value += int(value)
         else:
diff --git a/ichnaea/monitor/tasks.py b/ichnaea/monitor/tasks.py
index e5a11c46c..d980fc899 100644
--- a/ichnaea/monitor/tasks.py
+++ b/ichnaea/monitor/tasks.py
@@ -1,4 +1,5 @@
 from sqlalchemy import func
+from sqlalchemy.orm import load_only
 
 from ichnaea.async.app import celery_app
 from ichnaea.async.task import DatabaseTask
@@ -15,14 +16,10 @@
 def monitor_api_key_limits(self):
     result = {}
     try:
-        redis_client = self.app.redis_client
-        stats_client = self.stats_client
-        now = util.utcnow()
-        today = now.strftime("%Y%m%d")
-
-        keys = redis_client.keys('apilimit:*:' + today)
+        today = util.utcnow().strftime("%Y%m%d")
+        keys = self.redis_client.keys('apilimit:*:' + today)
         if keys:
-            values = redis_client.mget(keys)
+            values = self.redis_client.mget(keys)
             keys = [k.split(':')[1] for k in keys]
         else:
             values = []
@@ -30,18 +27,17 @@ def monitor_api_key_limits(self):
         names = {}
         if keys:
             with self.db_session() as session:
-                q = session.query(ApiKey.valid_key, ApiKey.shortname).filter(
-                    ApiKey.valid_key.in_(keys))
-                names = dict(q.all())
+                query = (ApiKey.querykeys(session, keys)
+                               .options(load_only('valid_key', 'shortname')))
+                for api_key in query.all():
+                    names[api_key.valid_key] = api_key.name
 
         result = {}
         for k, v in zip(keys, values):
-            name = names.get(k)
-            if not name:
-                name = k
+            name = names.get(k, k)
             value = int(v)
             result[name] = value
-            stats_client.gauge('apilimit.' + name, value)
+            self.stats_client.gauge('apilimit.' + name, value)
     except Exception:  # pragma: no cover
         # Log but ignore the exception
         self.raven_client.captureException()


---- SAMPLE #87 ----
Generated commit : [AIRFLOW-3297] Fix bug in qubole hook (#4836)
Ground-truth     : [AIRFLOW-1657] Handle failing qubole operator

Closes #2643 from rupesh92/AIRFLOW-1657
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0248 / 0.2000 / 0.0568 / 0.3211 / 0.2135
Diff:
diff --git a/airflow/contrib/hooks/qubole_hook.py b/airflow/contrib/hooks/qubole_hook.py
index 833c1c7ccf4b4..f3bcc202ed7f1 100755
--- a/airflow/contrib/hooks/qubole_hook.py
+++ b/airflow/contrib/hooks/qubole_hook.py
@@ -202,9 +202,6 @@ def create_cmd_args(self, context):
         args.append("--tags={0}".format(','.join(filter(None,tags))))
 
         if inplace_args is not None:
-            if cmd_type == 'hadoopcmd':
-                args += inplace_args.split(' ', 1)
-            else:
-                args += inplace_args.split(' ')
+            args += inplace_args.split(' ')
 
         return args
diff --git a/tests/contrib/operators/test_qubole_operator.py b/tests/contrib/operators/test_qubole_operator.py
index 0e6e13dc2f020..9b5bf3a7f4be8 100644
--- a/tests/contrib/operators/test_qubole_operator.py
+++ b/tests/contrib/operators/test_qubole_operator.py
@@ -89,6 +89,19 @@ def test_position_args_parameters(self):
         self.assertEqual(task.get_hook().create_cmd_args({'run_id':'dummy'})[2],
                          "key2=value2")
 
-
+        task = QuboleOperator(task_id=TASK_ID, command_type='hadoopcmd',
+                          sub_command="s3distcp --src s3n://airflow/source_hadoopcmd " +
+                                      "--dest s3n://airflow/destination_hadoopcmd", dag=dag)
+
+        self.assertEqual(task.get_hook().create_cmd_args({'run_id': 'dummy'})[1],
+                         "s3distcp")
+        self.assertEqual(task.get_hook().create_cmd_args({'run_id': 'dummy'})[2],
+                         "--src")
+        self.assertEqual(task.get_hook().create_cmd_args({'run_id': 'dummy'})[3],
+                         "s3n://airflow/source_hadoopcmd")
+        self.assertEqual(task.get_hook().create_cmd_args({'run_id': 'dummy'})[4],
+                         "--dest")
+        self.assertEqual(task.get_hook().create_cmd_args({'run_id': 'dummy'})[5],
+                         "s3n://airflow/destination_hadoopcmd")
 
 


---- SAMPLE #88 ----
Generated commit : nan
Ground-truth     : fix typo in data migration
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / 0.0695 / 0.1819
Diff:
diff --git a/kitsune/products/migrations/0010_auto_20240624_0616.py b/kitsune/products/migrations/0010_auto_20240624_0616.py
index 3815d1dd4ad..0ff39555914 100644
--- a/kitsune/products/migrations/0010_auto_20240624_0616.py
+++ b/kitsune/products/migrations/0010_auto_20240624_0616.py
@@ -11,7 +11,7 @@
 # If it exists, the articles will be moved to the existing topic for each product.
 # If the topics is created, a redirect will be added to the old topic.
 TOPICS_TO_MIGRATE = {
-    "install-and-update:": [
+    "install-and-update": [
         "install-and-update-firefox",
         "install-and-update-firefox-ios",
         "download-and-install",


---- SAMPLE #89 ----
Generated commit : Fixing small typo in config file (#2732)
Ground-truth     : Add alert plugins whitelist to docker alert config
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0286 / 0.1333 / 0.0633 / 0.1293 / 0.1784
Diff:
diff --git a/docker/conf/config.py b/docker/conf/config.py
index 2ca8788d6..bcde59953 100644
--- a/docker/conf/config.py
+++ b/docker/conf/config.py
@@ -17,6 +17,10 @@
     'unauth_ssh.AlertUnauthSSH': {'schedule': crontab(minute='*/1')},
 }
 
+ALERT_PLUGINS = [
+    # 'relative pythonfile name (exclude the .py) - EX: sso_dashboard',
+]
+
 RABBITMQ = {
     'mqserver': 'localhost',
     'mquser': 'guest',


---- SAMPLE #90 ----
Generated commit : fix: add the correct repo version check in the API (#314)
Ground-truth     : memorize last choice in addon firefox version (bug 729414)
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0224 / 0.1000 / 0.1087 / 0.1198 / 0.1838
Diff:
diff --git a/apps/search/tests/test_views.py b/apps/search/tests/test_views.py
index 5451fca6cdc..1d0a3ffd673 100644
--- a/apps/search/tests/test_views.py
+++ b/apps/search/tests/test_views.py
@@ -977,6 +977,28 @@ def test_results_other_applications(self):
         eq_(self.get_results(r), [sm_collection.id])
 
 
+    def test_session_version_sidebar(self):
+        request = RequestFactory()
+        request.session = {}
+        request.APP = amo.FIREFOX
+
+        request.get(reverse('search.search'))
+        facets = {
+            u'platforms': [{u'count': 58, u'term': 1}],
+            u'appversions': [{u'count': 58, u'term': 5000000200100}],
+            u'categories': [{u'count': 55, u'term': 1}],
+            u'tags': [],
+        }
+        versions = version_sidebar(request, {}, facets)
+        assert not versions[1].selected
+
+        versions = version_sidebar(request, {'appver': '5.0'}, facets)
+        assert versions[1].selected
+
+        versions = version_sidebar(request, {}, facets)
+        assert versions[1].selected
+
+
 def test_search_redirects():
     changes = (
         ('q=yeah&sort=newest', 'q=yeah&sort=updated'),
diff --git a/apps/search/views.py b/apps/search/views.py
index 8f659b53156..2f214f626ba 100644
--- a/apps/search/views.py
+++ b/apps/search/views.py
@@ -533,7 +533,9 @@ def category_sidebar(request, query, facets):
 
 
 def version_sidebar(request, query, facets):
-    appver = query.get('appver')
+    appver = query.get('appver') or request.session.get('search.appver')
+    if query.get('appver'):
+        request.session['search.appver'] = appver
     app = unicode(request.APP.pretty)
     exclude_versions = getattr(request.APP, 'exclude_versions', [])
     # L10n: {0} is an application, such as Firefox. This means "any version of


---- SAMPLE #91 ----
Generated commit : Fix bug in vio code snip point of incorrect type
Ground-truth     : MPP-2085 - Change premium plan map ID config to include new countries.
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / 0.0145 / 0.1756
Diff:
diff --git a/api/views/__init__.py b/api/views/__init__.py
index a48bea98a7..5ac09eabcc 100644
--- a/api/views/__init__.py
+++ b/api/views/__init__.py
@@ -24,7 +24,7 @@
 from allauth.socialaccount.providers.fxa.provider import FirefoxAccountsProvider  # type: ignore
 from allauth.socialaccount.providers.fxa.views import FirefoxAccountsOAuth2Adapter
 from django_filters import rest_framework as filters
-from waffle import get_waffle_flag_model
+from waffle import flag_is_active, get_waffle_flag_model
 from waffle.models import Switch, Sample
 from rest_framework import (
     decorators,
@@ -38,6 +38,7 @@
 
 from privaterelay.utils import (
     get_countries_info_from_request_and_mapping,
+    get_premium_country_language_mapping,
 )
 
 from emails.models import (
@@ -242,6 +243,8 @@ def runtime_data(request):
     switch_values = [(s.name, s.is_active()) for s in switches]
     samples = Sample.get_all()
     sample_values = [(s.name, s.is_active()) for s in samples]
+    eu_country_expansion = flag_is_active(request, "eu_country_expansion")
+    premium_mapping = get_premium_country_language_mapping(eu_country_expansion)
     return response.Response(
         {
             "FXA_ORIGIN": settings.FXA_BASE_ORIGIN,
@@ -250,7 +253,7 @@ def runtime_data(request):
             "BUNDLE_PRODUCT_ID": settings.BUNDLE_PROD_ID,
             "PHONE_PRODUCT_ID": settings.PHONE_PROD_ID,
             "PERIODICAL_PREMIUM_PLANS": get_countries_info_from_request_and_mapping(
-                request, settings.PERIODICAL_PREMIUM_PLAN_COUNTRY_LANG_MAPPING
+                request, premium_mapping
             ),
             "PHONE_PLANS": get_countries_info_from_request_and_mapping(
                 request, settings.PHONE_PLAN_COUNTRY_LANG_MAPPING
diff --git a/emails/models.py b/emails/models.py
index 0193af790e..a9bfca9240 100644
--- a/emails/models.py
+++ b/emails/models.py
@@ -22,7 +22,7 @@
 from rest_framework.authtoken.models import Token
 
 from api.exceptions import ErrorContextType, RelayAPIException
-from privaterelay.utils import flag_is_active_in_task
+from privaterelay.utils import flag_is_active_in_task, get_premium_countries
 
 
 emails_config = apps.get_app_config("emails")
@@ -172,11 +172,14 @@ def language(self):
     def fxa_locale_in_premium_country(self):
         if self.fxa and self.fxa.extra_data.get("locale"):
             accept_langs = parse_accept_lang_header(self.fxa.extra_data.get("locale"))
+            eu_country_expansion = flag_is_active_in_task(
+                "eu_country_expansion", self.user
+            )
+            premium_countries = get_premium_countries(eu_country_expansion)
             if (
                 len(accept_langs) >= 1
                 and len(accept_langs[0][0].split("-")) >= 2
-                and accept_langs[0][0].split("-")[1]
-                in settings.PERIODICAL_PREMIUM_PLAN_COUNTRY_LANG_MAPPING.keys()
+                and accept_langs[0][0].split("-")[1] in premium_countries
             ):
                 return True
             # If a language but no country is known, check if there's a country
@@ -186,8 +189,7 @@ def fxa_locale_in_premium_country(self):
             if (
                 len(accept_langs) >= 1
                 and len(accept_langs[0][0].split("-")) == 1
-                and accept_langs[0][0].split("-")[0]
-                in settings.PERIODICAL_PREMIUM_PLAN_COUNTRY_LANG_MAPPING.keys()
+                and accept_langs[0][0].split("-")[0] in premium_countries
             ):
                 return True
         return False
diff --git a/emails/tests/models_tests.py b/emails/tests/models_tests.py
index 46bf730c47..3f93568792 100644
--- a/emails/tests/models_tests.py
+++ b/emails/tests/models_tests.py
@@ -13,6 +13,7 @@
 )
 
 from allauth.socialaccount.models import SocialAccount
+from waffle.testutils import override_flag
 import pytest
 
 from model_bakery import baker
@@ -830,6 +831,26 @@ def test_locale_in_premium_country_returns_False_if_premium_not_available_in_cou
     def test_locale_in_premium_country_returns_False_if_no_fxa_account(self):
         assert self.profile.fxa_locale_in_premium_country is False
 
+    @override_flag("eu_country_expansion", active=True)
+    def test_locale_in_premium_country_with_eu_expansion_flag(self):
+        baker.make(
+            SocialAccount,
+            user=self.profile.user,
+            provider="fxa",
+            extra_data={"locale": "et-ee,et;q=0.8"},
+        )
+        assert self.profile.fxa_locale_in_premium_country is True
+
+    @override_flag("eu_country_expansion", active=False)
+    def test_locale_in_premium_country_without_eu_expansion_flag(self):
+        baker.make(
+            SocialAccount,
+            user=self.profile.user,
+            provider="fxa",
+            extra_data={"locale": "et-ee,et;q=0.8"},
+        )
+        assert self.profile.fxa_locale_in_premium_country is False
+
     def test_user_joined_before_premium_release_returns_True(self):
         user = baker.make(
             User, date_joined=datetime.fromisoformat("2021-10-18 17:00:00+00:00")
diff --git a/privaterelay/settings.py b/privaterelay/settings.py
index c216fdb0c2..bcf7e5a718 100644
--- a/privaterelay/settings.py
+++ b/privaterelay/settings.py
@@ -507,6 +507,129 @@ def _get_initial_middleware() -> list[str]:
                 "currency": "EUR",
             },
         },
+        "si": {
+            "monthly": {
+                "id": "price_1NHALmJNcmPzuWtR2nIoAzEt",
+                "price": 1.99,
+                "currency": "EUR",
+            },
+            "yearly": {
+                "id": "price_1NHAL9JNcmPzuWtRSZ3BWQs0",
+                "price": 0.99,
+                "currency": "EUR",
+            },
+        },
+        "sk": {
+            "monthly": {
+                "id": "price_1NHAJsJNcmPzuWtR71WX0Pz9",
+                "price": 1.99,
+                "currency": "EUR",
+            },
+            "yearly": {
+                "id": "price_1NHAKYJNcmPzuWtRtETl30gb",
+                "price": 0.99,
+                "currency": "EUR",
+            },
+        },
+        "pt": {
+            "monthly": {
+                "id": "price_1NHAI1JNcmPzuWtRx8jXjkrQ",
+                "price": 1.99,
+                "currency": "EUR",
+            },
+            "yearly": {
+                "id": "price_1NHAHWJNcmPzuWtRCRMnWyvK",
+                "price": 0.99,
+                "currency": "EUR",
+            },
+        },
+        "lu": {
+            "monthly": {
+                "id": "price_1NHAFZJNcmPzuWtRm5A7w5qJ",
+                "price": 1.99,
+                "currency": "EUR",
+            },
+            "yearly": {
+                "id": "price_1NHAF8JNcmPzuWtRG1FiPK0N",
+                "price": 0.99,
+                "currency": "EUR",
+            },
+        },
+        "lt": {
+            "monthly": {
+                "id": "price_1NHACcJNcmPzuWtR5ZJeVtJA",
+                "price": 1.99,
+                "currency": "EUR",
+            },
+            "yearly": {
+                "id": "price_1NHADOJNcmPzuWtR2PSMBMLr",
+                "price": 0.99,
+                "currency": "EUR",
+            },
+        },
+        "lv": {
+            "monthly": {
+                "id": "price_1NHAASJNcmPzuWtRpcliwx0R",
+                "price": 1.99,
+                "currency": "EUR",
+            },
+            "yearly": {
+                "id": "price_1NHA9lJNcmPzuWtRLf7DV6GA",
+                "price": 0.99,
+                "currency": "EUR",
+            },
+        },
+        "gr": {
+            "monthly": {
+                "id": "price_1NHA5CJNcmPzuWtR1JSmxqFA",
+                "price": 1.99,
+                "currency": "EUR",
+            },
+            "yearly": {
+                "id": "price_1NHA4lJNcmPzuWtRniS23IuE",
+                "price": 0.99,
+                "currency": "EUR",
+            },
+        },
+        "ee": {
+            "monthly": {
+                "id": "price_1NHA1tJNcmPzuWtRvSeyiVYH",
+                "price": 1.99,
+                "currency": "EUR",
+            },
+            "yearly": {
+                "id": "price_1NHA2TJNcmPzuWtR10yknZHf",
+                "price": 0.99,
+                "currency": "EUR",
+            },
+        },
+        "mt": {
+            "monthly": {
+                "id": "price_1NH9yxJNcmPzuWtRChanpIQU",
+                "price": 1.99,
+                "currency": "EUR",
+            },
+            "yearly": {
+                "id": "price_1NH9y3JNcmPzuWtRIJkQos9q",
+                "price": 0.99,
+                "currency": "EUR",
+            },
+        },
+        # TODO: clarify this entry
+        # "cy" the language code means Welsh
+        # "cy" means "Cyprus" in our usage, which is probably Greek or Turkish
+        "cy": {
+            "monthly": {
+                "id": "price_1NH9saJNcmPzuWtRpffF5I59",
+                "price": 1.99,
+                "currency": "EUR",
+            },
+            "yearly": {
+                "id": "price_1NH9rKJNcmPzuWtRzDiXCeEG",
+                "price": 0.99,
+                "currency": "EUR",
+            },
+        },
     },
     "usd": {
         "en": {
@@ -535,6 +658,7 @@ def _get_initial_middleware() -> list[str]:
         },
     },
 }
+
 PERIODICAL_PREMIUM_PLAN_COUNTRY_LANG_MAPPING = {
     # Austria
     "at": {
@@ -584,25 +708,74 @@ def _get_initial_middleware() -> list[str]:
     "fi": {
         "fi": PERIODICAL_PREMIUM_PLAN_ID_MATRIX["euro"]["fi"],
     },
+    # United States
     "us": {
         "en": PERIODICAL_PREMIUM_PLAN_ID_MATRIX["usd"]["en"],
     },
+    # United Kingdom
     "gb": {
         "en": PERIODICAL_PREMIUM_PLAN_ID_MATRIX["usd"]["gb"],
     },
+    # Canada
     "ca": {
         "en": PERIODICAL_PREMIUM_PLAN_ID_MATRIX["usd"]["en"],
     },
+    # New Zealand
     "nz": {
         "en": PERIODICAL_PREMIUM_PLAN_ID_MATRIX["usd"]["gb"],
     },
+    # Malaysia
     "my": {
         "en": PERIODICAL_PREMIUM_PLAN_ID_MATRIX["usd"]["gb"],
     },
+    # Singapore
     "sg": {
         "en": PERIODICAL_PREMIUM_PLAN_ID_MATRIX["usd"]["gb"],
     },
 }
+EU_EXPANSION_PREMIUM_PLAN_COUNTRY_LANG_MAPPING = {
+    # Cyprus
+    "cy": {
+        # TODO: Welsh (cy) seems wrong. Maybe el (greek) and tr (turkish)?
+        "en": PERIODICAL_PREMIUM_PLAN_ID_MATRIX["euro"]["cy"],
+    },
+    # Estonia
+    "ee": {
+        "et": PERIODICAL_PREMIUM_PLAN_ID_MATRIX["euro"]["ee"],
+    },
+    # Greece
+    "gr": {
+        "el": PERIODICAL_PREMIUM_PLAN_ID_MATRIX["euro"]["gr"],
+    },
+    # Latvia
+    "lv": {
+        "lv": PERIODICAL_PREMIUM_PLAN_ID_MATRIX["euro"]["lv"],
+    },
+    # Lithuania
+    "lt": {
+        "lt": PERIODICAL_PREMIUM_PLAN_ID_MATRIX["euro"]["lt"],
+    },
+    # Luxembourg
+    "lu": {
+        "en": PERIODICAL_PREMIUM_PLAN_ID_MATRIX["euro"]["lu"],
+    },
+    # Malta
+    "mt": {
+        "en": PERIODICAL_PREMIUM_PLAN_ID_MATRIX["euro"]["mt"],
+    },
+    # Portugal
+    "pt": {
+        "pt": PERIODICAL_PREMIUM_PLAN_ID_MATRIX["euro"]["pt"],
+    },
+    # Slovakia
+    "sk": {
+        "sk": PERIODICAL_PREMIUM_PLAN_ID_MATRIX["euro"]["sk"],
+    },
+    # Slovenia
+    "si": {
+        "sl": PERIODICAL_PREMIUM_PLAN_ID_MATRIX["euro"]["si"],
+    },
+}
 
 PHONE_PLAN_ID_MATRIX = {
     "usd": {
diff --git a/privaterelay/tests/utils_tests.py b/privaterelay/tests/utils_tests.py
index 073224f001..c0d28c52f0 100644
--- a/privaterelay/tests/utils_tests.py
+++ b/privaterelay/tests/utils_tests.py
@@ -17,27 +17,60 @@
 from ..utils import (
     flag_is_active_in_task,
     get_premium_country_lang,
+    get_premium_country_language_mapping,
 )
 
 
 class GetPremiumCountryLangTest(TestCase):
+    def setUp(self):
+        self.mapping = get_premium_country_language_mapping(None)
+
     def test_get_premium_country_lang(self):
-        cc, lang = get_premium_country_lang("en-au,")
+        cc, lang = get_premium_country_lang("en-au,", self.mapping)
         assert cc == "au"
         assert lang == "en"
 
-        cc, lang = get_premium_country_lang("en-us,")
+        cc, lang = get_premium_country_lang("en-us,", self.mapping)
         assert cc == "us"
         assert lang == "en"
 
-        cc, lang = get_premium_country_lang("de-be,")
+        cc, lang = get_premium_country_lang("de-be,", self.mapping)
         assert cc == "be"
         assert lang == "de"
 
-        cc, lang = get_premium_country_lang("de-be,", "at")
+        cc, lang = get_premium_country_lang("de-be,", self.mapping, "at")
         assert cc == "at"
         assert lang == "de"
 
+    def test_en_fallback(self) -> None:
+        cc, lang = get_premium_country_lang("en,", self.mapping)
+        assert cc == "us"
+        assert lang == "en"
+
+    def test_first_lang_fallback_two_parts(self) -> None:
+        accept_lang = "sgn-us,"  # American Sign Language
+        cc, lang = get_premium_country_lang(accept_lang, self.mapping)
+        assert cc == "us"
+        assert lang == "en"
+
+    def test_first_lang_fallback_three_parts(self) -> None:
+        accept_lang = "sgn-ch-de,"  # Swiss German Sign Language
+        cc, lang = get_premium_country_lang(accept_lang, self.mapping)
+        assert cc == "ch"
+        assert lang == "fr"
+
+    def test_eu_country_expansion_active(self) -> None:
+        mapping = get_premium_country_language_mapping(eu_country_expansion=True)
+        cc, lang = get_premium_country_lang("et-ee", mapping)
+        assert cc == "ee"
+        assert lang == "et"
+
+    def test_eu_country_expansion_inactive(self) -> None:
+        mapping = get_premium_country_language_mapping(eu_country_expansion=False)
+        cc, lang = get_premium_country_lang("et-ee", mapping)
+        assert cc == "ee"
+        assert lang == "en"
+
 
 #
 # flag_is_active_in_task tests
diff --git a/privaterelay/utils.py b/privaterelay/utils.py
index 0032b35afc..1f3cf42453 100644
--- a/privaterelay/utils.py
+++ b/privaterelay/utils.py
@@ -16,7 +16,7 @@
 
 
 def get_countries_info_from_request_and_mapping(request, mapping):
-    country_code = _get_cc_from_request(request)
+    country_code = _get_cc_from_request(request, mapping)
     countries = mapping.keys()
     available_in_country = country_code in countries
     return {
@@ -27,35 +27,48 @@ def get_countries_info_from_request_and_mapping(request, mapping):
     }
 
 
-def _get_cc_from_request(request):
+def _get_cc_from_request(request, premium_mapping):
     if "X-Client-Region" in request.headers:
         return request.headers["X-Client-Region"].lower()
     if "Accept-Language" in request.headers:
-        return get_premium_country_lang(request.headers["Accept-Language"])[0]
+        return get_premium_country_lang(
+            request.headers["Accept-Language"], premium_mapping
+        )[0]
     if settings.DEBUG:
         return "us"
     return "us"
 
 
-def get_premium_country_lang(accept_lang, cc=None):
+def get_premium_country_lang(accept_lang, mapping, cc=None):
     lang = accept_lang.split(",")[0]
     lang_parts = lang.split("-") if lang and "-" in lang else [lang]
     lang = lang_parts[0].lower()
     if cc is None:
-        cc = lang_parts[1] if len(lang_parts) == 2 else lang_parts[0]
+        cc = lang_parts[1] if len(lang_parts) >= 2 else lang_parts[0]
         cc = cc.lower()
         # if the language was just "en", default to US
         if cc == "en":
             cc = "us"
 
-    if cc in settings.PERIODICAL_PREMIUM_PLAN_COUNTRY_LANG_MAPPING.keys():
-        languages = settings.PERIODICAL_PREMIUM_PLAN_COUNTRY_LANG_MAPPING[cc]
+    if languages := mapping.get(cc):
         if lang in languages.keys():
             return cc, lang
         return cc, list(languages.keys())[0]
     return cc, "en"
 
 
+def get_premium_country_language_mapping(eu_country_expansion):
+    mapping = settings.PERIODICAL_PREMIUM_PLAN_COUNTRY_LANG_MAPPING.copy()
+    if eu_country_expansion:
+        mapping.update(settings.EU_EXPANSION_PREMIUM_PLAN_COUNTRY_LANG_MAPPING)
+    return mapping
+
+
+def get_premium_countries(eu_country_expansion):
+    mapping = get_premium_country_language_mapping(eu_country_expansion)
+    return set(mapping.keys())
+
+
 def enable_or_404(
     check_function: Callable[[], bool],
     message: str = "This conditional view is disabled.",


---- SAMPLE #92 ----
Generated commit : Fixing issue #233 by adding non-ascii characters to the list of licenses
Ground-truth     : Exclude unchanged translations from stats
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / 0.0141 / 0.1807
Diff:
diff --git a/pontoon/base/views.py b/pontoon/base/views.py
index f5a87ffd39..ae79c36438 100755
--- a/pontoon/base/views.py
+++ b/pontoon/base/views.py
@@ -19,7 +19,7 @@
 from django.contrib.auth.models import User
 from django.core.mail import send_mail
 from django.core.urlresolvers import reverse
-from django.db.models import Count
+from django.db.models import Count, F
 
 from django.http import (
     Http404,
@@ -313,7 +313,12 @@ def contributor(request, email, template='user.html'):
     except User.DoesNotExist:
         raise Http404
 
-    translations = Translation.objects.filter(user=user)
+    translations = (
+        Translation.objects.filter(user=user)
+        .exclude(string=F('entity__string'))
+        .exclude(string=F('entity__string_plural'))
+    )
+
     current = translations.exclude(entity__obsolete=True) \
         .extra({'day': "date(date)"})
 
@@ -357,7 +362,11 @@ def contributors(request, template='users.html'):
         .order_by('-translation_count')[:100]
 
     for user in users:
-        user.translations = Translation.objects.filter(user=user)
+        user.translations = (
+            Translation.objects.filter(user=user)
+            .exclude(string=F('entity__string'))
+            .exclude(string=F('entity__string_plural'))
+        )
         user.gravatar_url = get_gravatar_url(user.email, 44)
 
     data = {


---- SAMPLE #93 ----
Generated commit : [AIRFLOW-XXX] Fix flaky Google Cloud StorageDownload (#4275)
Ground-truth     : Improve example DAGs for Cloud Memorystore (#7855)
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0330 / 0.1333 / 0.0714 / 0.2073 / 0.1975
Diff:
diff --git a/airflow/providers/google/cloud/example_dags/example_cloud_memorystore.py b/airflow/providers/google/cloud/example_dags/example_cloud_memorystore.py
index 4c50e482955e3..55768ca9f9722 100644
--- a/airflow/providers/google/cloud/example_dags/example_cloud_memorystore.py
+++ b/airflow/providers/google/cloud/example_dags/example_cloud_memorystore.py
@@ -88,7 +88,11 @@
 
     # [START howto_operator_get_instance]
     get_instance = CloudMemorystoreGetInstanceOperator(
-        task_id="get-instance", location="europe-north1", instance=INSTANCE_NAME, project_id=GCP_PROJECT_ID
+        task_id="get-instance",
+        location="europe-north1",
+        instance=INSTANCE_NAME,
+        project_id=GCP_PROJECT_ID,
+        do_xcom_push=True,
     )
     # [END howto_operator_get_instance]
 
@@ -212,6 +216,7 @@
     create_instance_2 >> import_instance
     create_instance >> list_instances >> list_instances_result
     list_instances >> delete_instance
+    export_instance >> update_instance
     update_instance >> delete_instance
     get_instance >> set_acl_permission >> export_instance
     export_instance >> import_instance


---- SAMPLE #94 ----
Generated commit : [AIRFLOW-XXX] Fix docstring in gcs operator documentation (#5048)
Ground-truth     : [AIRFLOW-5384] Improve dst param info in FileToGCSOperator (#5985)

This commit add more info about dst parameter to indicate that the path
must include file name.
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0033 / 0.1143 / 0.0215 / 0.3392 / 0.3965
Diff:
diff --git a/airflow/operators/local_to_gcs.py b/airflow/operators/local_to_gcs.py
index 417a4beb22d4a..6170e104c51bb 100644
--- a/airflow/operators/local_to_gcs.py
+++ b/airflow/operators/local_to_gcs.py
@@ -33,7 +33,8 @@ class FileToGoogleCloudStorageOperator(BaseOperator):
 
     :param src: Path to the local file. (templated)
     :type src: str
-    :param dst: Destination path within the specified bucket. (templated)
+    :param dst: Destination path within the specified bucket, it must be the full file path
+        to destination object on GCS, including GCS object (ex. `path/to/file.txt`) (templated)
     :type dst: str
     :param bucket: The bucket to upload to. (templated)
     :type bucket: str


---- SAMPLE #95 ----
Generated commit : [AIRFLOW-XXX] Add wait_dagrun param to BaseOperator docstring (#3927)
Ground-truth     : Documented BaseOperator's wait_for_downstream attribute
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.1176 / 0.0000 / 0.1768 / 0.1982
Diff:
diff --git a/airflow/models.py b/airflow/models.py
index 11260a791d252..98b10d1f7fab4 100644
--- a/airflow/models.py
+++ b/airflow/models.py
@@ -813,7 +813,13 @@ class derived from this one results in the creation of a task object,
     :param depends_on_past: when set to true, task instances will run
         sequentially while relying on the previous task's schedule to
         succeed. The task instance for the start_date is allowed to run.
-    :type depends_on_past: boolean
+    :type depends_on_past: bool
+    :param wait_for_downstream: when set to true, the task instances
+        of task X will wait for the dependencies of the previous instance
+        of task X to finish before it moves on the to next schedule.
+        This is useful if the different instances of a task X alter
+        the same asset, and this asset is used by the dependencies of task X.
+    :type wait_for_downstream: bool
     :param dag: a reference to the dag the task is attached to (if any)
     :type dag: DAG
     """


---- SAMPLE #96 ----
Generated commit : Avoid triggering of incorrect credit in get_conn
Ground-truth     : [AIRFLOW-2826] Add GoogleCloudKMSHook (#3677)

Adds a hook enabling encryption and decryption through Google Cloud KMS.
This should also contribute to AIRFLOW-2062.
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / -0.1411 / 0.1631
Diff:
diff --git a/airflow/contrib/hooks/gcp_kms_hook.py b/airflow/contrib/hooks/gcp_kms_hook.py
new file mode 100644
index 0000000000000..6f2b3aedffdbf
--- /dev/null
+++ b/airflow/contrib/hooks/gcp_kms_hook.py
@@ -0,0 +1,108 @@
+# -*- coding: utf-8 -*-
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+#
+
+import base64
+
+from airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook
+
+from apiclient.discovery import build
+
+
+def _b64encode(s):
+    """ Base 64 encodes a bytes object to a string """
+    return base64.b64encode(s).decode('ascii')
+
+
+def _b64decode(s):
+    """ Base 64 decodes a string to bytes. """
+    return base64.b64decode(s.encode('utf-8'))
+
+
+class GoogleCloudKMSHook(GoogleCloudBaseHook):
+    """
+    Interact with Google Cloud KMS. This hook uses the Google Cloud Platform
+    connection.
+    """
+
+    def __init__(self, gcp_conn_id='google_cloud_default', delegate_to=None):
+        super(GoogleCloudKMSHook, self).__init__(gcp_conn_id, delegate_to=delegate_to)
+
+    def get_conn(self):
+        """
+        Returns a KMS service object.
+
+        :rtype: apiclient.discovery.Resource
+        """
+        http_authorized = self._authorize()
+        return build(
+            'cloudkms', 'v1', http=http_authorized, cache_discovery=False)
+
+    def encrypt(self, key_name, plaintext, authenticated_data=None):
+        """
+        Encrypts a plaintext message using Google Cloud KMS.
+
+        :param key_name: The Resource Name for the key (or key version)
+                         to be used for encyption. Of the form
+                         ``projects/*/locations/*/keyRings/*/cryptoKeys/**``
+        :type key_name: str
+        :param plaintext: The message to be encrypted.
+        :type plaintext: bytes
+        :param authenticated_data: Optional additional authenticated data that
+                                   must also be provided to decrypt the message.
+        :type authenticated_data: bytes
+        :return: The base 64 encoded ciphertext of the original message.
+        :rtype: str
+        """
+        keys = self.get_conn().projects().locations().keyRings().cryptoKeys()
+        body = {'plaintext': _b64encode(plaintext)}
+        if authenticated_data:
+            body['additionalAuthenticatedData'] = _b64encode(authenticated_data)
+
+        request = keys.encrypt(name=key_name, body=body)
+        response = request.execute()
+
+        ciphertext = response['ciphertext']
+        return ciphertext
+
+    def decrypt(self, key_name, ciphertext, authenticated_data=None):
+        """
+        Decrypts a ciphertext message using Google Cloud KMS.
+
+        :param key_name: The Resource Name for the key to be used for decyption.
+                         Of the form ``projects/*/locations/*/keyRings/*/cryptoKeys/**``
+        :type key_name: str
+        :param ciphertext: The message to be decrypted.
+        :type ciphertext: str
+        :param authenticated_data: Any additional authenticated data that was
+                                   provided when encrypting the message.
+        :type authenticated_data: bytes
+        :return: The original message.
+        :rtype: bytes
+        """
+        keys = self.get_conn().projects().locations().keyRings().cryptoKeys()
+        body = {'ciphertext': ciphertext}
+        if authenticated_data:
+            body['additionalAuthenticatedData'] = _b64encode(authenticated_data)
+
+        request = keys.decrypt(name=key_name, body=body)
+        response = request.execute()
+
+        plaintext = _b64decode(response['plaintext'])
+        return plaintext
diff --git a/tests/contrib/hooks/test_gcp_kms_hook.py b/tests/contrib/hooks/test_gcp_kms_hook.py
new file mode 100644
index 0000000000000..eabf20e5642b2
--- /dev/null
+++ b/tests/contrib/hooks/test_gcp_kms_hook.py
@@ -0,0 +1,160 @@
+# -*- coding: utf-8 -*-
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+#
+
+from __future__ import unicode_literals
+
+import unittest
+from base64 import b64encode
+
+from airflow.contrib.hooks.gcp_kms_hook import GoogleCloudKMSHook
+
+try:
+    from unittest import mock
+except ImportError:
+    try:
+        import mock
+    except ImportError:
+        mock = None
+
+BASE_STRING = 'airflow.contrib.hooks.gcp_api_base_hook.{}'
+KMS_STRING = 'airflow.contrib.hooks.gcp_kms_hook.{}'
+
+
+TEST_PROJECT = 'test-project'
+TEST_LOCATION = 'global'
+TEST_KEY_RING = 'test-key-ring'
+TEST_KEY = 'test-key'
+TEST_KEY_ID = 'projects/{}/locations/{}/keyRings/{}/cryptoKeys/{}'.format(
+    TEST_PROJECT, TEST_LOCATION, TEST_KEY_RING, TEST_KEY)
+
+
+def mock_init(self, gcp_conn_id, delegate_to=None):
+    pass
+
+
+class GoogleCloudKMSHookTest(unittest.TestCase):
+    def setUp(self):
+        with mock.patch(BASE_STRING.format('GoogleCloudBaseHook.__init__'),
+                        new=mock_init):
+            self.kms_hook = GoogleCloudKMSHook(gcp_conn_id='test')
+
+    @mock.patch(KMS_STRING.format('GoogleCloudKMSHook.get_conn'))
+    def test_encrypt(self, mock_service):
+        plaintext = b'Test plaintext'
+        ciphertext = 'Test ciphertext'
+        plaintext_b64 = b64encode(plaintext).decode('ascii')
+        body = {'plaintext': plaintext_b64}
+        response = {'ciphertext': ciphertext}
+
+        encrypt_method = (mock_service.return_value
+                          .projects.return_value
+                          .locations.return_value
+                          .keyRings.return_value
+                          .cryptoKeys.return_value
+                          .encrypt)
+        execute_method = encrypt_method.return_value.execute
+        execute_method.return_value = response
+
+        ret_val = self.kms_hook.encrypt(TEST_KEY_ID, plaintext)
+        encrypt_method.assert_called_with(name=TEST_KEY_ID,
+                                          body=body)
+        execute_method.assert_called_with()
+        self.assertEqual(ciphertext, ret_val)
+
+    @mock.patch(KMS_STRING.format('GoogleCloudKMSHook.get_conn'))
+    def test_encrypt_authdata(self, mock_service):
+        plaintext = b'Test plaintext'
+        auth_data = b'Test authdata'
+        ciphertext = 'Test ciphertext'
+        plaintext_b64 = b64encode(plaintext).decode('ascii')
+        auth_data_b64 = b64encode(auth_data).decode('ascii')
+        body = {
+            'plaintext': plaintext_b64,
+            'additionalAuthenticatedData': auth_data_b64
+        }
+        response = {'ciphertext': ciphertext}
+
+        encrypt_method = (mock_service.return_value
+                          .projects.return_value
+                          .locations.return_value
+                          .keyRings.return_value
+                          .cryptoKeys.return_value
+                          .encrypt)
+        execute_method = encrypt_method.return_value.execute
+        execute_method.return_value = response
+
+        ret_val = self.kms_hook.encrypt(TEST_KEY_ID, plaintext,
+                                        authenticated_data=auth_data)
+        encrypt_method.assert_called_with(name=TEST_KEY_ID,
+                                          body=body)
+        execute_method.assert_called_with()
+        self.assertEqual(ciphertext, ret_val)
+
+    @mock.patch(KMS_STRING.format('GoogleCloudKMSHook.get_conn'))
+    def test_decrypt(self, mock_service):
+        plaintext = b'Test plaintext'
+        ciphertext = 'Test ciphertext'
+        plaintext_b64 = b64encode(plaintext).decode('ascii')
+        body = {'ciphertext': ciphertext}
+        response = {'plaintext': plaintext_b64}
+
+        decrypt_method = (mock_service.return_value
+                          .projects.return_value
+                          .locations.return_value
+                          .keyRings.return_value
+                          .cryptoKeys.return_value
+                          .decrypt)
+        execute_method = decrypt_method.return_value.execute
+        execute_method.return_value = response
+
+        ret_val = self.kms_hook.decrypt(TEST_KEY_ID, ciphertext)
+        decrypt_method.assert_called_with(name=TEST_KEY_ID,
+                                          body=body)
+        execute_method.assert_called_with()
+        self.assertEqual(plaintext, ret_val)
+
+    @mock.patch(KMS_STRING.format('GoogleCloudKMSHook.get_conn'))
+    def test_decrypt_authdata(self, mock_service):
+        plaintext = b'Test plaintext'
+        auth_data = b'Test authdata'
+        ciphertext = 'Test ciphertext'
+        plaintext_b64 = b64encode(plaintext).decode('ascii')
+        auth_data_b64 = b64encode(auth_data).decode('ascii')
+        body = {
+            'ciphertext': ciphertext,
+            'additionalAuthenticatedData': auth_data_b64
+        }
+        response = {'plaintext': plaintext_b64}
+
+        decrypt_method = (mock_service.return_value
+                          .projects.return_value
+                          .locations.return_value
+                          .keyRings.return_value
+                          .cryptoKeys.return_value
+                          .decrypt)
+        execute_method = decrypt_method.return_value.execute
+        execute_method.return_value = response
+
+        ret_val = self.kms_hook.decrypt(TEST_KEY_ID, ciphertext,
+                                        authenticated_data=auth_data)
+        decrypt_method.assert_called_with(name=TEST_KEY_ID,
+                                          body=body)
+        execute_method.assert_called_with()
+        self.assertEqual(plaintext, ret_val)


---- SAMPLE #97 ----
Generated commit : Fix code scanning alert
Ground-truth     : MPP-3257: activate the user's language to translate subject
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0000 / 0.0000 / 0.0000 / 0.0367 / 0.1844
Diff:
diff --git a/emails/management/commands/send_welcome_emails.py b/emails/management/commands/send_welcome_emails.py
index 1227a410cc..c82f6a49f9 100644
--- a/emails/management/commands/send_welcome_emails.py
+++ b/emails/management/commands/send_welcome_emails.py
@@ -7,6 +7,7 @@
 from django.core.management.base import BaseCommand
 
 from allauth.socialaccount.models import SocialAccount
+import django_ftl
 
 from emails.apps import EmailsConfig
 from emails.models import Profile
@@ -43,6 +44,8 @@ def send_welcome_email(profile: Profile, **kwargs):
     ses_client = app_config.ses_client
     assert ses_client
     assert settings.RELAY_FROM_ADDRESS
+    with django_ftl.override(profile.language):
+        translated_subject = ftl_bundle.format("first-time-user-email-welcome")
     try:
         ses_client.send_email(
             Destination={
@@ -50,9 +53,7 @@ def send_welcome_email(profile: Profile, **kwargs):
             },
             Source=settings.RELAY_FROM_ADDRESS,
             Message={
-                "Subject": _ses_message_props(
-                    ftl_bundle.format("first-time-user-email-welcome")
-                ),
+                "Subject": _ses_message_props(translated_subject),
                 "Body": {
                     "Html": _ses_message_props(get_welcome_email(user, "html")),
                     "Text": _ses_message_props(get_welcome_email(user, "txt")),
diff --git a/emails/tests/mgmt_send_welcome_emails.py b/emails/tests/mgmt_send_welcome_emails.py
index 8b2caad6b5..01af0beb99 100644
--- a/emails/tests/mgmt_send_welcome_emails.py
+++ b/emails/tests/mgmt_send_welcome_emails.py
@@ -7,7 +7,7 @@
 from django.core.management import call_command
 
 from allauth.socialaccount.models import SocialAccount
-from django_ftl import activate
+import django_ftl
 
 from emails.models import Profile
 from emails.tests.models_tests import make_free_test_user
@@ -38,14 +38,18 @@ def test_no_profiles_need_welcome_email(caplog: pytest.LogCaptureFixture):
 def test_no_locale_defaults_to_en(
     mock_ses_client: MagicMock, caplog: pytest.LogCaptureFixture
 ):
+    ftl_bundle.reload()
     user = _make_user_who_needs_welcome_email_with_locale("")
 
     call_command(COMMAND_NAME)
     _assert_caplog_for_1_email_to_user(user, caplog)
 
-    to_addresses, source, body_html = _get_send_email_args(mock_ses_client)
+    to_addresses, source, subject, body_html = _get_send_email_args(mock_ses_client)
     assert to_addresses == [user.email]
     assert source == settings.RELAY_FROM_ADDRESS
+    with django_ftl.override("en"):
+        expected_subject = ftl_bundle.format("first-time-user-email-welcome")
+    assert subject == expected_subject
     assert 'lang="en"' in body_html
 
 
@@ -56,15 +60,18 @@ def test_send_welcome_emails(
 ):
     ftl_bundle.reload()
     user = _make_user_who_needs_welcome_email_with_locale(locale)
-    activate(user.profile.language)
 
     call_command(COMMAND_NAME)
     _assert_caplog_for_1_email_to_user(user, caplog)
 
-    to_addresses, source, body_html = _get_send_email_args(mock_ses_client)
+    to_addresses, source, subject, body_html = _get_send_email_args(mock_ses_client)
     assert to_addresses == [user.email]
     assert source == settings.RELAY_FROM_ADDRESS
-    assert ftl_bundle.format("first-time-user-email-hero-cta") in body_html
+    with django_ftl.override(user.profile.language):
+        expected_subject = ftl_bundle.format("first-time-user-email-welcome")
+        expected_cta = ftl_bundle.format("first-time-user-email-hero-cta")
+    assert subject == expected_subject
+    assert expected_cta in body_html
 
 
 def _make_user_who_needs_welcome_email_with_locale(locale: str = "") -> User:
@@ -96,5 +103,6 @@ def _get_send_email_args(mock_ses_client: MagicMock) -> Tuple:
     call_args = mock_ses_client.send_email.call_args[1]
     to_addresses = call_args["Destination"]["ToAddresses"]
     source = call_args["Source"]
+    subject = call_args["Message"]["Subject"]["Data"]
     body_html = call_args["Message"]["Body"]["Html"]["Data"]
-    return to_addresses, source, body_html
+    return to_addresses, source, subject, body_html


---- SAMPLE #98 ----
Generated commit : Add support for AWS API to get the latest PR (#3271)
Ground-truth     : Add parameter to pass role ARN to `GlueJobOperator ` (#33408)
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0224 / 0.2000 / 0.0990 / 0.1039 / 0.1981
Diff:
diff --git a/airflow/providers/amazon/aws/hooks/glue.py b/airflow/providers/amazon/aws/hooks/glue.py
index 1d4c87d6767b1..baf6780e07802 100644
--- a/airflow/providers/amazon/aws/hooks/glue.py
+++ b/airflow/providers/amazon/aws/hooks/glue.py
@@ -44,7 +44,8 @@ class GlueJobHook(AwsBaseHook):
     :param retry_limit: Maximum number of times to retry this job if it fails
     :param num_of_dpus: Number of AWS Glue DPUs to allocate to this Job
     :param region_name: aws region name (example: us-east-1)
-    :param iam_role_name: AWS IAM Role for Glue Job Execution
+    :param iam_role_name: AWS IAM Role for Glue Job Execution. If set `iam_role_arn` must equal None.
+    :param iam_role_arn: AWS IAM Role ARN for Glue Job Execution, If set `iam_role_name` must equal None.
     :param create_job_kwargs: Extra arguments for Glue Job Creation
     :param update_config: Update job configuration on Glue (default: False)
 
@@ -72,6 +73,7 @@ def __init__(
         retry_limit: int = 0,
         num_of_dpus: int | float | None = None,
         iam_role_name: str | None = None,
+        iam_role_arn: str | None = None,
         create_job_kwargs: dict | None = None,
         update_config: bool = False,
         job_poll_interval: int | float = 6,
@@ -85,6 +87,7 @@ def __init__(
         self.retry_limit = retry_limit
         self.s3_bucket = s3_bucket
         self.role_name = iam_role_name
+        self.role_arn = iam_role_arn
         self.s3_glue_logs = "logs/glue-logs/"
         self.create_job_kwargs = create_job_kwargs or {}
         self.update_config = update_config
@@ -93,6 +96,8 @@ def __init__(
         worker_type_exists = "WorkerType" in self.create_job_kwargs
         num_workers_exists = "NumberOfWorkers" in self.create_job_kwargs
 
+        if self.role_arn and self.role_name:
+            raise ValueError("Cannot set iam_role_arn and iam_role_name simultaneously")
         if worker_type_exists and num_workers_exists:
             if num_of_dpus is not None:
                 raise ValueError("Cannot specify num_of_dpus with custom WorkerType")
@@ -114,12 +119,16 @@ def create_glue_job_config(self) -> dict:
             "ScriptLocation": self.script_location,
         }
         command = self.create_job_kwargs.pop("Command", default_command)
-        execution_role = self.get_iam_execution_role()
+        if not self.role_arn:
+            execution_role = self.get_iam_execution_role()
+            role_arn = execution_role["Role"]["Arn"]
+        else:
+            role_arn = self.role_arn
 
         config = {
             "Name": self.job_name,
             "Description": self.desc,
-            "Role": execution_role["Role"]["Arn"],
+            "Role": role_arn,
             "ExecutionProperty": {"MaxConcurrentRuns": self.concurrent_run_limit},
             "Command": command,
             "MaxRetries": self.retry_limit,
@@ -144,7 +153,6 @@ def list_jobs(self) -> list:
         return self.conn.get_jobs()
 
     def get_iam_execution_role(self) -> dict:
-        """Get IAM Role for job execution."""
         try:
             iam_client = self.get_session(region_name=self.region_name).client(
                 "iam", endpoint_url=self.conn_config.endpoint_url, config=self.config, verify=self.verify
diff --git a/airflow/providers/amazon/aws/operators/glue.py b/airflow/providers/amazon/aws/operators/glue.py
index 265d057de51ae..d47d1c25de5ec 100644
--- a/airflow/providers/amazon/aws/operators/glue.py
+++ b/airflow/providers/amazon/aws/operators/glue.py
@@ -53,7 +53,8 @@ class GlueJobOperator(BaseOperator):
     :param num_of_dpus: Number of AWS Glue DPUs to allocate to this Job.
     :param region_name: aws region name (example: us-east-1)
     :param s3_bucket: S3 bucket where logs and local etl script will be uploaded
-    :param iam_role_name: AWS IAM Role for Glue Job Execution
+    :param iam_role_name: AWS IAM Role for Glue Job Execution. If set `iam_role_arn` must equal None.
+    :param iam_role_arn: AWS IAM ARN for Glue Job Execution. If set `iam_role_name` must equal None.
     :param create_job_kwargs: Extra arguments for Glue Job Creation
     :param run_job_kwargs: Extra arguments for Glue Job Run
     :param wait_for_completion: Whether to wait for job run completion. (default: True)
@@ -72,6 +73,7 @@ class GlueJobOperator(BaseOperator):
         "create_job_kwargs",
         "s3_bucket",
         "iam_role_name",
+        "iam_role_arn",
     )
     template_ext: Sequence[str] = ()
     template_fields_renderers = {
@@ -96,6 +98,7 @@ def __init__(
         region_name: str | None = None,
         s3_bucket: str | None = None,
         iam_role_name: str | None = None,
+        iam_role_arn: str | None = None,
         create_job_kwargs: dict | None = None,
         run_job_kwargs: dict | None = None,
         wait_for_completion: bool = True,
@@ -118,6 +121,7 @@ def __init__(
         self.region_name = region_name
         self.s3_bucket = s3_bucket
         self.iam_role_name = iam_role_name
+        self.iam_role_arn = iam_role_arn
         self.s3_protocol = "s3://"
         self.s3_artifacts_prefix = "artifacts/glue-scripts/"
         self.create_job_kwargs = create_job_kwargs
@@ -154,6 +158,7 @@ def glue_job_hook(self) -> GlueJobHook:
             region_name=self.region_name,
             s3_bucket=self.s3_bucket,
             iam_role_name=self.iam_role_name,
+            iam_role_arn=self.iam_role_arn,
             create_job_kwargs=self.create_job_kwargs,
             update_config=self.update_config,
             job_poll_interval=self.job_poll_interval,
diff --git a/tests/providers/amazon/aws/hooks/test_glue.py b/tests/providers/amazon/aws/hooks/test_glue.py
index c41598f3d9aa6..1fae16e339558 100644
--- a/tests/providers/amazon/aws/hooks/test_glue.py
+++ b/tests/providers/amazon/aws/hooks/test_glue.py
@@ -66,6 +66,22 @@ def test_get_iam_execution_role(self, role_path):
         assert "Arn" in iam_role["Role"]
         assert iam_role["Role"]["Arn"] == f"arn:aws:iam::123456789012:role{role_path}{expected_role}"
 
+    @mock.patch.object(GlueJobHook, "get_iam_execution_role")
+    @mock.patch.object(GlueJobHook, "conn")
+    def test_init_iam_role_value_error(self, mock_conn, mock_get_iam_execution_role):
+        mock_get_iam_execution_role.return_value = mock.MagicMock(
+            Role={"RoleName": "my_test_role_name", "RoleArn": "my_test_role"}
+        )
+
+        with pytest.raises(ValueError, match="Cannot set iam_role_arn and iam_role_name simultaneously"):
+            GlueJobHook(
+                job_name="aws_test_glue_job",
+                desc="This is test case job from Airflow",
+                s3_bucket="some-bucket",
+                iam_role_name="my_test_role_name",
+                iam_role_arn="my_test_role",
+            )
+
     @mock.patch.object(AwsBaseHook, "conn")
     def test_has_job_exists(self, mock_conn):
         job_name = "aws_test_glue_job"
@@ -90,6 +106,56 @@ class JobNotFoundException(Exception):
         assert result is False
         mock_conn.get_job.assert_called_once_with(JobName=job_name)
 
+    @mock.patch.object(GlueJobHook, "get_iam_execution_role")
+    @mock.patch.object(AwsBaseHook, "conn")
+    def test_role_arn_has_job_exists(self, mock_conn, mock_get_iam_execution_role):
+        """
+        Calls 'create_or_update_glue_job' with no existing job.
+        Should create a new job.
+        """
+
+        class JobNotFoundException(Exception):
+            pass
+
+        expected_job_name = "aws_test_glue_job"
+        job_description = "This is test case job from Airflow"
+        role_name = "my_test_role"
+        role_name_arn = "test_role"
+        some_s3_bucket = "bucket"
+
+        mock_conn.exceptions.EntityNotFoundException = JobNotFoundException
+        mock_conn.get_job.side_effect = JobNotFoundException()
+        mock_get_iam_execution_role.return_value = {"Role": {"RoleName": role_name, "Arn": role_name_arn}}
+
+        hook = GlueJobHook(
+            s3_bucket=some_s3_bucket,
+            job_name=expected_job_name,
+            desc=job_description,
+            concurrent_run_limit=2,
+            retry_limit=3,
+            num_of_dpus=5,
+            iam_role_arn=role_name_arn,
+            create_job_kwargs={"Command": {}},
+            region_name=self.some_aws_region,
+            update_config=True,
+        )
+
+        result = hook.create_or_update_glue_job()
+
+        mock_conn.get_job.assert_called_once_with(JobName=expected_job_name)
+        mock_conn.create_job.assert_called_once_with(
+            Command={},
+            Description=job_description,
+            ExecutionProperty={"MaxConcurrentRuns": 2},
+            LogUri=f"s3://{some_s3_bucket}/logs/glue-logs/{expected_job_name}",
+            MaxCapacity=5,
+            MaxRetries=3,
+            Name=expected_job_name,
+            Role=role_name_arn,
+        )
+        mock_conn.update_job.assert_not_called()
+        assert result == expected_job_name
+
     @mock.patch.object(GlueJobHook, "get_iam_execution_role")
     @mock.patch.object(GlueJobHook, "conn")
     def test_create_or_update_glue_job_create_new_job(self, mock_conn, mock_get_iam_execution_role):
diff --git a/tests/providers/amazon/aws/operators/test_glue.py b/tests/providers/amazon/aws/operators/test_glue.py
index 9eed48e47adc9..dc298563ae65c 100644
--- a/tests/providers/amazon/aws/operators/test_glue.py
+++ b/tests/providers/amazon/aws/operators/test_glue.py
@@ -48,6 +48,7 @@ def test_render_template(self, create_task_instance_of_operator):
             script_args="{{ dag.dag_id }}",
             create_job_kwargs="{{ dag.dag_id }}",
             iam_role_name="{{ dag.dag_id }}",
+            iam_role_arn="{{ dag.dag_id }}",
             s3_bucket="{{ dag.dag_id }}",
             job_name="{{ dag.dag_id }}",
         )
@@ -57,6 +58,7 @@ def test_render_template(self, create_task_instance_of_operator):
         assert DAG_ID == rendered_template.script_args
         assert DAG_ID == rendered_template.create_job_kwargs
         assert DAG_ID == rendered_template.iam_role_name
+        assert DAG_ID == rendered_template.iam_role_arn
         assert DAG_ID == rendered_template.s3_bucket
         assert DAG_ID == rendered_template.job_name
 
@@ -99,6 +101,27 @@ def test_execute_without_failure(
         mock_print_job_logs.assert_not_called()
         assert glue.job_name == JOB_NAME
 
+    @mock.patch.object(GlueJobHook, "initialize_job")
+    @mock.patch.object(GlueJobHook, "get_conn")
+    def test_role_arn_execute_deferrable(self, _, mock_initialize_job):
+        glue = GlueJobOperator(
+            task_id=TASK_ID,
+            job_name=JOB_NAME,
+            script_location="s3://folder/file",
+            aws_conn_id="aws_default",
+            region_name="us-west-2",
+            s3_bucket="some_bucket",
+            iam_role_arn="test_role",
+            deferrable=True,
+        )
+        mock_initialize_job.return_value = {"JobRunState": "RUNNING", "JobRunId": JOB_RUN_ID}
+
+        with pytest.raises(TaskDeferred) as defer:
+            glue.execute(mock.MagicMock())
+
+        assert defer.value.trigger.job_name == JOB_NAME
+        assert defer.value.trigger.run_id == JOB_RUN_ID
+
     @mock.patch.object(GlueJobHook, "initialize_job")
     @mock.patch.object(GlueJobHook, "get_conn")
     def test_execute_deferrable(self, _, mock_initialize_job):


---- SAMPLE #99 ----
Generated commit : Fix bug in the documentation for `env: 127` (#22)
Ground-truth     : Fix typo in docstring
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0285 / 0.3077 / 0.2222 / 0.2539 / 0.2019
Diff:
diff --git a/src/collectors/memcached/memcached.py b/src/collectors/memcached/memcached.py
index 9199f3a17..f2f7e8fe0 100644
--- a/src/collectors/memcached/memcached.py
+++ b/src/collectors/memcached/memcached.py
@@ -17,7 +17,7 @@
 
     [[[hosts]]]
 
-    [[[[app-1]]]
+    [[[[app-1]]]]
     host = localhost
     port = 11211
 ```


---- SAMPLE #100 ----
Generated commit : Remove unused `os.environ` (#3536)
Ground-truth     : Remove provide_session from Mypy plugin usage (#31498)
Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : 0.0380 / 0.1538 / 0.0746 / 0.2947 / 0.1888
Diff:
diff --git a/dev/mypy/plugin/decorators.py b/dev/mypy/plugin/decorators.py
index c05fd4560881b..1957b59996576 100644
--- a/dev/mypy/plugin/decorators.py
+++ b/dev/mypy/plugin/decorators.py
@@ -25,7 +25,6 @@
 from mypy.types import CallableType, NoneType, UnionType
 
 TYPED_DECORATORS = {
-    "airflow.utils.session.provide_session": [],
     "airflow.providers.google.cloud.hooks.dataflow._fallback_to_project_id_from_variables": ["project_id"],
     "fallback_to_default_project_id of GoogleBaseHook": ["project_id"],
     "provide_gcp_credential_file of GoogleBaseHook": [],


