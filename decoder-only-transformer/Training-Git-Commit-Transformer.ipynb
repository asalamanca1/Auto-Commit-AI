{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Initialize Dataset\n",
        "\n",
        "Make sure your CSV dataset is named `cleaned_python_commit_dataset.csv` and is placed inside a folder called `data/`. This cell reads the file and prints the number of rows to confirm it's loaded correctly."
      ],
      "metadata": {
        "id": "07pCvcBkBWf8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqnVON90A_p1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv('data/cleaned_python_commit_dataset.csv')\n",
        "\n",
        "# Get the number of rows\n",
        "num_rows = df.shape[0]\n",
        "\n",
        "print(\"Number of rows:\", num_rows)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Train BPE Tokenizer on Dataset\n",
        "\n",
        "Train a Byte-Pair Encoding (BPE) tokenizer using the HuggingFace `tokenizers` library. The tokenizer is trained on a dataset of Git diffs and commit messages extracted from a CSV file.\n",
        "\n",
        "### Inputs\n",
        "- **`csv_file`**: Path to the CSV file containing Git diffs and commit messages.\n",
        "- **`output_tokenizer_file`**: Path where the trained tokenizer JSON will be saved.\n",
        "- **`vocab_size`** *(default: 48000)*: Target vocabulary size.\n",
        "- **`special_tokens`** *(default: [`\"<pad>\"`, `\"<endOfDiff>\"`, `\"<endOfCommitMessage>\"`])*: Tokens to include in the vocabulary for special processing.\n",
        "- **`diff_column`** *(default: 'diff')*: Column name containing the Git diffs.\n",
        "- **`commit_msg_column`** *(default: 'commit_message')*: Column name containing the commit messages.\n",
        "\n",
        "### Processing Steps\n",
        "1. Reads the CSV and ensures the required columns exist.\n",
        "2. Combines each diff and commit message into a single training string, separated by a newline.\n",
        "3. Saves these training strings to a temporary file.\n",
        "4. Initializes a BPE tokenizer with byte-level pre-tokenization and decoding (compatible with GPT-2 style models).\n",
        "5. Trains the tokenizer on the prepared training texts.\n",
        "6. Saves the tokenizer as a JSON file for later use.\n",
        "7. Deletes the temporary training text file.\n",
        "\n",
        "### Output\n",
        "- A JSON tokenizer file (e.g., `custom_bpe_tokenizer.json`) that can be used with HuggingFace's `PreTrainedTokenizerFast`.\n",
        "\n",
        "This tokenizer will be used to encode the model inputs and outputs, ensuring that both Git diffs and commit messages are tokenized in a consistent and compact format.\n"
      ],
      "metadata": {
        "id": "G9Mmb6mCBr8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers\n",
        "\n",
        "def train_tokenizer(\n",
        "    csv_file: str,\n",
        "    output_tokenizer_file: str,\n",
        "    vocab_size: int = 48000,\n",
        "    special_tokens: list = None,\n",
        "    diff_column: str = 'diff',\n",
        "    commit_msg_column: str = 'commit_message'\n",
        "):\n",
        "\n",
        "    # Train a custom BPE tokenizer using data from a CSV file.\n",
        "    # Args:\n",
        "    #     csv_file (str): Path to the CSV file containing the dataset.\n",
        "    #     output_tokenizer_file (str): Path to save the trained tokenizer JSON.\n",
        "    #     vocab_size (int): The desired vocabulary size.\n",
        "    #     special_tokens (list): List of special tokens to add.\n",
        "    #     diff_column (str): Name of the CSV column containing git diffs.\n",
        "    #     commit_msg_column (str): Name of the CSV column containing commit messages.\n",
        "\n",
        "    # Default special tokens\n",
        "    if special_tokens is None:\n",
        "        special_tokens = [\"<pad>\", \"<endOfDiff>\", \"<endOfCommitMessage>\"]\n",
        "\n",
        "    # Load the CSV dataset\n",
        "    df = pd.read_csv(csv_file)\n",
        "    if diff_column not in df.columns or commit_msg_column not in df.columns:\n",
        "        raise ValueError(f\"CSV file must have columns '{diff_column}' and '{commit_msg_column}'.\")\n",
        "\n",
        "    # Combine the diff and commit message columns.\n",
        "    # Here we add a newline between the diff and the commit message.\n",
        "    training_texts = (df[diff_column].astype(str) + \"\\n\" + df[commit_msg_column].astype(str)).tolist()\n",
        "\n",
        "    # Save training texts to a temporary file (one text per line)\n",
        "    training_file = \"training_texts.txt\"\n",
        "    with open(training_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        for text in training_texts:\n",
        "            f.write(text + \"\\n\")\n",
        "    print(f\"Training texts saved to {training_file}\")\n",
        "\n",
        "    # Initialize the tokenizer with a BPE model\n",
        "    tokenizer = Tokenizer(models.BPE())\n",
        "\n",
        "    # Set pre-tokenizer and decoder.\n",
        "    # Using ByteLevel pre-tokenization and decoding mimics GPT-2's behavior.\n",
        "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
        "    tokenizer.decoder = decoders.ByteLevel()\n",
        "\n",
        "    # Configure the trainer with the vocabulary size and special tokens\n",
        "    trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=special_tokens)\n",
        "\n",
        "    # Train the tokenizer on the training file.\n",
        "    tokenizer.train([training_file], trainer=trainer)\n",
        "    print(\"Tokenizer training complete.\")\n",
        "\n",
        "    # Save the trained tokenizer to a JSON file.\n",
        "    tokenizer.save(output_tokenizer_file)\n",
        "    print(f\"Tokenizer saved to {output_tokenizer_file}\")\n",
        "\n",
        "    # Remove the temporary training file.\n",
        "    os.remove(training_file)\n",
        "    print(f\"Temporary training file {training_file} removed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Path to CSV file containing 'diff' and 'commit_message' columns.\n",
        "    csv_file = \"data/cleaned_python_commit_dataset.csv\"\n",
        "    # The filename to store your trained tokenizer.\n",
        "    output_tokenizer_file = \"custom_bpe_tokenizer.json\"\n",
        "    # Train the tokenizer with a vocabulary size of 48,000\n",
        "    train_tokenizer(csv_file, output_tokenizer_file, vocab_size=48000)\n"
      ],
      "metadata": {
        "id": "u_NO3vyHB4uQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Load the Trained Tokenizer\n",
        "\n",
        "Load the custom BPE tokenizer that was trained and saved in the previous step (`custom_bpe_tokenizer.json`) using Hugging Face’s `PreTrainedTokenizerFast`.\n",
        "\n",
        "### Special Tokens Added:\n",
        "- `<pad>` — used for padding sequences during batching\n",
        "- `<endOfCommitMessage>` — tells the model where the commit message ends\n",
        "- `<endOfDiff>` — separates the git diff from the commit message"
      ],
      "metadata": {
        "id": "ddTZ3e-4CMwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "custom_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"custom_bpe_tokenizer.json\")\n",
        "custom_tokenizer.add_special_tokens({\n",
        "    \"pad_token\": \"<pad>\",\n",
        "    \"eos_token\": \"<endOfCommitMessage>\"\n",
        "})\n",
        "custom_tokenizer.add_tokens([\"<endOfDiff>\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMyy4qjzCPU1",
        "outputId": "113a554e-0561-4561-d153-d02cb428416b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Initialize Function to Plot Loss for Training\n",
        "\n",
        "Helper function `plot_and_save_loss()` that plots the training and validation loss over epochs and saves the result as an image.\n",
        "\n",
        "### What it does:\n",
        "- Takes in two lists: `train_losses` and `val_losses`, one value per epoch.\n",
        "- Plots both loss curves on the same graph.\n",
        "- Labels axes and adds a legend for clarity.\n",
        "- Saves the figure as `loss_curve.png` (or a custom filename if specified).\n",
        "\n",
        "Use this after training to visualize how well your model is learning and spot signs of overfitting or underfitting."
      ],
      "metadata": {
        "id": "t9pjQEp7CVOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_and_save_loss(train_losses, val_losses, filename=\"loss_curve.png\"):\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, train_losses, label=\"Training Loss\")\n",
        "    plt.plot(epochs, val_losses,   label=\"Validation Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Loss Curve\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "    print(f\"Saved loss curve to {filename}\")\n"
      ],
      "metadata": {
        "id": "-SNOatf6CZA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Training the Decoder-only Transformer Model on Git Diffs + Commit Messages\n",
        "\n",
        "This code builds and trains a decoder-only GPT-2 model from scratch to generate commit messages based on Git diffs. It uses the custom tokenizer trained earlier, a 20-layer Transformer architecture, and tracks model behavior over time.\n",
        "\n",
        "1. **Defines a custom PyTorch dataset (`GitDiffDataset`)** to prepare each sample in the form: `<git_diff> <endOfDiff> <commit_message> <endOfCommitMessage>`\n",
        "2. **Pads each batch** using a custom `collate_fn` to align sequence lengths while respecting the model's max context window.\n",
        "3. **Configures a GPT-2 model** from scratch using the following hyperparameters:\n",
        "\n",
        "- **`vocab_size=48000`**  \n",
        "  Matches the number of tokens in our custom tokenizer. This sets the size of the model's input/output vocabulary.\n",
        "\n",
        "- **`n_positions=1024`**  \n",
        "  Maximum number of tokens the model can see in a single forward pass. Both the input (diff + commit message) and the output must fit within this limit.\n",
        "\n",
        "- **`n_ctx=1024`**  \n",
        "  Same as `n_positions`; used for backward compatibility. It defines the length of the attention context window.\n",
        "\n",
        "- **`n_embd=768`**  \n",
        "  Embedding size — each token is represented as a 768-dimensional vector. Also used internally throughout the model layers.\n",
        "\n",
        "- **`n_layer=20`**  \n",
        "  The number of Transformer blocks stacked in the model. More layers = more capacity to learn patterns, but also more compute.\n",
        "\n",
        "- **`n_head=12`**  \n",
        "  Number of attention heads in each layer. The model splits the embedding space into 12 parts and attends to different positions in parallel.\n",
        "\n",
        "- **`resid_pdrop=0.1`**  \n",
        "  Dropout applied to the residual (skip) connections to prevent overfitting.\n",
        "\n",
        "- **`embd_pdrop=0.1`**  \n",
        "  Dropout applied right after the token and positional embeddings.\n",
        "\n",
        "- **`attn_pdrop=0.1`**  \n",
        "  Dropout applied inside the self-attention mechanism — helps regularize attention weights.\n",
        "\n",
        "These values are inspired by the GPT-2 Medium configuration and give the model enough capacity to learn meaningful patterns from Git diffs and commit messages, while still being trainable on a single GPU setup like an A100.\n",
        "\n",
        "> **Loss Function**:  \n",
        "> The model is trained using **cross entropy loss**, which is the default in Hugging Face’s `GPT2LMHeadModel` when you provide `labels=batch`. It compares the predicted token distribution to the actual next token and penalizes incorrect predictions. This is standard for autoregressive language modeling.\n",
        "\n",
        "4. **Implements a training loop** that:\n",
        "- Trains for 10 epochs using AdamW with `lr=2e-4`\n",
        "- Tracks both training and validation loss\n",
        "- Plots loss vs. weight norm to monitor training dynamics\n",
        "- Saves `loss_curve.png` and `loss_vs_weight_norm.png`\n",
        "\n",
        "### Input\n",
        "- CSV file: `data/cleaned_python_commit_dataset.csv`\n",
        "- Columns: `diff`, `commit_message`\n",
        "\n",
        "### Output\n",
        "- Trained model saved to `trained_model/`\n",
        "- Loss curve: `loss_curve.png`\n",
        "- Weight norm vs. loss: `loss_vs_weight_norm.png`\n",
        "- Printed commit message generated from a test diff\n",
        "\n",
        "Make sure you've already run the tokenizer training step, and that `custom_bpe_tokenizer.json` is in the current directory.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1jeGbs2QCd1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Config, GPT2LMHeadModel\n",
        "from torch.optim import AdamW\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "\n",
        "\n",
        "# Each sample is a tuple: (git_diff, commit_message).\n",
        "# Use \"<endOfDiff>\" as a separator between the git diff and commit message,\n",
        "# and append \"<endOfCommitMessage>\" to mark the end of the commit message.\n",
        "class GitDiffDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        \"\"\"\n",
        "        data: List of tuples (git_diff_text, commit_message_text)\n",
        "        tokenizer: Instance of our custom tokenizer.\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the git diff and commit message for the current sample.\n",
        "        git_diff_text, commit_msg_text = self.data[idx]\n",
        "\n",
        "        # Ensure both values are strings ***\n",
        "        git_diff_text = str(git_diff_text)\n",
        "        commit_msg_text = str(commit_msg_text)\n",
        "\n",
        "        # Define a custom separator and EOS token.\n",
        "        separator = \"<endOfDiff>\"\n",
        "        eos_token = \"<endOfCommitMessage>\"\n",
        "        # Concatenate git diff, separator, commit message, and EOS token.\n",
        "        full_text = git_diff_text + separator + commit_msg_text + eos_token\n",
        "        # Encode the concatenated text into token IDs.\n",
        "        token_ids = self.tokenizer.encode(full_text)\n",
        "        return torch.tensor(token_ids, dtype=torch.long)\n",
        "\n",
        "# Pads sequences in a batch to the same length.\n",
        "# This function is called by DataLoader to combine samples into a mini-batch.\n",
        "def collate_fn(batch):\n",
        "    max_length = max(seq.size(0) for seq in batch)\n",
        "    # Limit max_length to n_positions to avoid IndexError\n",
        "    max_length = min(max_length, model.config.n_positions)  # <-- Added this line\n",
        "\n",
        "    padded_batch = []\n",
        "    for seq in batch:\n",
        "        pad_len = max_length - seq.size(0)\n",
        "        # Pad sequences with 0 (our designated pad token).\n",
        "        padded_seq = F.pad(seq, (0, pad_len), value=custom_tokenizer.pad_token_id)\n",
        "        padded_batch.append(padded_seq.unsqueeze(0))\n",
        "    return torch.cat(padded_batch, dim=0)\n",
        "\n",
        "\n",
        "# GPT2LMHeadModel from transformers\n",
        "# Create a GPT2 configuration from scratch.\n",
        "config = GPT2Config(\n",
        "    vocab_size=custom_tokenizer.vocab_size,  # Using our custom BPE tokenizer.\n",
        "    n_positions=1024,             # Maximum number of tokens in a sequence.\n",
        "    n_ctx=1024,                   # Context size (should match n_positions).\n",
        "    n_embd=768,                   # Embedding size (d_model).\n",
        "    n_layer=20,                    # Number of transformer layers.\n",
        "    n_head=12,                     # Number of attention heads.\n",
        "    resid_pdrop=0.1,              # Dropout probability for residual connections.\n",
        "    embd_pdrop=0.1,               # Dropout probability for embeddings.\n",
        "    attn_pdrop=0.1,               # Dropout probability for attention weights.\n",
        ")\n",
        "\n",
        "# Initialize the GPT2 model with a language modeling head from scratch.\n",
        "model = GPT2LMHeadModel(config)\n",
        "# Since we're training from scratch, resize the token embeddings\n",
        "# to accommodate any added tokens:\n",
        "model.resize_token_embeddings(custom_tokenizer.vocab_size)\n",
        "\n",
        "\n",
        "# Training Loop\n",
        "# This function trains the model using the provided training and validation data.\n",
        "def train_model(model, train_loader, val_loader, epochs=10, lr=2e-4, device='cuda'):\n",
        "    # Use the AdamW optimizer which is well-suited for transformers.\n",
        "    # lr used to be 1e-4. we are now experimenting with 2e-4\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    model.to(device)\n",
        "\n",
        "    # Track losses per epoch\n",
        "    train_losses, val_losses = [], []\n",
        "    weight_norms = []\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Set model to training mode.\n",
        "        total_loss = 0.0\n",
        "\n",
        "        # tqdm progress bar for the training loop.\n",
        "        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\", leave=False)\n",
        "        for batch_idx, batch in enumerate(train_bar):\n",
        "            batch = batch.to(device)\n",
        "            # The target is the input shifted by one (language modeling objective).\n",
        "            outputs = model(batch, labels=batch)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            optimizer.zero_grad()  # Clear previous gradients.\n",
        "            loss.backward()        # Compute gradients.\n",
        "            optimizer.step()       # Update model weights.\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            # Update progress bar with current average loss.\n",
        "            train_bar.set_postfix(loss=f\"{total_loss/(batch_idx+1):.4f}\")\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation phase after each epoch.\n",
        "        model.eval()  # Set model to evaluation mode.\n",
        "        val_loss = 0.0\n",
        "        # tqdm progress bar for the validation loop.\n",
        "        val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\", leave=False)\n",
        "        with torch.no_grad():\n",
        "            for batch in val_bar:\n",
        "                batch = batch.to(device)\n",
        "                outputs = model(batch, labels=batch)\n",
        "                val_loss += outputs.loss.item()\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        # compute L2 norm of *all* parameters\n",
        "        total_norm_sq = 0.0\n",
        "        for p in model.parameters():\n",
        "            total_norm_sq += p.data.norm().item()**2\n",
        "        weight_norm = math.sqrt(total_norm_sq)\n",
        "        weight_norms.append(weight_norm)\n",
        "\n",
        "        print(f\"Epoch {epoch+1} Training Loss: {avg_train_loss:.4f}\")\n",
        "        print(f\"Epoch {epoch+1} Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1} Weight Norm: {weight_norm:.4f}\")\n",
        "\n",
        "    # Once training is done, plot weight-norm vs. loss:\n",
        "    plt.figure()\n",
        "    plt.plot(weight_norms, train_losses)\n",
        "    plt.xlabel(\"Weight L2-Norm\")\n",
        "    plt.ylabel(\"Training Loss\")\n",
        "    plt.title(\"Training Loss vs. Weight Norm\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"loss_vs_weight_norm.png\")\n",
        "    plt.close()\n",
        "    print(\"Saved plot to loss_vs_weight_norm.png\")\n",
        "\n",
        "\n",
        "    # After all epochs, plot & save\n",
        "    plot_and_save_loss(train_losses, val_losses, filename=\"loss_curve.png\")\n",
        "    return train_losses, val_losses, weight_norms\n",
        "\n",
        "\n",
        "# Loading CSV Data and Putting it All Together\n",
        "if __name__ == \"__main__\":\n",
        "    # The CSV should have two columns: 'git_diff' and 'commit_message'.\n",
        "    df = pd.read_csv(\"data/cleaned_python_commit_dataset.csv\")\n",
        "    # Convert DataFrame columns into a list of tuples.\n",
        "    data = list(zip(df['diff'].tolist(), df['commit_message'].tolist()))\n",
        "\n",
        "    # Shuffle the data and split it into training (80%) and validation (20%) sets.\n",
        "    random.shuffle(data)\n",
        "    split_idx = int(0.8 * len(data))\n",
        "    train_data = data[:split_idx]\n",
        "    val_data = data[split_idx:]\n",
        "\n",
        "    # Create PyTorch datasets for training and validation.\n",
        "    train_dataset = GitDiffDataset(train_data, custom_tokenizer)\n",
        "    val_dataset = GitDiffDataset(val_data, custom_tokenizer)\n",
        "\n",
        "    # Create DataLoaders to batch and shuffle the data.\n",
        "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    # Choose the device (GPU if available, otherwise CPU).\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Train the model using our training loop\n",
        "    train_model(model, train_loader, val_loader, epochs=10, lr=2e-4, device=device)\n",
        "\n",
        "    # Save the trained model.\n",
        "    # This will save both the model weights and configuration.\n",
        "    save_directory = \"trained_model\"\n",
        "    model.save_pretrained(save_directory)\n",
        "    print(f\"Model saved to {save_directory}\")\n"
      ],
      "metadata": {
        "id": "p2GmjQbcCwLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the trained model\n",
        "\n",
        "Zip up the `trained_model/` directory and download it to your local machine.\n",
        "\n",
        "- The folder is compressed into a file called `trained_model.zip`.\n",
        "- Make sure the model was saved to `trained_model/` before running this.\n",
        "\n"
      ],
      "metadata": {
        "id": "2bzy7ootDJgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Zip the 'trained_model' directory into a file named 'trained_model.zip'\n",
        "shutil.make_archive('trained_model', 'zip', 'trained_model')\n",
        "\n",
        "# Download the zipped model\n",
        "files.download('trained_model.zip')\n"
      ],
      "metadata": {
        "id": "5QumX6VZDMWF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}