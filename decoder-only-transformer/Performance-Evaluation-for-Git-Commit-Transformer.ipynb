{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kktT3nDgEoe8"
      },
      "source": [
        "## Generate evaluation scores for performance testing\n",
        "\n",
        "This cell runs the model on 100 random samples from the dataset and scores each output using multiple metrics:\n",
        "\n",
        "- **BLEU**: Measures n-gram overlap between generated and reference commit messages.\n",
        "- **ROUGE-L**: Captures longest matching subsequences.\n",
        "- **METEOR**: Similar to BLEU but accounts for synonyms using WordNet.\n",
        "- **BERTScore**: Uses BERT embeddings to measure semantic similarity.\n",
        "- **RAGAs (Answer Correctness)**: Evaluates how correct the generated message is in context, using the OpenAI API.\n",
        "\n",
        "What this does:\n",
        "- Loads the tokenizer and resizes the model’s embeddings.\n",
        "- Defines a `safe_generate()` function to handle long inputs and decode outputs.\n",
        "- Samples 100 rows from the cleaned dataset and generates commit messages.\n",
        "- Computes all the metrics above and saves the results to a JSON file.\n",
        "- Downloads the final file: `sample_metrics_with_ragas_and_bertscore.json`.\n",
        "\n",
        "> **Note:** You must have the correct dataset path defined in the line:\n",
        "> `df = pd.read_csv(\"data/cleaned_python_commit_dataset.csv\")`.  \n",
        "> Adjust it if your file is located elsewhere or named something else.\n",
        "\n",
        "> Make sure you’ve set a working OpenAI API key in `os.environ[\"OPENAI_API_KEY\"]` before running this.\n",
        "> You’ll also need to have **active credits or a paid plan** on your OpenAI account — RAGAs uses GPT-based evaluation under the hood.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0ISnBd_Ebuq"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------\n",
        "# Eval on 100 samples — BLEU, ROUGE-L, METEOR, BERTScore, RAGAs Answer Correctness → JSON + download\n",
        "# ------------------------------------------------------------\n",
        "!pip install -q \"nltk==3.8.1\" rouge-score bert-score ragas datasets\n",
        "\n",
        "import torch, pandas as pd, json, nltk, random, time, os\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from rouge_score import rouge_scorer\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "from bert_score import BERTScorer\n",
        "from ragas.metrics import answer_correctness\n",
        "from ragas import evaluate\n",
        "from datasets import Dataset\n",
        "\n",
        "# Set OpenAI API Key\n",
        "api_key = \"\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "# WordNet download for METEOR\n",
        "nltk.download(\"wordnet\", quiet=True)\n",
        "nltk.download(\"omw-1.4\", quiet=True)\n",
        "\n",
        "# --- tokenizer setup --------------------------------------------------------------\n",
        "tok = PreTrainedTokenizerFast(tokenizer_file=\"custom_bpe_tokenizer.json\")\n",
        "tok.add_special_tokens({\n",
        "    \"pad_token\": \"<pad>\",\n",
        "    \"eos_token\": \"<endOfCommitMessage>\"\n",
        "})\n",
        "tok.add_tokens([\"<endOfDiff>\"])\n",
        "model.resize_token_embeddings(len(tok))\n",
        "\n",
        "MAX_CTX, GEN_SPACE = model.config.n_positions, 50\n",
        "SAFE_LIM = MAX_CTX - GEN_SPACE\n",
        "device = next(model.parameters()).device  # e.g. 'cuda:0'\n",
        "\n",
        "# --- helper functions --------------------------------------------------------------\n",
        "def safe_generate(model, tokenizer, diff_text, device=\"cuda\"):\n",
        "    sep, eos = \"<endOfDiff>\", \"<endOfCommitMessage>\"\n",
        "    ids = tokenizer.encode(diff_text + sep)\n",
        "    if len(ids) > SAFE_LIM:\n",
        "        ids = ids[-SAFE_LIM:]\n",
        "    inp = torch.tensor([ids]).to(device)\n",
        "    out = model.generate(\n",
        "        inp,\n",
        "        max_length=len(ids) + GEN_SPACE,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.encode(eos)[0],\n",
        "        do_sample=False,\n",
        "        repetition_penalty=1.2,\n",
        "        no_repeat_ngram_size=3\n",
        "    )[0].tolist()\n",
        "    txt = tokenizer.decode(out)\n",
        "    return txt.split(sep, 1)[1].split(eos, 1)[0].strip()\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv(\"data/cleaned_python_commit_dataset.csv\")\n",
        "\n",
        "# --- sampling setup --------------------------------------------------------------\n",
        "sample_df = (\n",
        "    df.sample(n=100, random_state=42)\n",
        "      .reset_index(drop=True)\n",
        "      .fillna({\"diff\": \"\", \"commit_message\": \"\"})\n",
        ")\n",
        "sample_df[\"diff\"] = sample_df[\"diff\"].astype(str)\n",
        "sample_df[\"commit_message\"] = sample_df[\"commit_message\"].astype(str)\n",
        "\n",
        "smooth_fn = SmoothingFunction().method1\n",
        "rouge = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
        "bert_scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)\n",
        "\n",
        "# Buffers to build a HuggingFace Dataset for RAGAs\n",
        "ragas_examples = {\"question\": [], \"answer\": [], \"ground_truth\": []}\n",
        "results = []\n",
        "\n",
        "bar = tqdm(list(zip(sample_df[\"diff\"], sample_df[\"commit_message\"])),\n",
        "           total=len(sample_df), desc=\"Scoring\")\n",
        "\n",
        "for diff_text, ref in bar:\n",
        "    hyp = safe_generate(model, tok, diff_text, device=device)\n",
        "\n",
        "    if ref.strip():\n",
        "        ref_tokens = ref.split()\n",
        "        hyp_tokens = hyp.split()\n",
        "        bleu = sentence_bleu([ref_tokens], hyp_tokens, smoothing_function=smooth_fn)\n",
        "        rougeL = rouge.score(ref, hyp)[\"rougeL\"].fmeasure\n",
        "        meteor = meteor_score([ref_tokens], hyp_tokens)\n",
        "    else:\n",
        "        bleu = rougeL = meteor = 0.0\n",
        "\n",
        "    P, R, F1 = bert_scorer.score([hyp], [ref])\n",
        "    bert_f1 = F1[0].item()\n",
        "\n",
        "    bar.set_postfix(\n",
        "        BLEU=f\"{bleu:.4f}\",\n",
        "        ROUGE_L=f\"{rougeL:.4f}\",\n",
        "        METEOR=f\"{meteor:.4f}\",\n",
        "        BERTScore=f\"{bert_f1:.4f}\"\n",
        "    )\n",
        "\n",
        "    ragas_examples[\"question\"].append(diff_text)\n",
        "    ragas_examples[\"answer\"].append(hyp)\n",
        "    ragas_examples[\"ground_truth\"].append(ref)\n",
        "\n",
        "    results.append({\n",
        "        \"diff\": diff_text,\n",
        "        \"generated_commit\": hyp,\n",
        "        \"label_commit\": ref,\n",
        "        \"bleu\": bleu,\n",
        "        \"rouge_l\": rougeL,\n",
        "        \"meteor\": meteor,\n",
        "        \"bert_score_f1\": bert_f1\n",
        "    })\n",
        "\n",
        "bar.close()\n",
        "\n",
        "# --- run RAGAs Answer Correctness with retry loop -----------------------------\n",
        "ragas_ds = Dataset.from_dict(ragas_examples)\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        ragas_res = evaluate(ragas_ds, metrics=[answer_correctness])\n",
        "        break  # success\n",
        "    except TimeoutError:\n",
        "        print(\"Timeout occurred, retrying in 5 seconds...\")\n",
        "        time.sleep(5)\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}. Retrying in 5 seconds...\")\n",
        "        time.sleep(5)\n",
        "\n",
        "df_corr = ragas_res.to_pandas()\n",
        "for i, row in enumerate(df_corr.itertuples()):\n",
        "    results[i][\"answer_correctness\"] = row.answer_correctness\n",
        "\n",
        "# --- save and download -------------------------------------------------------\n",
        "json_file = \"sample_metrics_with_ragas_and_bertscore.json\"\n",
        "with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "files.download(json_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASw95PIqE4Ll"
      },
      "source": [
        "## Write test scores to a .txt file for manual analysis\n",
        "\n",
        "This cell takes the JSON file of model evaluation results (`sample_metrics_with_ragas_and_bertscore.json`) and converts it into a readable text file.\n",
        "\n",
        "What it does:\n",
        "- Loads all evaluation samples from the JSON file.\n",
        "- For each sample, it writes:\n",
        "  - The generated commit message\n",
        "  - The ground-truth commit message\n",
        "  - All evaluation scores (BLEU, ROUGE-L, METEOR, BERTScore-F1, AnswerCorrectness)\n",
        "  - The associated Git diff\n",
        "- Outputs everything in a clean, readable format to `all_diffs.txt`, with clear separators between samples.\n",
        "\n",
        "This is useful if you want to manually review model outputs and compare them to the reference messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9siDxk3yE5Qn"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pathlib\n",
        "\n",
        "in_path  = pathlib.Path(\"sample_metrics_with_ragas_and_bertscore.json\")   # JSON array produced earlier\n",
        "out_path = pathlib.Path(\"all_diffs.txt\")             # destination text file\n",
        "\n",
        "# 1) Load the whole JSON array\n",
        "with in_path.open(encoding=\"utf-8\") as src:\n",
        "    samples = json.load(src)     \n",
        "\n",
        "# 2) Dump each sample in a readable block\n",
        "with out_path.open(\"w\", encoding=\"utf-8\") as dst:\n",
        "    for i, obj in enumerate(samples, 1):\n",
        "        dst.write(f\"---- SAMPLE #{i} ----\\n\")\n",
        "        dst.write(f\"Generated commit : {obj.get('generated_commit','')}\\n\")\n",
        "        dst.write(f\"Ground-truth     : {obj.get('label_commit','')}\\n\")\n",
        "        dst.write(\n",
        "            \"Scores (BLEU / ROUGE-L / METEOR / BERTScore-F1 / AnswerCorrectness) : \"\n",
        "            f\"{obj.get('bleu', 0.0):.4f} / \"\n",
        "            f\"{obj.get('rouge_l', 0.0):.4f} / \"\n",
        "            f\"{obj.get('meteor', 0.0):.4f} / \"\n",
        "            f\"{obj.get('bert_score_f1', 0.0):.4f} / \"\n",
        "            f\"{obj.get('answer_correctness', 0.0):.4f}\\n\"\n",
        "        )\n",
        "        dst.write(\"Diff:\\n\")\n",
        "        dst.write(obj.get(\"diff\",\"\"))\n",
        "        dst.write(\"\\n\\n\")  # blank line between samples\n",
        "\n",
        "print(f\"Wrote {len(samples)} samples to {out_path.resolve()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ_sJ95VE5sd"
      },
      "source": [
        "## Create plots for results and save\n",
        "\n",
        "This cell loads the evaluation results from `sample_metrics_with_ragas_and_bertscore.json` and visualizes the distribution of all five metrics across the 100 samples.\n",
        "\n",
        "What it does:\n",
        "- Loads the JSON into a pandas DataFrame.\n",
        "- Extracts:  \n",
        "  - `bleu`  \n",
        "  - `rouge_l`  \n",
        "  - `meteor`  \n",
        "  - `bert_score_f1`  \n",
        "  - `answer_correctness`  \n",
        "- Plots a histogram for each metric to show how scores are distributed across the dataset.\n",
        "- Saves each plot as a separate PNG file:\n",
        "  - `bleu4_distribution.png`\n",
        "  - `rouge_l_distribution.png`\n",
        "  - `meteor_distribution.png`\n",
        "  - `bert_score_f1_distribution.png`\n",
        "  - `answer_correctness_distribution.png`\n",
        "\n",
        "This gives a quick visual sense of how well the model is performing across different evaluation dimensions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmIwrO0DE9Z-"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "json_path = \"sample_metrics_with_ragas_and_bertscore.json\"   # <-- change if yours lives elsewhere\n",
        "\n",
        "# Load JSON and select all five metrics\n",
        "with open(json_path, encoding=\"utf-8\") as f:\n",
        "    df = pd.DataFrame(json.load(f))\n",
        "scores = df[[\"bleu\", \"rouge_l\", \"meteor\", \"bert_score_f1\", \"answer_correctness\"]].fillna(0.0)\n",
        "\n",
        "# BLEU‑4 distribution\n",
        "plt.figure()\n",
        "plt.hist(scores[\"bleu\"], bins=30)\n",
        "plt.title(\"Distribution of BLEU‑4 scores\")\n",
        "plt.xlabel(\"BLEU‑4\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.savefig(\"bleu4_distribution.png\")\n",
        "\n",
        "# ROUGE‑L F1 distribution\n",
        "plt.figure()\n",
        "plt.hist(scores[\"rouge_l\"], bins=30)\n",
        "plt.title(\"Distribution of ROUGE‑L F1 scores\")\n",
        "plt.xlabel(\"ROUGE‑L F1\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.savefig(\"rouge_l_distribution.png\")\n",
        "\n",
        "# METEOR distribution\n",
        "plt.figure()\n",
        "plt.hist(scores[\"meteor\"], bins=30)\n",
        "plt.title(\"Distribution of METEOR scores\")\n",
        "plt.xlabel(\"METEOR\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.savefig(\"meteor_distribution.png\")\n",
        "\n",
        "# BERTScore‑F1 distribution\n",
        "plt.figure()\n",
        "plt.hist(scores[\"bert_score_f1\"], bins=30)\n",
        "plt.title(\"Distribution of BERTScore‑F1 scores\")\n",
        "plt.xlabel(\"BERTScore‑F1\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.savefig(\"bert_score_f1_distribution.png\")\n",
        "\n",
        "# Answer Correctness distribution\n",
        "plt.figure()\n",
        "plt.hist(scores[\"answer_correctness\"], bins=30)\n",
        "plt.title(\"Distribution of Answer Correctness scores\")\n",
        "plt.xlabel(\"Answer Correctness\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.savefig(\"answer_correctness_distribution.png\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
